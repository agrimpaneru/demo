[{"content":"LMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\nIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u0026ldquo;Attention Is All You Need\u0026rdquo; shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\nEmbeddings representing the Words as vectors Initially the words are converted into tokens, Here we assume tokens are equal to index.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] Output for this code is :\n[tensor([0.8823, 0.9150, 0.3829]), tensor([0.9593, 0.3904, 0.6009]), tensor([0.2566, 0.7936, 0.9408]), tensor([0.1332, 0.9346, 0.5936]), tensor([0.8694, 0.5677, 0.7411])] Here the corresponding Embeddings for each word is initialized randomly. Here we represented each word as 3 dimensional embedding.\n{\u0026#39;the\u0026#39;: tensor([0.8823, 0.9150, 0.3829]), \u0026#39;cat\u0026#39;: tensor([0.9593, 0.3904, 0.6009]), \u0026#39;sat\u0026#39;: tensor([0.2566, 0.7936, 0.9408]), \u0026#39;on\u0026#39;: tensor([0.1332, 0.9346, 0.5936]), \u0026#39;mat\u0026#39;: tensor([0.8694, 0.5677, 0.7411])} Attention Mechanism Attention Mechanism is inspired by how humans focus on specific parts of information when processing it—just like when we read a sentence, we don\u0026rsquo;t focus on every word equally. Instead, we \u0026ldquo;attend\u0026rdquo; to the most relevant words that help us understand the meaning.\nLets take an example for understanding attention mechanism:\nImagine reading the sentence: \u0026ldquo;The cat sat on the mat.\u0026rdquo;\nIf asked, \u0026ldquo;Where is the cat?\u0026rdquo;, attention would primarily focus on the word \u0026ldquo;cat\u0026rdquo; and the word \u0026ldquo;mat\u0026rdquo;. Other words like \u0026ldquo;the\u0026rdquo; or \u0026ldquo;on\u0026rdquo; are ignored since they don’t carry much relevance to the question.\nLets take a sentence:\nI am going to cinema to watch ………?\nThe most probable answers would be \u0026ldquo;movie,\u0026rdquo; \u0026ldquo;action movie,\u0026rdquo; or something similar. Words like \u0026ldquo;book\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; don\u0026rsquo;t fit the context and are irrelevant to predicting the correct word. The key to predicting the correct word is in the context and information provided in the preceding words. To make accurate predictions, we need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the model to focus on the most relevant words to that context to make predictions.\nUnderstanding Scaled Dot-Product Attention The paper Attention Is All You Need introduced Scaled Dot-Product Attention .\nLets first calculate the Similarity between Query and Key\nThis is done as\nQ = K = V = embeddings similarity_matrix=torch.matmul(Q,K.T) In our case the dimension of output is 5*5.\nSimilarity Matrix:\ntensor([[1.7622, 1.4337, 1.3127, 1.1999, 1.5702], [1.4337, 1.4338, 1.1213, 0.8494, 1.5010], [1.3127, 1.1213, 1.5807, 1.3343, 1.3708], [1.1999, 0.8494, 1.3343, 1.2435, 1.0863], [1.5702, 1.5010, 1.3708, 1.0863, 1.6274]]) Here’s each value represents the similarity between words as described below.\nthe cat sat on mat the the-the the-cat the-sat the-on the-mat cat cat-the cat-cat cat-sat cat-on cat-mat sat sat-the sat-cat sat-sat sat-on sat-mat on on-the on-cat on-sat on-on on-mat mat mat-the mat-cat mat-sat mat-on mat-mat The similarity matrix captures the similarities between the query and key vectors. Higher similarity means the model should focus more on that word when making predictions, while lower similarity means the word is less relevant.\nScaling the compatibility matrix In the second step, we scale the dot-product of the query and key vectors by a factor of,\n$$ \\frac{1}{d_k} $$ . In our case We’ve only taken dimension of 3.\nHowever, when the dimension d is very large, dot-product attention without this scaling factor performs poorly. The authors of the paper “Attention Is All You Need” attribute this poor performance to having large variance.\nAs d_k increases, the variance of the dot product also becomes larger, leading to some values becoming extremely small after being passed through the softmax function. This causes issues during gradient calculation, similar to the vanishing gradient problem, where the gradients become too small to effectively update the model. To mitigate this, the dot product is scaled by $$ \\sqrt{\\frac{1}{d_k}} $$\nbefore applying softmax , which stabilizes the training process and improves performance.\nTo visualize this effect lets take output from softmax before and after scaling for embedding dimension of 256.\nApplying Softmax Activation The softmax function is used to convert the logits into probabilities. It takes a vector of raw scores (logits) and transforms them into a probability distribution. The softmax function is defined as:\n$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\nSoftmax is applied pointwise, so it doesn\u0026rsquo;t change the dimension of the input vector.\nimport torch.nn.functional as F similarity_matrix_scaled=similarity_matrix/(1/torch.sqrt(torch.tensor(3))) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) softmax_similarity_matrix_scaled Its output is:\ntensor([[0.2372, 0.1962, 0.1830, 0.1714, 0.2123], [0.2179, 0.2180, 0.1820, 0.1555, 0.2266], [0.1957, 0.1752, 0.2285, 0.1982, 0.2024], [0.2058, 0.1681, 0.2224, 0.2110, 0.1927], [0.2154, 0.2070, 0.1920, 0.1629, 0.2227]]) It can be visualized in Heatmap as\nComputing the Context Vector as final output. So for final step We multiply the attention matrix from previous step and multiply it with the Value matrix V. The final product gives the new context vector for all the words in vocabulary in our case 5.\nnew_context=torch.matmul(softmax_similarity_matrix_scaled,V) new_context Output:\ntensor([[0.6518, 0.7195, 0.6399], [0.6658, 0.7029, 0.6459], [0.6018, 0.7289, 0.6628], [0.5955, 0.7371, 0.6571], [0.6532, 0.7090, 0.6492]]) Conclusion In this section, we saw how contextual embeddings are generated using self-attention. We also discussed the importance of scaling and how to implement a simple self-attention block. Notably, there were no trainable parameters involved here, and we treated the Query (Q), Key (K), and Value (V) matrices as all being the same.\nHowever, to make the attention block trainable, we need to introduce learnable weights when deriving the Q, K, and V vectors from the original embedding. These weights will allow the model to adjust and fine-tune the attention mechanism during training. In the next section, we\u0026rsquo;ll explore how to implement this by adding trainable parameters to the attention block\nAttention Mechanism with trainable Parameters Alright! Now that we understand how self-attention is calculated, let\u0026rsquo;s take it a step further by introducing trainable parameters so that the model can learn how to find the context of words effectively.\nAdding Trainable Parameters to Self-Attention Q (Query), K (Key), and V (Value) matrices are not just direct copies of word embeddings. Instead, they are learned transformations of these embeddings.\nTo achieve this, we introduce three trainable weight matrices:\nW_q (Query weight matrix): Learns how to project word embeddings into the query space. W_k (Key weight matrix): Learns how to project word embeddings into the key space. W_v (Value weight matrix): Learns how to project word embeddings into the value space. Each of these matrices will be optimized during training.\nW_q=torch.randn(embedding_dim,embedding_dim) W_v=torch.randn(embedding_dim,embedding_dim W_k=torch.randn(embedding_dim,embedding_dim) Note that the dimension of these matrix can be made different. However the first dimension must be equal to embedding dimension to satisfy the matrix multiplication condition. Changing the second dimension implies projecting the embedding dimension from one dimension to other .\nLets get the new Query, Key and Value vector.\nQ = torch.matmul(embeddings, W_q) # (5, 3) K = torch.matmul(embeddings, W_k) # (5, 3) V = torch.matmul(embeddings, W_v) # (5, 3) Then we calculate the self attention similar to how we did above.\nsimilarity_matrix = torch.matmul(Q, K.T) # (5, 5) similarity_matrix_scaled = similarity_matrix / torch.sqrt(torch.tensor(embedding_dim, dtype=torch.float32)) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(softmax_similarity_matrix_scaled, V) This is what the final flow looks like.\nHere Lets wrap up this in Class Based Implementation\nimport torch import torch.nn.functional as F class SelfAttention: def __init__(self, embedding_dim): torch.manual_seed(42) self.embedding_dim = embedding_dim # Initialize weight matrices self.W_q = torch.randn(embedding_dim, embedding_dim) self.W_k = torch.randn(embedding_dim, embedding_dim) self.W_v = torch.randn(embedding_dim, embedding_dim) def forward(self, embeddings): # Compute Query, Key, and Value matrices Q = torch.matmul(embeddings, self.W_q) K = torch.matmul(embeddings, self.W_k) V = torch.matmul(embeddings, self.W_v) # Compute similarity (dot product attention) similarity_matrix = torch.matmul(Q, K.T) # (num_words, num_words) # Scale by sqrt(embedding_dim) similarity_matrix_scaled = similarity_matrix / torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32) ) # Apply softmax to get attention weights attention_weights = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(attention_weights, V) return new_context, attention_weights embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] # Generate random embeddings for words embeddings = torch.stack([torch.rand(embedding_dim) for _ in words]) # Initialize attention mechanism self_attention = SelfAttention(embedding_dim) # Forward pass to compute attention new_context, attention_weights = self_attention.forward(embeddings) print(\u0026#34;New Context Vectors:\\n\u0026#34;, new_context) print(\u0026#34;\\nAttention Weights:\\n\u0026#34;, attention_weights) Output:\nNew Context Vectors: tensor([[ 0.3689, -0.4890, 1.3677], [ 0.2569, -0.4673, 1.3297], [ 0.3284, -0.4752, 1.3515], [ 0.1971, -0.4685, 1.3124], [ 0.3061, -0.4846, 1.3491]]) Attention Weights: tensor([[0.4672, 0.0538, 0.0879, 0.0829, 0.3082], [0.3362, 0.1266, 0.1921, 0.1077, 0.2375], [0.4212, 0.0851, 0.1492, 0.0815, 0.2631], [0.2728, 0.1547, 0.1886, 0.1526, 0.2314], [0.3922, 0.0832, 0.1179, 0.1143, 0.2923]]) So this ends up this blog. Congratulations on making it to the end. Feel free to experiment with different dimensions and input data to see how the self-attention mechanism adapts and scales.\nReferences Implementing Self-Attention from Scratch in PyTorch | by Mohd Faraaz | Medium\nUnderstanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\nSelf Attention in Transformers | Deep Learning | Simple Explanation with Code!\nSelf-attention from scratch | Blogs by Anil\n","permalink":"http://localhost:1313/prtfoio/blog/self-attention-pytorch/","summary":"\u003cp\u003eLMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\u003c/p\u003e\n\u003cp\u003eIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u003cem\u003e\u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/em\u003e shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\u003c/p\u003e","title":"Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch"},{"content":"Implementing Multihead attention from scratch with pytorch In our previous article, we built Self-Attention from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\nNow, let’s take things a step further and implement Multi-Head Attention from scratch. This post focuses more on the implementation rather than the theory, so I assume you’re already familiar with how self-attention works.\nLet’s get started!\nMultihead Attention Similar to Self-Attention, Multi-Head Attention also creates a new context vector. But instead of using a single attention mechanism, it uses multiple heads, each learning different relationships between words.\nFor example, consider these two sentences:\n1️⃣ I went to the bank to withdraw cash.\n2️⃣ I went for a picnic by the river bank.\nHere, the word \u0026ldquo;bank\u0026rdquo; has different meanings depending on the context. The idea behind Multi-Head Attention is that different heads can focus on different aspects of a word’s meaning, capturing various relationships more effectively.\nIn the above example with a single attention head, the model might have difficulty capturing both the financial and geographical meanings of the word \u0026ldquo;bank\u0026rdquo; at the same time. Multi-Head Attention, on the other hand, allows the model to learn both meanings separately—one head could focus on financial context while another focuses on the geographical context. This way, the model learns richer, more nuanced embeddings of the word \u0026ldquo;bank\u0026rdquo; based on different contexts.\nFor a more intuitive explanation of Multi-Head Attention, I highly recommend checking out this medium post.\nNow Lets Implement it.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 8 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) Similar to our previous Self-Attention implementation, we’ll use the following sentence with a random embedding of dimension 8. However, in real-world applications, embeddings are typically a combination of word embeddings and positional encodings. To keep things simple, we won’t dive into that here. Now, let’s define the Wq, Wk, and Wv matrices. We could use separate smaller weight matrices to get the Query, Key, and Value vectors directly, but a more efficient approach is to use a single large weight matrix and split it afterward for multiple attention heads. This method is more computationally efficient, as it allows for parallel processing across heads.\nWq = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wk = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wv = torch.rand(embedding_dim, embedding_dim) # (8, 8) Lets apply the projection to Get Query Key and Value matrix.\n# Apply projection Q = embeddings @ Wq # Shape: (5, 8) K = embeddings @ Wk # Shape: (5, 8) V = embeddings @ Wv # Shape: (5, 8) Now ,We’ll split the projected vectors into multiple heads . Remember the Embedding dimension must be exactly divisible by head\nd_k = embedding_dim // num_heads Now we obtain the Query Key and Value Matrix for multiple heads.\nQ = Q.view(seq_len, num_heads, d_k).transpose(0, 1) K = K.view(seq_len, num_heads, d_k).transpose(0, 1) V = V.view(seq_len, num_heads, d_k).transpose(0, 1) The output shape of Q, K, and V after the split will be (2, 5, 4), where:\n2 corresponds to the number of heads, 5 corresponds to the sequence length (number of words), and 4 is the new dimension of each vector for each head. We calculate attention matrix by,\nattention_scores=Q @ K.transpose(-2,-1) \u0026#39;\u0026#39;\u0026#39;For 3D matrix multiplication, the first dimension of both matrices should match, #while the second dimension of the first matrix must align with the third dimension of the second matrix according to the matrix multiplication rule\u0026#39;\u0026#39;\u0026#39; Here, we get two attention scores from each head. We’ll proceed with scaling these scores and applying the softmax activation, just like in Self-Attention. For a detailed explanation of this process, please refer to my previous post on Self-Attention\nBut before applying the scaling and softmax activation, we’ll mask the attention scores. Masking is necessary to ensure that the prediction for a given token only depends on the current and previous tokens. Without masking, the model could \u0026ldquo;cheat\u0026rdquo; by looking ahead at future tokens which in not desirable.\nMasked Multi-Head Attention In the decoder of a transformer, we use masked multi-head attention to prevent the model from looking at future tokens when predicting the next one. This ensures that the model can only base its prediction on past tokens, not the future ones. In contrast, the encoder doesn’t need masking because it has access to the entire sequence at once, so it can freely attend to all parts of the input.\nThis masking enables the model to process all tokens in parallel during training, enhancing efficiency.\nWe will implement Masked Multi-Head Attention, ensuring that the model only attends to past words by masking the attention scores. This prevents the decoder from seeing future words, enforcing an autoregressive learning process\nMasking is applied before softmax by setting forbidden positions to−∞ to ensure they receive zero probability after normalization.\nmask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) mask = mask.bool() mask=mask.unsqueeze(0) attention_scores=attention_scores.masked_fill(mask,-1e11) attention_scores=attention_scores/math.sqrt(d_k) #This is mask i.e Upper triangular matrix tensor([[0., 1., 1., 1., 1.], [0., 0., 1., 1., 1.], [0., 0., 0., 1., 1.], [0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.]]) #Attention Scores before masking tensor([[[24.6185, 22.6470, 19.6726, 10.3703, 23.2266], [25.3424, 23.5643, 20.2438, 10.8568, 24.0848], [20.2856, 18.6674, 16.2010, 8.5272, 19.1661], [11.0366, 10.1522, 8.8451, 4.5771, 10.3952], [23.4003, 21.7570, 18.6598, 10.0209, 22.2936]], [[24.1949, 25.1107, 20.5201, 12.3558, 24.2538], [24.3185, 25.1608, 20.5903, 12.5245, 24.4242], [19.3101, 20.0426, 16.3541, 9.9390, 19.3887], [11.4448, 11.6265, 9.6817, 5.8675, 11.3187], [23.9947, 24.8808, 20.3205, 12.3524, 24.1055]]]) #Attention Scores after masking tensor([[[ 2.4619e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.5342e+01, 2.3564e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.0286e+01, 1.8667e+01, 1.6201e+01, -1.0000e+11, -1.0000e+11], [ 1.1037e+01, 1.0152e+01, 8.8451e+00, 4.5771e+00, -1.0000e+11], [ 2.3400e+01, 2.1757e+01, 1.8660e+01, 1.0021e+01, 2.2294e+01]], [[ 2.4195e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.4318e+01, 2.5161e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 1.9310e+01, 2.0043e+01, 1.6354e+01, -1.0000e+11, -1.0000e+11], [ 1.1445e+01, 1.1626e+01, 9.6817e+00, 5.8675e+00, -1.0000e+11], [ 2.3995e+01, 2.4881e+01, 2.0321e+01, 1.2352e+01, 2.4105e+01]]]) So what we did here was created a lower triangular matrix and applied it as a mask to attention score and scaled it and passed to softmax to ensure each token can only attend to itself and previous tokens.\nAfter softmax, the row sum will be one and this ensures the attention weights are a valid probability distribution, allowing the model to learn proportional contributions from past tokens.\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5130, 0.4870, 0.0000, 0.0000, 0.0000], [0.3420, 0.3315, 0.3265, 0.0000, 0.0000], [0.2540, 0.2510, 0.2486, 0.2465, 0.0000], [0.2029, 0.2000, 0.1984, 0.1980, 0.2007]], [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.4935, 0.5065, 0.0000, 0.0000, 0.0000], [0.3342, 0.3391, 0.3267, 0.0000, 0.0000], [0.2521, 0.2528, 0.2485, 0.2466, 0.0000], [0.2006, 0.2022, 0.1984, 0.1981, 0.2007]]]) Here is what out attention scores looks like.\nNow we multiply out Attention score with Value matrix and concatenate it,\noutput=attention_scores@V output.transpose(0,1).reshape(seq_len,embedding_dim) After Concatenation we achieve the final Contextual embedding of each vectors.\nHere\u0026rsquo;s a cleaner and more efficient implementation of the Multi-Head Attention module in PyTorch, wrapped in a class:\nimport torch import math class MultiHeadAttention(torch.nn.Module): def __init__(self, num_heads, embedding_dim): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.embedding_dim = embedding_dim self.d_k = embedding_dim // num_heads self.Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) def forward(self, embeddings): seq_len = embeddings.size(0) Q = embeddings @ self.Wq # (seq_len, embedding_dim) K = embeddings @ self.Wk V = embeddings @ self.Wv # Converting to multiheaded attention Q = Q.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # (num_heads, seq_len, d_k) K = K.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) V = V.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # Compute attention scores attention_scores = torch.matmul(Q, K.transpose(-2, -1)) # (num_heads, seq_len, seq_len) # Apply mask (upper triangular mask for causal attention) mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() mask = mask.unsqueeze(0).expand_as(attention_scores) attention_scores = attention_scores.masked_fill(mask, -1e11) # Scale the attention scores attention_scores = attention_scores / math.sqrt(self.d_k) # Apply softmax to get attention weights attention_weights = torch.softmax(attention_scores, dim=-1) # (num_heads, seq_len, seq_len) # Compute the output (weighted sum of values) output = torch.matmul(attention_weights, V) output = output.transpose(0, 1).contiguous().view(seq_len, self.embedding_dim) return output # Example usage torch.manual_seed(42) embedding_dim = 8 num_heads = 2 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) mha = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim) # Forward pass through the model output = mha(embeddings) print(\u0026#34;Output shape:\u0026#34;, output.shape) ","permalink":"http://localhost:1313/prtfoio/blog/multi-head-attention-pytorch/","summary":"\u003ch1 id=\"implementing-multihead-attention-from-scratch-with-pytorch\"\u003eImplementing Multihead attention from scratch with pytorch\u003c/h1\u003e\n\u003cp\u003eIn our \u003ca href=\"https://agrimpaneru.com.np/blog/self-attention-pytorch/\"\u003eprevious article\u003c/a\u003e, we built \u003cstrong\u003eSelf-Attention\u003c/strong\u003e from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\u003c/p\u003e\n\u003cp\u003eNow, let’s take things a step further and implement \u003cstrong\u003eMulti-Head Attention\u003c/strong\u003e from scratch. This post focuses more on the \u003cstrong\u003eimplementation\u003c/strong\u003e rather than the theory, so I assume you’re already familiar with how self-attention works.\u003c/p\u003e\n\u003cp\u003eLet’s get started!\u003c/p\u003e","title":"MultiHead Attention Explained:Implementing Masked Multihead attention from Scratch in PyTorch"},{"content":"LSTM from Scratch In this post, we will implement a simple next word predictor LSTM from scratch using torch.\nA gentle Introduction to LSTM Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997). As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\nThe LSTM model we’re gonna implement follows this architecture.\nTo learn about their detailed structure here is a reference to this awesome blog Understanding LSTM Networks \u0026ndash; colah\u0026rsquo;s blog.\nFor simplified architecture:\nImports Lets import the required library\nimport torch import torch.nn as nn import torch.nn.functional as F Data Preparation We’ll be using autoregressive sentence generation for our dataset. This approach involves predicting the next word in a sequence based on the words that came before it.\nConsider a sentence,\n\u0026ldquo;I am Peter the hero.\u0026rdquo;\nInput Sequence Target Output [I] AM [I, AM] Peter [ I, AM, Peter] THE [I, AM, Peter, THE] HERO We\u0026rsquo;ll be working with George Orwell\u0026rsquo;s essay The Spike as our dataset. Since the dataset is relatively small, it allows us to focus on developing a deeper intuition without getting overwhelmed by the volume of data.\nTokenization Next, I\u0026rsquo;ll create a dictionary to convert words into indices—a common practice in Natural Language Processing (NLP). Instead of relying on libraries, I\u0026rsquo;ll build it from scratch to better understand how it works.\nchar2idx={char:i for i,char in enumerate(set(data))} # char2idx[\u0026#39;.\u0026#39;]=0 idx2char={i:char for i,char in enumerate(set(data))} We can run char2idx to view the word mapping to indices.\nx=[] for i in orgi.split(\u0026#34;\\n\u0026#34;): x.append([char2idx[j] for j in i.split(\u0026#34; \u0026#34;)]) This snippet processes a text dataset to convert words into their corresponding indices using the char2idx dictionary we created earlier.\nX_train = [] # List to store input sequences Y_train = [] # List to store corresponding outputs for sequence in x: for i in range(1, len(sequence)-1): # Input is the subsequence from start to the ith element X_train.append(sequence[:i]) # Output is the ith element (next token) Y_train.append(sequence[i]) This code prepares the dataset for autoregressive sentence generation, a technique where the model predicts the next token in a sequence based on previous tokens\nFor example, if the sequence is [1, 2, 3, 4], the resulting pairs would look like this:\nInput: [1], Output: 2 Input: [1, 2], Output: 3 Input: [1, 2, 3], Output: 4 Since LSTMs require input sequences of the same length, I padded all sequences to a fixed maximum length. I chose to pad at the beginning of the sequence because it preserves the actual data at the end, which is where the LSTM focuses more when processing the sequence. This ensures that the more meaningful tokens remain closer to the output\nmax_len=max([len(i) for i in X_train]) vocab_size=len(set(data)) def pre_pad_sequences_pytorch(sequences, max_len): padded_sequences = [] for seq in sequences: # If the sequence is shorter than max_len, pad with zeros at the beginning if len(seq) \u0026lt; max_len: padded_seq = [0] * (max_len - len(seq)) + seq # Pre-padding with 0 # If the sequence is longer than max_len, truncate it else: padded_seq = seq[-max_len:] padded_sequences.append(padded_seq) return torch.tensor(padded_sequences) X_train_padded = pre_pad_sequences_pytorch(X_train, max_len) X_train_padded=X_train_padded.unsqueeze(-1) Y_train=torch.tensor(Y_train) This creates a dataset ready to be set into LSTM network.\nFinally, the LSTM ##### Long Short-Term Memory Network Class ##### class LSTM: def __init__(self, vocab_size, embedding_dim, hidden_size, output_size): self.hidden_size = hidden_size #embedding layer self.embedding = torch.randn(vocab_size, embedding_dim, requires_grad=True) # Initialize weights with requires_grad=True #forget gate self.Wf = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bf = torch.zeros(hidden_size, requires_grad=True) #input gate self.Wi = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bi = torch.zeros(hidden_size, requires_grad=True) #candidate gate self.Wc = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bc = torch.zeros(hidden_size, requires_grad=True) #output gate self.Wo = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bo = torch.zeros(hidden_size, requires_grad=True) #final gate self.Wv = torch.randn(output_size, hidden_size, requires_grad=True) self.bv = torch.zeros(output_size, requires_grad=True) self._initialize_weights() This is a custom implementation of a Long Short-Term Memory (LSTM) network. It takes four inputs\nvocab_size,embedding_dim,hidden_size and output_size vocab_size is the number of unique words in your dataset.\nembedding_dim is the size of the word embeddings. Instead of representing words as simple indices (like 1, 2, 3, etc.), we represent them as dense vectors (e.g., [0.2, -0.5, 0.7, \u0026hellip;]).\nWhy embedding layer is needed ? Words are initially represented as indices (e.g., \u0026ldquo;cat\u0026rdquo; = 1, \u0026ldquo;dog\u0026rdquo; = 2, etc.). But these indices don’t carry any meaningful information about the words. The embedding layer converts these indices into dense vectors of size embedding_dim. These vectors are trainable, meaning the model will learn the best way to represent each word during training. For example, if embedding_dim =4, the word \u0026ldquo;cat\u0026rdquo; might be represented as a vector like [0.1, -0.3, 0.5, 0.9]. https://medium.com/analytics-vidhya/embeddings-the-what-the-why-and-the-how-15a6a3c99ce8 This blogs deeps dives into the working and importance of Embeddings layer.\nThe other function we’ll be defining is a Xavier Initialization. Recurrent neural networks are very sensitive to the initialization used, so choosing the right one is important. It’s probably enough to know that the Xavier Initialization is a good choice; however, we can take a look at the math here.\ndef _initialize_weights(self): nn.init.xavier_uniform_(self.Wf) nn.init.xavier_uniform_(self.Wi) nn.init.xavier_uniform_(self.Wc) nn.init.xavier_uniform_(self.Wo) nn.init.xavier_uniform_(self.Wv) def parameters(self): # Return a list of all parameters (weights and biases) in the model return [self.Wf, self.bf, self.Wi, self.bi, self.Wc, self.bc, self.Wo, self.bo, self.Wv, self.bv, self.embedding] The next function to define is the forward function.\ndef forward(self, x, init_states=None): # Apply embedding layer to input indices x=x.squeeze(dim=-1) x = self.embedding[x] batch_size, seq_len, _ = x.size() # Initialize h_t and c_t if init_states is None if init_states is None: h_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) c_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) else: h_t, c_t = init_states outputs = [] for t in range(seq_len): x_t = x[:, t, :] # Shape: (batch_size, embedding_dim) Z_t = torch.cat([x_t, h_t], dim=1) # Shape: (batch_size, embedding_dim + hidden_size) # Forget gate ft = torch.sigmoid(Z_t @ self.Wf.t() + self.bf) # Input gate it = torch.sigmoid(Z_t @ self.Wi.t() + self.bi) # Candidate cell state can = torch.tanh(Z_t @ self.Wc.t() + self.bc) # Output gate ot = torch.sigmoid(Z_t @ self.Wo.t() + self.bo) c_t = c_t * ft + can * it h_t = ot * torch.tanh(c_t) # Compute output for current time step y_t = h_t @ self.Wv.t() + self.bv return y_t, (h_t, c_t) When building a model for next-word prediction, the goal is to predict the most likely word that follows a given sequence. In this context, we don’t need outputs from every timestep of the sequence—only the final output (from the last timestep) is necessary to make the prediction.\nNow lets initialize our Model.\nWe’ll be using a hidden size of 128 and an embedding dimension of 10 for our model. For comparison, GPT-2 Small (with 117 million parameters) used an embedding dimension of 768, and the latest GPT-4 uses 16,384. Since we are working with a small dataset, a smaller embedding dimension should work, and I chose 10 . You can play with this value to see how it plays out.\nmodel = LSTM(vocab_size=vocab_size, embedding_dim=128, hidden_size=128, output_size=vocab_size) params = model.parameters() optimizer = torch.optim.Adam(params, lr=0.005) Training Loop hidden_state = None # Initialize hidden state for _ in range(500): # Sample a batch batch_indices = torch.randint(0, X_train_padded.shape[0], (128,)) x_train = X_train_padded[batch_indices] # Shape: (batch_size, seq_len) # Forward pass outputs,hidden_state = model.forward(x_train, init_states=hidden_state) # print(outputs.shape) h_t, c_t = hidden_state hidden_state = (h_t.detach(), c_t.detach()) # Detach hidden state for next batch # Compute loss y_batch = Y_train[batch_indices] # Shape: (batch_size, seq_len, vocab_size) loss = criterion(outputs, y_batch) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(loss) This training approach grabs a random batch of 128 data points, puts them into the model, and hopes for the best, kind of like cramming for an exam with random notes you found in your bag. While it’s not the most efficient way to train, I went with it to really wrap my head around how batches flow through the network. After each iteration, the hidden state is politely told, \u0026ldquo;Take a break, you’ve done enough,\u0026rdquo; and detached. This prevents PyTorch from becoming a hoarder and building a massive computation graph that would slow things down to a crawl. For optimization, I used the Adam optimizer. Though simple, this setup helped me grasp the inner workings of batching and hidden state management in LSTMs.\nBefore we begin training the network, let’s see how our model performs with its randomly initialized weights. This will give us a baseline idea of what kind of text it generates before learning anything meaningful from the data.\nThe following code preprocess the input and passes it to the model\ndef generate_sequence(model, seed_string, char2idx, idx2char, sequence_length, max_len=55): seed_indices = [char2idx[word] for word in seed_string.split(\u0026#34; \u0026#34;) if word in char2idx] seed_tensor = torch.tensor(seed_indices).unsqueeze(0) # Shape: (1, seq_len) generated_indices = seed_indices[:] hidden_state = None for _ in range(sequence_length): # Pre-pad the input sequence to match the model\u0026#39;s expected input size padded_input = pre_pad_sequences_pytorch([generated_indices], max_len).unsqueeze(-1) # Get the model output and hidden state output, (hidden_state) = model.forward(padded_input, hidden_state) # Take the output corresponding to the last token next_token_logits = output # Shape: (1, vocab_size) # Use softmax to get probabilities and sample the next token next_token_prob = torch.softmax(next_token_logits, dim=-1) # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item() next_token_idx=torch.argmax(next_token_prob).item() # Append the predicted token to the sequence generated_indices.append(next_token_idx) # Convert indices back to characters generated_words = [idx2char[idx] for idx in generated_indices] return \u0026#34; \u0026#34;.join(generated_words) # Example usage: seed_string = \u0026#34;scum\u0026#34; sequence_length = 20 generated_text = generate_sequence(model, seed_string, char2idx, idx2char, sequence_length) print(\u0026#34;Generated Text:\u0026#34;) print(generated_text) Now for a sequence_length of 20 this is what our model outputs.\nIt is.\u0026#39; if bath, is.\u0026#39; if bath, cockney side, black serve is.\u0026#39; go three asleep straw bath, is.\u0026#39; cotton when when This up apparatus kind where Majors tub a stripped eight Doesn’t makes any sense.\nNow Lets train the model and run this code again.\nNow I trained the model for 300 iteration, it started with an initial loss of 6.85 reaching to 0.1084 at the end of 300 iteration. So, our model learnt well from the data. Lets see how our model performs after training on the same seed string.\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to Much better . Lets generate a paragraph now .\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to make off and hide in a shed used for storing potatoes, together with some workhouse paupers who were skulking to avoid the Sunday-morning service. There was a stove burning there, and comfortable packing cases to sit on, and back numbers of the Family Herald, and even a copy of Raffles from the workhouse library. It was paradise after the grime, of Sunday. It appeared the Tramp Major served the than before and the hot pipes. The cotton blankets were almost useless. One spent the night in turning from side to side, falling asleep for ten minutes and waking half frozen, and watching for the medical tables. It was a gloomy, chilly, limewashed place, consisting only of a bathroom and dining-room and about a hundred narrow stone cells. The terrible Tramp Major met us at the door and herded us into the bathroom to be stripped and searched. He was a gruff, soldierly man of forty, a pound of bread, a bit of margarine, and a pint of so-called tea. It took us five minutes to gulp down the cheap, noxious food. Then the Tramp Major served us with three cotton blankets each, and drove us off to our cells for the night. The doors were locked on the outside a little before seven in the evening, and would stay locked for the next twelve twelve hours in the spike, told tales of mooching, of pints stood him in the boozers, of the night for giving a back answer. When You, came to be searched, he fair held you upside down and shook you. If you were caught with tobacco there was bell to. Pay, and if you went in with money (which is against the law) God help help had no socks, except the tramps had already washed to our about and and for the lot of spring, perhaps—the authorities little It of the water, and decided to go dirty for the queer habit of sleeping in his hat, grumbled about a parcel of tommy that he had lost on the toad. Bill the moocher, the best built man of But it looks like our small model memorized the paragraph instead of learning any real patterns. This is a case of overfitting. Since we trained the model for 300 epochs on such a small dataset, it essentially “rote learned” the paragraph and just outputs the exact same text from the dataset\nIn generating text, we initially used argmax with torch.argmax(next_token_prob).item(), which simply picks the word with the highest probability from the output distribution. This approach is deterministic, meaning it will always choose the same word if the probabilities don\u0026rsquo;t change.\nNow, let’s try using a multinomial distribution instead. A multinomial distribution creates a probability distribution over all possible tokens, allowing us to sample the next token randomly based on the probabilities. Instead of always choosing the word with the highest probability, the multinomial distribution introduces some randomness by selecting tokens based on their likelihood.\nFor this uncomment the line # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item().\nThis method introduces variations in the text output, as the model can now generate different sequences even when given the same starting point. It\u0026rsquo;s like giving the model a little creative freedom, kind of like letting it freestyle instead of sticking to a strict script.\nThat wraps up this blog. Here, we generated text sequences using a trained LSTM model. I know there’s a lot of room for improvement, but the goal was to get a basic understanding of data preprocessing and how LSTMs work. For more structured and efficient code, it would make sense to use PyTorch’s nn.LSTM module.\nWhat\u0026rsquo;s Next? To improve this model:\nAdd Dropout: Prevent overfitting with regularization techniques. Use Better Sampling: Replace the random batch sampling with more structured approaches like sequential data loading. Increase Dataset Size: Larger datasets will yield more meaningful insights during training. Thanks for sticking around till the end! This being my first blog, I’m sure it’s not perfect, but I’ll work on improving my writing in the future. Appreciate you reading through!\nHere’s the GitHub repository with the code and dataset used for this project. Feel free to check it out!\nResources The Long Short-Term Memory (LSTM) Network from Scratch | Medium Implementing a LSTM from scratch with Numpy - Christina Kouridi Building makemore Part 2: MLP ","permalink":"http://localhost:1313/prtfoio/blog/lstm-from-scratch/","summary":"\u003ch1 id=\"lstm-from-scratch\"\u003eLSTM from Scratch\u003c/h1\u003e\n\u003cp\u003eIn this post, we will implement a simple next word predictor LSTM from scratch using torch.\u003c/p\u003e\n\u003ch2 id=\"a-gentle-introduction-to-lstm\"\u003eA gentle Introduction to LSTM\u003c/h2\u003e\n\u003cp\u003eLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by \u003ca href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\"\u003eHochreiter \u0026amp; Schmidhuber (1997)\u003c/a\u003e. As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\u003c/p\u003e","title":"Implementing LSTM from scratch in PyTorch step-by-step."},{"content":"Jenkins 🚀 What is Jenkins? Jenkins is an open-source automation server used primarily for continuous integration (CI) and continuous deployment (CD).\nContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a Jenkins pipeline to automate CI/CD for a project.\nIn this Blog, I’ll cover:\nJenkins architecture Writing a Jenkins pipeline for automating CI/CD Setting up webhook to trigger deployments automatically. Jenkins Architecture Jenkins follows a master-agent setup (formerly called master-slave), where the Jenkins server (master) manages everything, and agents (slaves) do the actual work.\nJenkins Server (Master)\nManages jobs, users, security, and plugins. Schedules tasks and decides which agent should run them. Provides the web interface where you monitor and control everything. Jenkins Agent (Slave)\nRun the actual build and deployment tasks. Communicate with the master to receive instructions. Report back with logs and results. 💡Note: Since we\u0026rsquo;re running Jenkins locally, both the server and agent are on the same machine. The server manages jobs and the web interface, while the agent executes tasks—everything happens on our local system.\n📝 Jenkinsfile A Jenkinsfile defines the automation process using a declarative syntax. It tells Jenkins what to do, when to do it, and how to handle failures This is a Jenkins pipeline file written in Declarative Pipeline Syntax.\npipeline { agent any stages { stage(\u0026#39;Checkout\u0026#39;) { steps { checkout scm } } stage(\u0026#39;Cleanup\u0026#39;) { steps { script { // Stop and remove existing containers sh \u0026#39;\u0026#39;\u0026#39; docker-compose down # Additional cleanup in case docker-compose down didn\u0026#39;t work docker rm -f mysql-db || true \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Start MySQL\u0026#39;) { steps { script { // Start only MySQL sh \u0026#39;docker-compose up -d db\u0026#39; // Wait for 1 minute sh \u0026#39;sleep 60\u0026#39; echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; } } } stage(\u0026#39;Start Other Services\u0026#39;) { steps { script { // Start remaining services sh \u0026#39;docker-compose up -d\u0026#39; } } } } post { failure { sh \u0026#39;docker-compose down\u0026#39; echo \u0026#39;Deployment failed\u0026#39; } success { echo \u0026#39;Deployment successful\u0026#39; } } } Understanding Jenkinsfile 1. Checkout Stage: It Retrieves the code from the repository (using SCM — Source Code Management) so that the pipeline can work with the latest code. This command checks out the repository, ensuring that the pipeline works with the latest version of the code.\ncheckout scm 2. Cleanup Stage: It Stops and removes any existing containers from a previous deployment to ensure a clean environment before starting fresh\ndocker-compose down docker rm -f mysql-db || true docker-compose down stops all services defined in the docker-compose.yml and removes the containers. docker rm -f mysql-db || true forces the removal of the MySQL container (mysql-db), even if it\u0026rsquo;s not running. The || true ensures the pipeline doesn\u0026rsquo;t fail if the container doesn\u0026rsquo;t exist. 3. Start MySQL Stage: It Starts the MySQL container using docker-compose and waits for it to initialize before starting other services.\ndocker-compose up -d db sleep 60 echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; docker-compose up -d db starts the MySQL container (db service) in detached mode (d), allowing the pipeline to continue without waiting for MySQL to be ready. sleep 60 pauses the pipeline for 1 minute, giving MySQL time to start and initialize. 4.Start Other Services Stage: Starts all the remaining services defined in the docker-compose.yml (other than MySQL, which was started separately in the previous stage).This command starts the remaining services (e.g., frontend, backend) in detached mode.\ndocker-compose up -d 5. Post Section: Defines steps that should be executed after the pipeline has completed, regardless of success or failure.\nIf the deployment fails, the docker-compose down command is executed to clean up containers, and a failure message is printed:\ndocker-compose down echo \u0026#39;Deployment failed\u0026#39; If the deployment is successful, a success message is printed:\necho \u0026#39;Deployment successful\u0026#39; 🔧 Setting Up Jenkins on Ubuntu Follow this to install Jenkins in Ubuntu.\n1. Add the Jenkins Repository Key Run the following command to download the Jenkins repository key and store it:\nsudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\ https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key 2. Add the Jenkins Repository Now, add the Jenkins repository to your package sources list using the key you just downloaded:\necho \u0026#34;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\u0026#34; \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null 3. Update Package Lists Next, update your package list to include the newly added Jenkins repository:\nsudo apt-get update 4. Install Jenkins Now, install Jenkins using the following command:\nsudo apt-get install jenkins 5. Start Jenkins After the installation completes, start Jenkins:\nsudo systemctl start jenkins Visit for web interface.\nhttp://localhost:8080/ After you login to Jenkins, - Run the command to copy the Jenkins Admin Password - sudo cat /var/lib/jenkins/secrets/initialAdminPassword - Enter the Administrator password\n🐳 Grant Docker Permissions to Jenkins Grant Jenkins User permission to use docker daemon. Jenkins User is automatically created when installing Jenkins.\nsudo su - usermod -aG docker jenkins systemctl restart docker Now switch to Jenkins and see if Docker is running inside it\nsu - jenkins docker run hello-world Great. Now lets setup Jenkins from web.\nInstall Docker plugin inside Jenkins to run docker as agent.\nAfter successful installation restart the Jenkins. You can restart by simply adding /restart in URL.\nCreating Jenkins Pipeline For this project I\u0026rsquo;m using this repo. It has a frontend, backend, and SQL database—a simple to-do list app. The frontend calls the backend, and the backend interacts with the database to fetch, add, and delete list items.\nClick on Pipeline . Here we’ll have Jenkins file in our repo so Jenkins will fetch the file from there.\nNow select on apply and Save.\nGo to dashboard and start the build.\nBuild Success Lets check our application running on port 8000.\nSo this verifies our task running smoothly that is deployed using Jenkins.\nSetting Up Webhook Let\u0026rsquo;s set up a webhook in Jenkins and GitHub to enable a seamless CI process. When the webhook is enabled, GitHub sends an HTTP POST request to Jenkins with information about the push event. Jenkins listens for this request and triggers the configured pipeline to run, deploying the updated code.\nTo make Jenkins accessible to GitHub, we need to expose Jenkins running on localhost to the public. Since GitHub needs to send an HTTP POST request to a public IP, we’ll use ngrok for port forwarding. Ngrok creates a secure tunnel to your local machine, allowing GitHub to access Jenkins through a public UR\nSetting up ngrok Go to ngrok website and follow the instruction provided there for WSL.\nInstall ngrok via Apt with the following command:\ncurl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\ | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc \u0026gt;/dev/null \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://ngrok-agent.s3.amazonaws.com buster main\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/ngrok.list \\ \u0026amp;\u0026amp; sudo apt update \\ \u0026amp;\u0026amp; sudo apt install ngrok Run the following command to add your authtoken to the default ngrok.yml\nngrok config add-authtoken \u0026lt;your_auth_token\u0026gt; Now run the command:\nngrok http http://localhost:8080 The command ngrok http http://localhost:8080 creates a secure tunnel to your local Jenkins server running on port 8080, exposing it to the public internet via a unique ngrok URL.\nNow we have can access our port 8080 from any device with the given ngrok URL.\nSetting Up Web Hook Create a new pipeline and In trigger option select this option.\nGo to your GitHub repo\u0026rsquo;s Settings \u0026gt; Webhooks \u0026gt; Add webhook to configure the webhook.\nNow add URL provided by ngrok appended with /github-webhook.\nThen click on Add Webhook.\nLet\u0026rsquo;s make a change in our code and push it to GitHub to test if the webhook works:\nLets commit the changes and push it to Github.\nAs soon as i push the changes to GitHub the webhook triggers the Jenkins and runs our pipeline.\nBuild 4 is successful. Now lets check if our changes has been implemented or not.\nHere we can see our changes has been implemented.\nIn this blog, we wrote a Jenkins pipeline and set up a webhook to automate the CI/CD process in our local environment. To make Jenkins accessible to GitHub, we used ngrok to tunnel into our local machine. After pushing code to the repository, we observed Jenkins automatically triggering and running the pipeline, handling the deployment process without any manual intervention.\nThe repo used for this blog is https://github.com/agrimpaneru/dockerized-todo. Feel free to fork it and experiment with it.\n","permalink":"http://localhost:1313/prtfoio/blog/jenkins-webhooks-cicd/","summary":"\u003ch1 id=\"jenkins\"\u003eJenkins\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"-what-is-jenkins\"\u003e🚀 \u003cstrong\u003eWhat is Jenkins?\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eJenkins is an open-source automation server used primarily for \u003cstrong\u003econtinuous integration (CI) and continuous deployment (CD)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e to automate CI/CD for a project.\u003c/p\u003e\n\u003cp\u003eIn this Blog, I’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJenkins architecture\u003c/li\u003e\n\u003cli\u003eWriting a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e for automating CI/CD\u003c/li\u003e\n\u003cli\u003eSetting up webhook to trigger deployments automatically.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"jenkins-architecture\"\u003e\u003cstrong\u003eJenkins Architecture\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image%201.png\"\u003e\u003c/p\u003e","title":"Integrating Jenkins with Webhooks for Efficient CI/CD Automation"},{"content":"LMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\nIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u0026ldquo;Attention Is All You Need\u0026rdquo; shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\nEmbeddings representing the Words as vectors Initially the words are converted into tokens, Here we assume tokens are equal to index.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] Output for this code is :\n[tensor([0.8823, 0.9150, 0.3829]), tensor([0.9593, 0.3904, 0.6009]), tensor([0.2566, 0.7936, 0.9408]), tensor([0.1332, 0.9346, 0.5936]), tensor([0.8694, 0.5677, 0.7411])] Here the corresponding Embeddings for each word is initialized randomly. Here we represented each word as 3 dimensional embedding.\n{\u0026#39;the\u0026#39;: tensor([0.8823, 0.9150, 0.3829]), \u0026#39;cat\u0026#39;: tensor([0.9593, 0.3904, 0.6009]), \u0026#39;sat\u0026#39;: tensor([0.2566, 0.7936, 0.9408]), \u0026#39;on\u0026#39;: tensor([0.1332, 0.9346, 0.5936]), \u0026#39;mat\u0026#39;: tensor([0.8694, 0.5677, 0.7411])} Attention Mechanism Attention Mechanism is inspired by how humans focus on specific parts of information when processing it—just like when we read a sentence, we don\u0026rsquo;t focus on every word equally. Instead, we \u0026ldquo;attend\u0026rdquo; to the most relevant words that help us understand the meaning.\nLets take an example for understanding attention mechanism:\nImagine reading the sentence: \u0026ldquo;The cat sat on the mat.\u0026rdquo;\nIf asked, \u0026ldquo;Where is the cat?\u0026rdquo;, attention would primarily focus on the word \u0026ldquo;cat\u0026rdquo; and the word \u0026ldquo;mat\u0026rdquo;. Other words like \u0026ldquo;the\u0026rdquo; or \u0026ldquo;on\u0026rdquo; are ignored since they don’t carry much relevance to the question.\nLets take a sentence:\nI am going to cinema to watch ………?\nThe most probable answers would be \u0026ldquo;movie,\u0026rdquo; \u0026ldquo;action movie,\u0026rdquo; or something similar. Words like \u0026ldquo;book\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; don\u0026rsquo;t fit the context and are irrelevant to predicting the correct word. The key to predicting the correct word is in the context and information provided in the preceding words. To make accurate predictions, we need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the model to focus on the most relevant words to that context to make predictions.\nUnderstanding Scaled Dot-Product Attention The paper Attention Is All You Need introduced Scaled Dot-Product Attention .\nLets first calculate the Similarity between Query and Key\nThis is done as\nQ = K = V = embeddings similarity_matrix=torch.matmul(Q,K.T) In our case the dimension of output is 5*5.\nSimilarity Matrix:\ntensor([[1.7622, 1.4337, 1.3127, 1.1999, 1.5702], [1.4337, 1.4338, 1.1213, 0.8494, 1.5010], [1.3127, 1.1213, 1.5807, 1.3343, 1.3708], [1.1999, 0.8494, 1.3343, 1.2435, 1.0863], [1.5702, 1.5010, 1.3708, 1.0863, 1.6274]]) Here’s each value represents the similarity between words as described below.\nthe cat sat on mat the the-the the-cat the-sat the-on the-mat cat cat-the cat-cat cat-sat cat-on cat-mat sat sat-the sat-cat sat-sat sat-on sat-mat on on-the on-cat on-sat on-on on-mat mat mat-the mat-cat mat-sat mat-on mat-mat The similarity matrix captures the similarities between the query and key vectors. Higher similarity means the model should focus more on that word when making predictions, while lower similarity means the word is less relevant.\nScaling the compatibility matrix In the second step, we scale the dot-product of the query and key vectors by a factor of,\n$$ \\frac{1}{d_k} $$ . In our case We’ve only taken dimension of 3.\nHowever, when the dimension d is very large, dot-product attention without this scaling factor performs poorly. The authors of the paper “Attention Is All You Need” attribute this poor performance to having large variance.\nAs d_k increases, the variance of the dot product also becomes larger, leading to some values becoming extremely small after being passed through the softmax function. This causes issues during gradient calculation, similar to the vanishing gradient problem, where the gradients become too small to effectively update the model. To mitigate this, the dot product is scaled by $$ \\sqrt{\\frac{1}{d_k}} $$\nbefore applying softmax , which stabilizes the training process and improves performance.\nTo visualize this effect lets take output from softmax before and after scaling for embedding dimension of 256.\nApplying Softmax Activation The softmax function is used to convert the logits into probabilities. It takes a vector of raw scores (logits) and transforms them into a probability distribution. The softmax function is defined as:\n$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\nSoftmax is applied pointwise, so it doesn\u0026rsquo;t change the dimension of the input vector.\nimport torch.nn.functional as F similarity_matrix_scaled=similarity_matrix/(1/torch.sqrt(torch.tensor(3))) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) softmax_similarity_matrix_scaled Its output is:\ntensor([[0.2372, 0.1962, 0.1830, 0.1714, 0.2123], [0.2179, 0.2180, 0.1820, 0.1555, 0.2266], [0.1957, 0.1752, 0.2285, 0.1982, 0.2024], [0.2058, 0.1681, 0.2224, 0.2110, 0.1927], [0.2154, 0.2070, 0.1920, 0.1629, 0.2227]]) It can be visualized in Heatmap as\nComputing the Context Vector as final output. So for final step We multiply the attention matrix from previous step and multiply it with the Value matrix V. The final product gives the new context vector for all the words in vocabulary in our case 5.\nnew_context=torch.matmul(softmax_similarity_matrix_scaled,V) new_context Output:\ntensor([[0.6518, 0.7195, 0.6399], [0.6658, 0.7029, 0.6459], [0.6018, 0.7289, 0.6628], [0.5955, 0.7371, 0.6571], [0.6532, 0.7090, 0.6492]]) Conclusion In this section, we saw how contextual embeddings are generated using self-attention. We also discussed the importance of scaling and how to implement a simple self-attention block. Notably, there were no trainable parameters involved here, and we treated the Query (Q), Key (K), and Value (V) matrices as all being the same.\nHowever, to make the attention block trainable, we need to introduce learnable weights when deriving the Q, K, and V vectors from the original embedding. These weights will allow the model to adjust and fine-tune the attention mechanism during training. In the next section, we\u0026rsquo;ll explore how to implement this by adding trainable parameters to the attention block\nAttention Mechanism with trainable Parameters Alright! Now that we understand how self-attention is calculated, let\u0026rsquo;s take it a step further by introducing trainable parameters so that the model can learn how to find the context of words effectively.\nAdding Trainable Parameters to Self-Attention Q (Query), K (Key), and V (Value) matrices are not just direct copies of word embeddings. Instead, they are learned transformations of these embeddings.\nTo achieve this, we introduce three trainable weight matrices:\nW_q (Query weight matrix): Learns how to project word embeddings into the query space. W_k (Key weight matrix): Learns how to project word embeddings into the key space. W_v (Value weight matrix): Learns how to project word embeddings into the value space. Each of these matrices will be optimized during training.\nW_q=torch.randn(embedding_dim,embedding_dim) W_v=torch.randn(embedding_dim,embedding_dim W_k=torch.randn(embedding_dim,embedding_dim) Note that the dimension of these matrix can be made different. However the first dimension must be equal to embedding dimension to satisfy the matrix multiplication condition. Changing the second dimension implies projecting the embedding dimension from one dimension to other .\nLets get the new Query, Key and Value vector.\nQ = torch.matmul(embeddings, W_q) # (5, 3) K = torch.matmul(embeddings, W_k) # (5, 3) V = torch.matmul(embeddings, W_v) # (5, 3) Then we calculate the self attention similar to how we did above.\nsimilarity_matrix = torch.matmul(Q, K.T) # (5, 5) similarity_matrix_scaled = similarity_matrix / torch.sqrt(torch.tensor(embedding_dim, dtype=torch.float32)) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(softmax_similarity_matrix_scaled, V) This is what the final flow looks like.\nHere Lets wrap up this in Class Based Implementation\nimport torch import torch.nn.functional as F class SelfAttention: def __init__(self, embedding_dim): torch.manual_seed(42) self.embedding_dim = embedding_dim # Initialize weight matrices self.W_q = torch.randn(embedding_dim, embedding_dim) self.W_k = torch.randn(embedding_dim, embedding_dim) self.W_v = torch.randn(embedding_dim, embedding_dim) def forward(self, embeddings): # Compute Query, Key, and Value matrices Q = torch.matmul(embeddings, self.W_q) K = torch.matmul(embeddings, self.W_k) V = torch.matmul(embeddings, self.W_v) # Compute similarity (dot product attention) similarity_matrix = torch.matmul(Q, K.T) # (num_words, num_words) # Scale by sqrt(embedding_dim) similarity_matrix_scaled = similarity_matrix / torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32) ) # Apply softmax to get attention weights attention_weights = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(attention_weights, V) return new_context, attention_weights embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] # Generate random embeddings for words embeddings = torch.stack([torch.rand(embedding_dim) for _ in words]) # Initialize attention mechanism self_attention = SelfAttention(embedding_dim) # Forward pass to compute attention new_context, attention_weights = self_attention.forward(embeddings) print(\u0026#34;New Context Vectors:\\n\u0026#34;, new_context) print(\u0026#34;\\nAttention Weights:\\n\u0026#34;, attention_weights) Output:\nNew Context Vectors: tensor([[ 0.3689, -0.4890, 1.3677], [ 0.2569, -0.4673, 1.3297], [ 0.3284, -0.4752, 1.3515], [ 0.1971, -0.4685, 1.3124], [ 0.3061, -0.4846, 1.3491]]) Attention Weights: tensor([[0.4672, 0.0538, 0.0879, 0.0829, 0.3082], [0.3362, 0.1266, 0.1921, 0.1077, 0.2375], [0.4212, 0.0851, 0.1492, 0.0815, 0.2631], [0.2728, 0.1547, 0.1886, 0.1526, 0.2314], [0.3922, 0.0832, 0.1179, 0.1143, 0.2923]]) So this ends up this blog. Congratulations on making it to the end. Feel free to experiment with different dimensions and input data to see how the self-attention mechanism adapts and scales.\nReferences Implementing Self-Attention from Scratch in PyTorch | by Mohd Faraaz | Medium\nUnderstanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\nSelf Attention in Transformers | Deep Learning | Simple Explanation with Code!\nSelf-attention from scratch | Blogs by Anil\n","permalink":"http://localhost:1313/prtfoio/blog/self-attention-pytorch/","summary":"\u003cp\u003eLMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\u003c/p\u003e\n\u003cp\u003eIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u003cem\u003e\u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/em\u003e shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\u003c/p\u003e","title":"Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch"},{"content":"Implementing Multihead attention from scratch with pytorch In our previous article, we built Self-Attention from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\nNow, let’s take things a step further and implement Multi-Head Attention from scratch. This post focuses more on the implementation rather than the theory, so I assume you’re already familiar with how self-attention works.\nLet’s get started!\nMultihead Attention Similar to Self-Attention, Multi-Head Attention also creates a new context vector. But instead of using a single attention mechanism, it uses multiple heads, each learning different relationships between words.\nFor example, consider these two sentences:\n1️⃣ I went to the bank to withdraw cash.\n2️⃣ I went for a picnic by the river bank.\nHere, the word \u0026ldquo;bank\u0026rdquo; has different meanings depending on the context. The idea behind Multi-Head Attention is that different heads can focus on different aspects of a word’s meaning, capturing various relationships more effectively.\nIn the above example with a single attention head, the model might have difficulty capturing both the financial and geographical meanings of the word \u0026ldquo;bank\u0026rdquo; at the same time. Multi-Head Attention, on the other hand, allows the model to learn both meanings separately—one head could focus on financial context while another focuses on the geographical context. This way, the model learns richer, more nuanced embeddings of the word \u0026ldquo;bank\u0026rdquo; based on different contexts.\nFor a more intuitive explanation of Multi-Head Attention, I highly recommend checking out this medium post.\nNow Lets Implement it.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 8 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) Similar to our previous Self-Attention implementation, we’ll use the following sentence with a random embedding of dimension 8. However, in real-world applications, embeddings are typically a combination of word embeddings and positional encodings. To keep things simple, we won’t dive into that here. Now, let’s define the Wq, Wk, and Wv matrices. We could use separate smaller weight matrices to get the Query, Key, and Value vectors directly, but a more efficient approach is to use a single large weight matrix and split it afterward for multiple attention heads. This method is more computationally efficient, as it allows for parallel processing across heads.\nWq = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wk = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wv = torch.rand(embedding_dim, embedding_dim) # (8, 8) Lets apply the projection to Get Query Key and Value matrix.\n# Apply projection Q = embeddings @ Wq # Shape: (5, 8) K = embeddings @ Wk # Shape: (5, 8) V = embeddings @ Wv # Shape: (5, 8) Now ,We’ll split the projected vectors into multiple heads . Remember the Embedding dimension must be exactly divisible by head\nd_k = embedding_dim // num_heads Now we obtain the Query Key and Value Matrix for multiple heads.\nQ = Q.view(seq_len, num_heads, d_k).transpose(0, 1) K = K.view(seq_len, num_heads, d_k).transpose(0, 1) V = V.view(seq_len, num_heads, d_k).transpose(0, 1) The output shape of Q, K, and V after the split will be (2, 5, 4), where:\n2 corresponds to the number of heads, 5 corresponds to the sequence length (number of words), and 4 is the new dimension of each vector for each head. We calculate attention matrix by,\nattention_scores=Q @ K.transpose(-2,-1) \u0026#39;\u0026#39;\u0026#39;For 3D matrix multiplication, the first dimension of both matrices should match, #while the second dimension of the first matrix must align with the third dimension of the second matrix according to the matrix multiplication rule\u0026#39;\u0026#39;\u0026#39; Here, we get two attention scores from each head. We’ll proceed with scaling these scores and applying the softmax activation, just like in Self-Attention. For a detailed explanation of this process, please refer to my previous post on Self-Attention\nBut before applying the scaling and softmax activation, we’ll mask the attention scores. Masking is necessary to ensure that the prediction for a given token only depends on the current and previous tokens. Without masking, the model could \u0026ldquo;cheat\u0026rdquo; by looking ahead at future tokens which in not desirable.\nMasked Multi-Head Attention In the decoder of a transformer, we use masked multi-head attention to prevent the model from looking at future tokens when predicting the next one. This ensures that the model can only base its prediction on past tokens, not the future ones. In contrast, the encoder doesn’t need masking because it has access to the entire sequence at once, so it can freely attend to all parts of the input.\nThis masking enables the model to process all tokens in parallel during training, enhancing efficiency.\nWe will implement Masked Multi-Head Attention, ensuring that the model only attends to past words by masking the attention scores. This prevents the decoder from seeing future words, enforcing an autoregressive learning process\nMasking is applied before softmax by setting forbidden positions to−∞ to ensure they receive zero probability after normalization.\nmask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) mask = mask.bool() mask=mask.unsqueeze(0) attention_scores=attention_scores.masked_fill(mask,-1e11) attention_scores=attention_scores/math.sqrt(d_k) #This is mask i.e Upper triangular matrix tensor([[0., 1., 1., 1., 1.], [0., 0., 1., 1., 1.], [0., 0., 0., 1., 1.], [0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.]]) #Attention Scores before masking tensor([[[24.6185, 22.6470, 19.6726, 10.3703, 23.2266], [25.3424, 23.5643, 20.2438, 10.8568, 24.0848], [20.2856, 18.6674, 16.2010, 8.5272, 19.1661], [11.0366, 10.1522, 8.8451, 4.5771, 10.3952], [23.4003, 21.7570, 18.6598, 10.0209, 22.2936]], [[24.1949, 25.1107, 20.5201, 12.3558, 24.2538], [24.3185, 25.1608, 20.5903, 12.5245, 24.4242], [19.3101, 20.0426, 16.3541, 9.9390, 19.3887], [11.4448, 11.6265, 9.6817, 5.8675, 11.3187], [23.9947, 24.8808, 20.3205, 12.3524, 24.1055]]]) #Attention Scores after masking tensor([[[ 2.4619e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.5342e+01, 2.3564e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.0286e+01, 1.8667e+01, 1.6201e+01, -1.0000e+11, -1.0000e+11], [ 1.1037e+01, 1.0152e+01, 8.8451e+00, 4.5771e+00, -1.0000e+11], [ 2.3400e+01, 2.1757e+01, 1.8660e+01, 1.0021e+01, 2.2294e+01]], [[ 2.4195e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.4318e+01, 2.5161e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 1.9310e+01, 2.0043e+01, 1.6354e+01, -1.0000e+11, -1.0000e+11], [ 1.1445e+01, 1.1626e+01, 9.6817e+00, 5.8675e+00, -1.0000e+11], [ 2.3995e+01, 2.4881e+01, 2.0321e+01, 1.2352e+01, 2.4105e+01]]]) So what we did here was created a lower triangular matrix and applied it as a mask to attention score and scaled it and passed to softmax to ensure each token can only attend to itself and previous tokens.\nAfter softmax, the row sum will be one and this ensures the attention weights are a valid probability distribution, allowing the model to learn proportional contributions from past tokens.\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5130, 0.4870, 0.0000, 0.0000, 0.0000], [0.3420, 0.3315, 0.3265, 0.0000, 0.0000], [0.2540, 0.2510, 0.2486, 0.2465, 0.0000], [0.2029, 0.2000, 0.1984, 0.1980, 0.2007]], [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.4935, 0.5065, 0.0000, 0.0000, 0.0000], [0.3342, 0.3391, 0.3267, 0.0000, 0.0000], [0.2521, 0.2528, 0.2485, 0.2466, 0.0000], [0.2006, 0.2022, 0.1984, 0.1981, 0.2007]]]) Here is what out attention scores looks like.\nNow we multiply out Attention score with Value matrix and concatenate it,\noutput=attention_scores@V output.transpose(0,1).reshape(seq_len,embedding_dim) After Concatenation we achieve the final Contextual embedding of each vectors.\nHere\u0026rsquo;s a cleaner and more efficient implementation of the Multi-Head Attention module in PyTorch, wrapped in a class:\nimport torch import math class MultiHeadAttention(torch.nn.Module): def __init__(self, num_heads, embedding_dim): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.embedding_dim = embedding_dim self.d_k = embedding_dim // num_heads self.Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) def forward(self, embeddings): seq_len = embeddings.size(0) Q = embeddings @ self.Wq # (seq_len, embedding_dim) K = embeddings @ self.Wk V = embeddings @ self.Wv # Converting to multiheaded attention Q = Q.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # (num_heads, seq_len, d_k) K = K.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) V = V.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # Compute attention scores attention_scores = torch.matmul(Q, K.transpose(-2, -1)) # (num_heads, seq_len, seq_len) # Apply mask (upper triangular mask for causal attention) mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() mask = mask.unsqueeze(0).expand_as(attention_scores) attention_scores = attention_scores.masked_fill(mask, -1e11) # Scale the attention scores attention_scores = attention_scores / math.sqrt(self.d_k) # Apply softmax to get attention weights attention_weights = torch.softmax(attention_scores, dim=-1) # (num_heads, seq_len, seq_len) # Compute the output (weighted sum of values) output = torch.matmul(attention_weights, V) output = output.transpose(0, 1).contiguous().view(seq_len, self.embedding_dim) return output # Example usage torch.manual_seed(42) embedding_dim = 8 num_heads = 2 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) mha = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim) # Forward pass through the model output = mha(embeddings) print(\u0026#34;Output shape:\u0026#34;, output.shape) ","permalink":"http://localhost:1313/prtfoio/blog/multi-head-attention-pytorch/","summary":"\u003ch1 id=\"implementing-multihead-attention-from-scratch-with-pytorch\"\u003eImplementing Multihead attention from scratch with pytorch\u003c/h1\u003e\n\u003cp\u003eIn our \u003ca href=\"https://agrimpaneru.com.np/blog/self-attention-pytorch/\"\u003eprevious article\u003c/a\u003e, we built \u003cstrong\u003eSelf-Attention\u003c/strong\u003e from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\u003c/p\u003e\n\u003cp\u003eNow, let’s take things a step further and implement \u003cstrong\u003eMulti-Head Attention\u003c/strong\u003e from scratch. This post focuses more on the \u003cstrong\u003eimplementation\u003c/strong\u003e rather than the theory, so I assume you’re already familiar with how self-attention works.\u003c/p\u003e\n\u003cp\u003eLet’s get started!\u003c/p\u003e","title":"MultiHead Attention Explained:Implementing Masked Multihead attention from Scratch in PyTorch"},{"content":"LSTM from Scratch In this post, we will implement a simple next word predictor LSTM from scratch using torch.\nA gentle Introduction to LSTM Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997). As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\nThe LSTM model we’re gonna implement follows this architecture.\nTo learn about their detailed structure here is a reference to this awesome blog Understanding LSTM Networks \u0026ndash; colah\u0026rsquo;s blog.\nFor simplified architecture:\nImports Lets import the required library\nimport torch import torch.nn as nn import torch.nn.functional as F Data Preparation We’ll be using autoregressive sentence generation for our dataset. This approach involves predicting the next word in a sequence based on the words that came before it.\nConsider a sentence,\n\u0026ldquo;I am Peter the hero.\u0026rdquo;\nInput Sequence Target Output [I] AM [I, AM] Peter [ I, AM, Peter] THE [I, AM, Peter, THE] HERO We\u0026rsquo;ll be working with George Orwell\u0026rsquo;s essay The Spike as our dataset. Since the dataset is relatively small, it allows us to focus on developing a deeper intuition without getting overwhelmed by the volume of data.\nTokenization Next, I\u0026rsquo;ll create a dictionary to convert words into indices—a common practice in Natural Language Processing (NLP). Instead of relying on libraries, I\u0026rsquo;ll build it from scratch to better understand how it works.\nchar2idx={char:i for i,char in enumerate(set(data))} # char2idx[\u0026#39;.\u0026#39;]=0 idx2char={i:char for i,char in enumerate(set(data))} We can run char2idx to view the word mapping to indices.\nx=[] for i in orgi.split(\u0026#34;\\n\u0026#34;): x.append([char2idx[j] for j in i.split(\u0026#34; \u0026#34;)]) This snippet processes a text dataset to convert words into their corresponding indices using the char2idx dictionary we created earlier.\nX_train = [] # List to store input sequences Y_train = [] # List to store corresponding outputs for sequence in x: for i in range(1, len(sequence)-1): # Input is the subsequence from start to the ith element X_train.append(sequence[:i]) # Output is the ith element (next token) Y_train.append(sequence[i]) This code prepares the dataset for autoregressive sentence generation, a technique where the model predicts the next token in a sequence based on previous tokens\nFor example, if the sequence is [1, 2, 3, 4], the resulting pairs would look like this:\nInput: [1], Output: 2 Input: [1, 2], Output: 3 Input: [1, 2, 3], Output: 4 Since LSTMs require input sequences of the same length, I padded all sequences to a fixed maximum length. I chose to pad at the beginning of the sequence because it preserves the actual data at the end, which is where the LSTM focuses more when processing the sequence. This ensures that the more meaningful tokens remain closer to the output\nmax_len=max([len(i) for i in X_train]) vocab_size=len(set(data)) def pre_pad_sequences_pytorch(sequences, max_len): padded_sequences = [] for seq in sequences: # If the sequence is shorter than max_len, pad with zeros at the beginning if len(seq) \u0026lt; max_len: padded_seq = [0] * (max_len - len(seq)) + seq # Pre-padding with 0 # If the sequence is longer than max_len, truncate it else: padded_seq = seq[-max_len:] padded_sequences.append(padded_seq) return torch.tensor(padded_sequences) X_train_padded = pre_pad_sequences_pytorch(X_train, max_len) X_train_padded=X_train_padded.unsqueeze(-1) Y_train=torch.tensor(Y_train) This creates a dataset ready to be set into LSTM network.\nFinally, the LSTM ##### Long Short-Term Memory Network Class ##### class LSTM: def __init__(self, vocab_size, embedding_dim, hidden_size, output_size): self.hidden_size = hidden_size #embedding layer self.embedding = torch.randn(vocab_size, embedding_dim, requires_grad=True) # Initialize weights with requires_grad=True #forget gate self.Wf = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bf = torch.zeros(hidden_size, requires_grad=True) #input gate self.Wi = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bi = torch.zeros(hidden_size, requires_grad=True) #candidate gate self.Wc = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bc = torch.zeros(hidden_size, requires_grad=True) #output gate self.Wo = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bo = torch.zeros(hidden_size, requires_grad=True) #final gate self.Wv = torch.randn(output_size, hidden_size, requires_grad=True) self.bv = torch.zeros(output_size, requires_grad=True) self._initialize_weights() This is a custom implementation of a Long Short-Term Memory (LSTM) network. It takes four inputs\nvocab_size,embedding_dim,hidden_size and output_size vocab_size is the number of unique words in your dataset.\nembedding_dim is the size of the word embeddings. Instead of representing words as simple indices (like 1, 2, 3, etc.), we represent them as dense vectors (e.g., [0.2, -0.5, 0.7, \u0026hellip;]).\nWhy embedding layer is needed ? Words are initially represented as indices (e.g., \u0026ldquo;cat\u0026rdquo; = 1, \u0026ldquo;dog\u0026rdquo; = 2, etc.). But these indices don’t carry any meaningful information about the words. The embedding layer converts these indices into dense vectors of size embedding_dim. These vectors are trainable, meaning the model will learn the best way to represent each word during training. For example, if embedding_dim =4, the word \u0026ldquo;cat\u0026rdquo; might be represented as a vector like [0.1, -0.3, 0.5, 0.9]. https://medium.com/analytics-vidhya/embeddings-the-what-the-why-and-the-how-15a6a3c99ce8 This blogs deeps dives into the working and importance of Embeddings layer.\nThe other function we’ll be defining is a Xavier Initialization. Recurrent neural networks are very sensitive to the initialization used, so choosing the right one is important. It’s probably enough to know that the Xavier Initialization is a good choice; however, we can take a look at the math here.\ndef _initialize_weights(self): nn.init.xavier_uniform_(self.Wf) nn.init.xavier_uniform_(self.Wi) nn.init.xavier_uniform_(self.Wc) nn.init.xavier_uniform_(self.Wo) nn.init.xavier_uniform_(self.Wv) def parameters(self): # Return a list of all parameters (weights and biases) in the model return [self.Wf, self.bf, self.Wi, self.bi, self.Wc, self.bc, self.Wo, self.bo, self.Wv, self.bv, self.embedding] The next function to define is the forward function.\ndef forward(self, x, init_states=None): # Apply embedding layer to input indices x=x.squeeze(dim=-1) x = self.embedding[x] batch_size, seq_len, _ = x.size() # Initialize h_t and c_t if init_states is None if init_states is None: h_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) c_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) else: h_t, c_t = init_states outputs = [] for t in range(seq_len): x_t = x[:, t, :] # Shape: (batch_size, embedding_dim) Z_t = torch.cat([x_t, h_t], dim=1) # Shape: (batch_size, embedding_dim + hidden_size) # Forget gate ft = torch.sigmoid(Z_t @ self.Wf.t() + self.bf) # Input gate it = torch.sigmoid(Z_t @ self.Wi.t() + self.bi) # Candidate cell state can = torch.tanh(Z_t @ self.Wc.t() + self.bc) # Output gate ot = torch.sigmoid(Z_t @ self.Wo.t() + self.bo) c_t = c_t * ft + can * it h_t = ot * torch.tanh(c_t) # Compute output for current time step y_t = h_t @ self.Wv.t() + self.bv return y_t, (h_t, c_t) When building a model for next-word prediction, the goal is to predict the most likely word that follows a given sequence. In this context, we don’t need outputs from every timestep of the sequence—only the final output (from the last timestep) is necessary to make the prediction.\nNow lets initialize our Model.\nWe’ll be using a hidden size of 128 and an embedding dimension of 10 for our model. For comparison, GPT-2 Small (with 117 million parameters) used an embedding dimension of 768, and the latest GPT-4 uses 16,384. Since we are working with a small dataset, a smaller embedding dimension should work, and I chose 10 . You can play with this value to see how it plays out.\nmodel = LSTM(vocab_size=vocab_size, embedding_dim=128, hidden_size=128, output_size=vocab_size) params = model.parameters() optimizer = torch.optim.Adam(params, lr=0.005) Training Loop hidden_state = None # Initialize hidden state for _ in range(500): # Sample a batch batch_indices = torch.randint(0, X_train_padded.shape[0], (128,)) x_train = X_train_padded[batch_indices] # Shape: (batch_size, seq_len) # Forward pass outputs,hidden_state = model.forward(x_train, init_states=hidden_state) # print(outputs.shape) h_t, c_t = hidden_state hidden_state = (h_t.detach(), c_t.detach()) # Detach hidden state for next batch # Compute loss y_batch = Y_train[batch_indices] # Shape: (batch_size, seq_len, vocab_size) loss = criterion(outputs, y_batch) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(loss) This training approach grabs a random batch of 128 data points, puts them into the model, and hopes for the best, kind of like cramming for an exam with random notes you found in your bag. While it’s not the most efficient way to train, I went with it to really wrap my head around how batches flow through the network. After each iteration, the hidden state is politely told, \u0026ldquo;Take a break, you’ve done enough,\u0026rdquo; and detached. This prevents PyTorch from becoming a hoarder and building a massive computation graph that would slow things down to a crawl. For optimization, I used the Adam optimizer. Though simple, this setup helped me grasp the inner workings of batching and hidden state management in LSTMs.\nBefore we begin training the network, let’s see how our model performs with its randomly initialized weights. This will give us a baseline idea of what kind of text it generates before learning anything meaningful from the data.\nThe following code preprocess the input and passes it to the model\ndef generate_sequence(model, seed_string, char2idx, idx2char, sequence_length, max_len=55): seed_indices = [char2idx[word] for word in seed_string.split(\u0026#34; \u0026#34;) if word in char2idx] seed_tensor = torch.tensor(seed_indices).unsqueeze(0) # Shape: (1, seq_len) generated_indices = seed_indices[:] hidden_state = None for _ in range(sequence_length): # Pre-pad the input sequence to match the model\u0026#39;s expected input size padded_input = pre_pad_sequences_pytorch([generated_indices], max_len).unsqueeze(-1) # Get the model output and hidden state output, (hidden_state) = model.forward(padded_input, hidden_state) # Take the output corresponding to the last token next_token_logits = output # Shape: (1, vocab_size) # Use softmax to get probabilities and sample the next token next_token_prob = torch.softmax(next_token_logits, dim=-1) # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item() next_token_idx=torch.argmax(next_token_prob).item() # Append the predicted token to the sequence generated_indices.append(next_token_idx) # Convert indices back to characters generated_words = [idx2char[idx] for idx in generated_indices] return \u0026#34; \u0026#34;.join(generated_words) # Example usage: seed_string = \u0026#34;scum\u0026#34; sequence_length = 20 generated_text = generate_sequence(model, seed_string, char2idx, idx2char, sequence_length) print(\u0026#34;Generated Text:\u0026#34;) print(generated_text) Now for a sequence_length of 20 this is what our model outputs.\nIt is.\u0026#39; if bath, is.\u0026#39; if bath, cockney side, black serve is.\u0026#39; go three asleep straw bath, is.\u0026#39; cotton when when This up apparatus kind where Majors tub a stripped eight Doesn’t makes any sense.\nNow Lets train the model and run this code again.\nNow I trained the model for 300 iteration, it started with an initial loss of 6.85 reaching to 0.1084 at the end of 300 iteration. So, our model learnt well from the data. Lets see how our model performs after training on the same seed string.\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to Much better . Lets generate a paragraph now .\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to make off and hide in a shed used for storing potatoes, together with some workhouse paupers who were skulking to avoid the Sunday-morning service. There was a stove burning there, and comfortable packing cases to sit on, and back numbers of the Family Herald, and even a copy of Raffles from the workhouse library. It was paradise after the grime, of Sunday. It appeared the Tramp Major served the than before and the hot pipes. The cotton blankets were almost useless. One spent the night in turning from side to side, falling asleep for ten minutes and waking half frozen, and watching for the medical tables. It was a gloomy, chilly, limewashed place, consisting only of a bathroom and dining-room and about a hundred narrow stone cells. The terrible Tramp Major met us at the door and herded us into the bathroom to be stripped and searched. He was a gruff, soldierly man of forty, a pound of bread, a bit of margarine, and a pint of so-called tea. It took us five minutes to gulp down the cheap, noxious food. Then the Tramp Major served us with three cotton blankets each, and drove us off to our cells for the night. The doors were locked on the outside a little before seven in the evening, and would stay locked for the next twelve twelve hours in the spike, told tales of mooching, of pints stood him in the boozers, of the night for giving a back answer. When You, came to be searched, he fair held you upside down and shook you. If you were caught with tobacco there was bell to. Pay, and if you went in with money (which is against the law) God help help had no socks, except the tramps had already washed to our about and and for the lot of spring, perhaps—the authorities little It of the water, and decided to go dirty for the queer habit of sleeping in his hat, grumbled about a parcel of tommy that he had lost on the toad. Bill the moocher, the best built man of But it looks like our small model memorized the paragraph instead of learning any real patterns. This is a case of overfitting. Since we trained the model for 300 epochs on such a small dataset, it essentially “rote learned” the paragraph and just outputs the exact same text from the dataset\nIn generating text, we initially used argmax with torch.argmax(next_token_prob).item(), which simply picks the word with the highest probability from the output distribution. This approach is deterministic, meaning it will always choose the same word if the probabilities don\u0026rsquo;t change.\nNow, let’s try using a multinomial distribution instead. A multinomial distribution creates a probability distribution over all possible tokens, allowing us to sample the next token randomly based on the probabilities. Instead of always choosing the word with the highest probability, the multinomial distribution introduces some randomness by selecting tokens based on their likelihood.\nFor this uncomment the line # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item().\nThis method introduces variations in the text output, as the model can now generate different sequences even when given the same starting point. It\u0026rsquo;s like giving the model a little creative freedom, kind of like letting it freestyle instead of sticking to a strict script.\nThat wraps up this blog. Here, we generated text sequences using a trained LSTM model. I know there’s a lot of room for improvement, but the goal was to get a basic understanding of data preprocessing and how LSTMs work. For more structured and efficient code, it would make sense to use PyTorch’s nn.LSTM module.\nWhat\u0026rsquo;s Next? To improve this model:\nAdd Dropout: Prevent overfitting with regularization techniques. Use Better Sampling: Replace the random batch sampling with more structured approaches like sequential data loading. Increase Dataset Size: Larger datasets will yield more meaningful insights during training. Thanks for sticking around till the end! This being my first blog, I’m sure it’s not perfect, but I’ll work on improving my writing in the future. Appreciate you reading through!\nHere’s the GitHub repository with the code and dataset used for this project. Feel free to check it out!\nResources The Long Short-Term Memory (LSTM) Network from Scratch | Medium Implementing a LSTM from scratch with Numpy - Christina Kouridi Building makemore Part 2: MLP ","permalink":"http://localhost:1313/prtfoio/blog/lstm-from-scratch/","summary":"\u003ch1 id=\"lstm-from-scratch\"\u003eLSTM from Scratch\u003c/h1\u003e\n\u003cp\u003eIn this post, we will implement a simple next word predictor LSTM from scratch using torch.\u003c/p\u003e\n\u003ch2 id=\"a-gentle-introduction-to-lstm\"\u003eA gentle Introduction to LSTM\u003c/h2\u003e\n\u003cp\u003eLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by \u003ca href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\"\u003eHochreiter \u0026amp; Schmidhuber (1997)\u003c/a\u003e. As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\u003c/p\u003e","title":"Implementing LSTM from scratch in PyTorch step-by-step."},{"content":"Jenkins 🚀 What is Jenkins? Jenkins is an open-source automation server used primarily for continuous integration (CI) and continuous deployment (CD).\nContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a Jenkins pipeline to automate CI/CD for a project.\nIn this Blog, I’ll cover:\nJenkins architecture Writing a Jenkins pipeline for automating CI/CD Setting up webhook to trigger deployments automatically. Jenkins Architecture Jenkins follows a master-agent setup (formerly called master-slave), where the Jenkins server (master) manages everything, and agents (slaves) do the actual work.\nJenkins Server (Master)\nManages jobs, users, security, and plugins. Schedules tasks and decides which agent should run them. Provides the web interface where you monitor and control everything. Jenkins Agent (Slave)\nRun the actual build and deployment tasks. Communicate with the master to receive instructions. Report back with logs and results. 💡Note: Since we\u0026rsquo;re running Jenkins locally, both the server and agent are on the same machine. The server manages jobs and the web interface, while the agent executes tasks—everything happens on our local system.\n📝 Jenkinsfile A Jenkinsfile defines the automation process using a declarative syntax. It tells Jenkins what to do, when to do it, and how to handle failures This is a Jenkins pipeline file written in Declarative Pipeline Syntax.\npipeline { agent any stages { stage(\u0026#39;Checkout\u0026#39;) { steps { checkout scm } } stage(\u0026#39;Cleanup\u0026#39;) { steps { script { // Stop and remove existing containers sh \u0026#39;\u0026#39;\u0026#39; docker-compose down # Additional cleanup in case docker-compose down didn\u0026#39;t work docker rm -f mysql-db || true \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Start MySQL\u0026#39;) { steps { script { // Start only MySQL sh \u0026#39;docker-compose up -d db\u0026#39; // Wait for 1 minute sh \u0026#39;sleep 60\u0026#39; echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; } } } stage(\u0026#39;Start Other Services\u0026#39;) { steps { script { // Start remaining services sh \u0026#39;docker-compose up -d\u0026#39; } } } } post { failure { sh \u0026#39;docker-compose down\u0026#39; echo \u0026#39;Deployment failed\u0026#39; } success { echo \u0026#39;Deployment successful\u0026#39; } } } Understanding Jenkinsfile 1. Checkout Stage: It Retrieves the code from the repository (using SCM — Source Code Management) so that the pipeline can work with the latest code. This command checks out the repository, ensuring that the pipeline works with the latest version of the code.\ncheckout scm 2. Cleanup Stage: It Stops and removes any existing containers from a previous deployment to ensure a clean environment before starting fresh\ndocker-compose down docker rm -f mysql-db || true docker-compose down stops all services defined in the docker-compose.yml and removes the containers. docker rm -f mysql-db || true forces the removal of the MySQL container (mysql-db), even if it\u0026rsquo;s not running. The || true ensures the pipeline doesn\u0026rsquo;t fail if the container doesn\u0026rsquo;t exist. 3. Start MySQL Stage: It Starts the MySQL container using docker-compose and waits for it to initialize before starting other services.\ndocker-compose up -d db sleep 60 echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; docker-compose up -d db starts the MySQL container (db service) in detached mode (d), allowing the pipeline to continue without waiting for MySQL to be ready. sleep 60 pauses the pipeline for 1 minute, giving MySQL time to start and initialize. 4.Start Other Services Stage: Starts all the remaining services defined in the docker-compose.yml (other than MySQL, which was started separately in the previous stage).This command starts the remaining services (e.g., frontend, backend) in detached mode.\ndocker-compose up -d 5. Post Section: Defines steps that should be executed after the pipeline has completed, regardless of success or failure.\nIf the deployment fails, the docker-compose down command is executed to clean up containers, and a failure message is printed:\ndocker-compose down echo \u0026#39;Deployment failed\u0026#39; If the deployment is successful, a success message is printed:\necho \u0026#39;Deployment successful\u0026#39; 🔧 Setting Up Jenkins on Ubuntu Follow this to install Jenkins in Ubuntu.\n1. Add the Jenkins Repository Key Run the following command to download the Jenkins repository key and store it:\nsudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\ https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key 2. Add the Jenkins Repository Now, add the Jenkins repository to your package sources list using the key you just downloaded:\necho \u0026#34;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\u0026#34; \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null 3. Update Package Lists Next, update your package list to include the newly added Jenkins repository:\nsudo apt-get update 4. Install Jenkins Now, install Jenkins using the following command:\nsudo apt-get install jenkins 5. Start Jenkins After the installation completes, start Jenkins:\nsudo systemctl start jenkins Visit for web interface.\nhttp://localhost:8080/ After you login to Jenkins, - Run the command to copy the Jenkins Admin Password - sudo cat /var/lib/jenkins/secrets/initialAdminPassword - Enter the Administrator password\n🐳 Grant Docker Permissions to Jenkins Grant Jenkins User permission to use docker daemon. Jenkins User is automatically created when installing Jenkins.\nsudo su - usermod -aG docker jenkins systemctl restart docker Now switch to Jenkins and see if Docker is running inside it\nsu - jenkins docker run hello-world Great. Now lets setup Jenkins from web.\nInstall Docker plugin inside Jenkins to run docker as agent.\nAfter successful installation restart the Jenkins. You can restart by simply adding /restart in URL.\nCreating Jenkins Pipeline For this project I\u0026rsquo;m using this repo. It has a frontend, backend, and SQL database—a simple to-do list app. The frontend calls the backend, and the backend interacts with the database to fetch, add, and delete list items.\nClick on Pipeline . Here we’ll have Jenkins file in our repo so Jenkins will fetch the file from there.\nNow select on apply and Save.\nGo to dashboard and start the build.\nBuild Success Lets check our application running on port 8000.\nSo this verifies our task running smoothly that is deployed using Jenkins.\nSetting Up Webhook Let\u0026rsquo;s set up a webhook in Jenkins and GitHub to enable a seamless CI process. When the webhook is enabled, GitHub sends an HTTP POST request to Jenkins with information about the push event. Jenkins listens for this request and triggers the configured pipeline to run, deploying the updated code.\nTo make Jenkins accessible to GitHub, we need to expose Jenkins running on localhost to the public. Since GitHub needs to send an HTTP POST request to a public IP, we’ll use ngrok for port forwarding. Ngrok creates a secure tunnel to your local machine, allowing GitHub to access Jenkins through a public UR\nSetting up ngrok Go to ngrok website and follow the instruction provided there for WSL.\nInstall ngrok via Apt with the following command:\ncurl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\ | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc \u0026gt;/dev/null \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://ngrok-agent.s3.amazonaws.com buster main\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/ngrok.list \\ \u0026amp;\u0026amp; sudo apt update \\ \u0026amp;\u0026amp; sudo apt install ngrok Run the following command to add your authtoken to the default ngrok.yml\nngrok config add-authtoken \u0026lt;your_auth_token\u0026gt; Now run the command:\nngrok http http://localhost:8080 The command ngrok http http://localhost:8080 creates a secure tunnel to your local Jenkins server running on port 8080, exposing it to the public internet via a unique ngrok URL.\nNow we have can access our port 8080 from any device with the given ngrok URL.\nSetting Up Web Hook Create a new pipeline and In trigger option select this option.\nGo to your GitHub repo\u0026rsquo;s Settings \u0026gt; Webhooks \u0026gt; Add webhook to configure the webhook.\nNow add URL provided by ngrok appended with /github-webhook.\nThen click on Add Webhook.\nLet\u0026rsquo;s make a change in our code and push it to GitHub to test if the webhook works:\nLets commit the changes and push it to Github.\nAs soon as i push the changes to GitHub the webhook triggers the Jenkins and runs our pipeline.\nBuild 4 is successful. Now lets check if our changes has been implemented or not.\nHere we can see our changes has been implemented.\nIn this blog, we wrote a Jenkins pipeline and set up a webhook to automate the CI/CD process in our local environment. To make Jenkins accessible to GitHub, we used ngrok to tunnel into our local machine. After pushing code to the repository, we observed Jenkins automatically triggering and running the pipeline, handling the deployment process without any manual intervention.\nThe repo used for this blog is https://github.com/agrimpaneru/dockerized-todo. Feel free to fork it and experiment with it.\n","permalink":"http://localhost:1313/prtfoio/blog/jenkins-webhooks-cicd/","summary":"\u003ch1 id=\"jenkins\"\u003eJenkins\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"-what-is-jenkins\"\u003e🚀 \u003cstrong\u003eWhat is Jenkins?\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eJenkins is an open-source automation server used primarily for \u003cstrong\u003econtinuous integration (CI) and continuous deployment (CD)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e to automate CI/CD for a project.\u003c/p\u003e\n\u003cp\u003eIn this Blog, I’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJenkins architecture\u003c/li\u003e\n\u003cli\u003eWriting a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e for automating CI/CD\u003c/li\u003e\n\u003cli\u003eSetting up webhook to trigger deployments automatically.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"jenkins-architecture\"\u003e\u003cstrong\u003eJenkins Architecture\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image%201.png\"\u003e\u003c/p\u003e","title":"Integrating Jenkins with Webhooks for Efficient CI/CD Automation"},{"content":"LMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\nIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u0026ldquo;Attention Is All You Need\u0026rdquo; shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\nEmbeddings representing the Words as vectors Initially the words are converted into tokens, Here we assume tokens are equal to index.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] Output for this code is :\n[tensor([0.8823, 0.9150, 0.3829]), tensor([0.9593, 0.3904, 0.6009]), tensor([0.2566, 0.7936, 0.9408]), tensor([0.1332, 0.9346, 0.5936]), tensor([0.8694, 0.5677, 0.7411])] Here the corresponding Embeddings for each word is initialized randomly. Here we represented each word as 3 dimensional embedding.\n{\u0026#39;the\u0026#39;: tensor([0.8823, 0.9150, 0.3829]), \u0026#39;cat\u0026#39;: tensor([0.9593, 0.3904, 0.6009]), \u0026#39;sat\u0026#39;: tensor([0.2566, 0.7936, 0.9408]), \u0026#39;on\u0026#39;: tensor([0.1332, 0.9346, 0.5936]), \u0026#39;mat\u0026#39;: tensor([0.8694, 0.5677, 0.7411])} Attention Mechanism Attention Mechanism is inspired by how humans focus on specific parts of information when processing it—just like when we read a sentence, we don\u0026rsquo;t focus on every word equally. Instead, we \u0026ldquo;attend\u0026rdquo; to the most relevant words that help us understand the meaning.\nLets take an example for understanding attention mechanism:\nImagine reading the sentence: \u0026ldquo;The cat sat on the mat.\u0026rdquo;\nIf asked, \u0026ldquo;Where is the cat?\u0026rdquo;, attention would primarily focus on the word \u0026ldquo;cat\u0026rdquo; and the word \u0026ldquo;mat\u0026rdquo;. Other words like \u0026ldquo;the\u0026rdquo; or \u0026ldquo;on\u0026rdquo; are ignored since they don’t carry much relevance to the question.\nLets take a sentence:\nI am going to cinema to watch ………?\nThe most probable answers would be \u0026ldquo;movie,\u0026rdquo; \u0026ldquo;action movie,\u0026rdquo; or something similar. Words like \u0026ldquo;book\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; don\u0026rsquo;t fit the context and are irrelevant to predicting the correct word. The key to predicting the correct word is in the context and information provided in the preceding words. To make accurate predictions, we need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the model to focus on the most relevant words to that context to make predictions.\nUnderstanding Scaled Dot-Product Attention The paper Attention Is All You Need introduced Scaled Dot-Product Attention .\nLets first calculate the Similarity between Query and Key\nThis is done as\nQ = K = V = embeddings similarity_matrix=torch.matmul(Q,K.T) In our case the dimension of output is 5*5.\nSimilarity Matrix:\ntensor([[1.7622, 1.4337, 1.3127, 1.1999, 1.5702], [1.4337, 1.4338, 1.1213, 0.8494, 1.5010], [1.3127, 1.1213, 1.5807, 1.3343, 1.3708], [1.1999, 0.8494, 1.3343, 1.2435, 1.0863], [1.5702, 1.5010, 1.3708, 1.0863, 1.6274]]) Here’s each value represents the similarity between words as described below.\nthe cat sat on mat the the-the the-cat the-sat the-on the-mat cat cat-the cat-cat cat-sat cat-on cat-mat sat sat-the sat-cat sat-sat sat-on sat-mat on on-the on-cat on-sat on-on on-mat mat mat-the mat-cat mat-sat mat-on mat-mat The similarity matrix captures the similarities between the query and key vectors. Higher similarity means the model should focus more on that word when making predictions, while lower similarity means the word is less relevant.\nScaling the compatibility matrix In the second step, we scale the dot-product of the query and key vectors by a factor of,\n$$ \\frac{1}{d_k} $$ . In our case We’ve only taken dimension of 3.\nHowever, when the dimension d is very large, dot-product attention without this scaling factor performs poorly. The authors of the paper “Attention Is All You Need” attribute this poor performance to having large variance.\nAs d_k increases, the variance of the dot product also becomes larger, leading to some values becoming extremely small after being passed through the softmax function. This causes issues during gradient calculation, similar to the vanishing gradient problem, where the gradients become too small to effectively update the model. To mitigate this, the dot product is scaled by $$ \\sqrt{\\frac{1}{d_k}} $$\nbefore applying softmax , which stabilizes the training process and improves performance.\nTo visualize this effect lets take output from softmax before and after scaling for embedding dimension of 256.\nApplying Softmax Activation The softmax function is used to convert the logits into probabilities. It takes a vector of raw scores (logits) and transforms them into a probability distribution. The softmax function is defined as:\n$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\nSoftmax is applied pointwise, so it doesn\u0026rsquo;t change the dimension of the input vector.\nimport torch.nn.functional as F similarity_matrix_scaled=similarity_matrix/(1/torch.sqrt(torch.tensor(3))) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) softmax_similarity_matrix_scaled Its output is:\ntensor([[0.2372, 0.1962, 0.1830, 0.1714, 0.2123], [0.2179, 0.2180, 0.1820, 0.1555, 0.2266], [0.1957, 0.1752, 0.2285, 0.1982, 0.2024], [0.2058, 0.1681, 0.2224, 0.2110, 0.1927], [0.2154, 0.2070, 0.1920, 0.1629, 0.2227]]) It can be visualized in Heatmap as\nComputing the Context Vector as final output. So for final step We multiply the attention matrix from previous step and multiply it with the Value matrix V. The final product gives the new context vector for all the words in vocabulary in our case 5.\nnew_context=torch.matmul(softmax_similarity_matrix_scaled,V) new_context Output:\ntensor([[0.6518, 0.7195, 0.6399], [0.6658, 0.7029, 0.6459], [0.6018, 0.7289, 0.6628], [0.5955, 0.7371, 0.6571], [0.6532, 0.7090, 0.6492]]) Conclusion In this section, we saw how contextual embeddings are generated using self-attention. We also discussed the importance of scaling and how to implement a simple self-attention block. Notably, there were no trainable parameters involved here, and we treated the Query (Q), Key (K), and Value (V) matrices as all being the same.\nHowever, to make the attention block trainable, we need to introduce learnable weights when deriving the Q, K, and V vectors from the original embedding. These weights will allow the model to adjust and fine-tune the attention mechanism during training. In the next section, we\u0026rsquo;ll explore how to implement this by adding trainable parameters to the attention block\nAttention Mechanism with trainable Parameters Alright! Now that we understand how self-attention is calculated, let\u0026rsquo;s take it a step further by introducing trainable parameters so that the model can learn how to find the context of words effectively.\nAdding Trainable Parameters to Self-Attention Q (Query), K (Key), and V (Value) matrices are not just direct copies of word embeddings. Instead, they are learned transformations of these embeddings.\nTo achieve this, we introduce three trainable weight matrices:\nW_q (Query weight matrix): Learns how to project word embeddings into the query space. W_k (Key weight matrix): Learns how to project word embeddings into the key space. W_v (Value weight matrix): Learns how to project word embeddings into the value space. Each of these matrices will be optimized during training.\nW_q=torch.randn(embedding_dim,embedding_dim) W_v=torch.randn(embedding_dim,embedding_dim W_k=torch.randn(embedding_dim,embedding_dim) Note that the dimension of these matrix can be made different. However the first dimension must be equal to embedding dimension to satisfy the matrix multiplication condition. Changing the second dimension implies projecting the embedding dimension from one dimension to other .\nLets get the new Query, Key and Value vector.\nQ = torch.matmul(embeddings, W_q) # (5, 3) K = torch.matmul(embeddings, W_k) # (5, 3) V = torch.matmul(embeddings, W_v) # (5, 3) Then we calculate the self attention similar to how we did above.\nsimilarity_matrix = torch.matmul(Q, K.T) # (5, 5) similarity_matrix_scaled = similarity_matrix / torch.sqrt(torch.tensor(embedding_dim, dtype=torch.float32)) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(softmax_similarity_matrix_scaled, V) This is what the final flow looks like.\nHere Lets wrap up this in Class Based Implementation\nimport torch import torch.nn.functional as F class SelfAttention: def __init__(self, embedding_dim): torch.manual_seed(42) self.embedding_dim = embedding_dim # Initialize weight matrices self.W_q = torch.randn(embedding_dim, embedding_dim) self.W_k = torch.randn(embedding_dim, embedding_dim) self.W_v = torch.randn(embedding_dim, embedding_dim) def forward(self, embeddings): # Compute Query, Key, and Value matrices Q = torch.matmul(embeddings, self.W_q) K = torch.matmul(embeddings, self.W_k) V = torch.matmul(embeddings, self.W_v) # Compute similarity (dot product attention) similarity_matrix = torch.matmul(Q, K.T) # (num_words, num_words) # Scale by sqrt(embedding_dim) similarity_matrix_scaled = similarity_matrix / torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32) ) # Apply softmax to get attention weights attention_weights = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(attention_weights, V) return new_context, attention_weights embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] # Generate random embeddings for words embeddings = torch.stack([torch.rand(embedding_dim) for _ in words]) # Initialize attention mechanism self_attention = SelfAttention(embedding_dim) # Forward pass to compute attention new_context, attention_weights = self_attention.forward(embeddings) print(\u0026#34;New Context Vectors:\\n\u0026#34;, new_context) print(\u0026#34;\\nAttention Weights:\\n\u0026#34;, attention_weights) Output:\nNew Context Vectors: tensor([[ 0.3689, -0.4890, 1.3677], [ 0.2569, -0.4673, 1.3297], [ 0.3284, -0.4752, 1.3515], [ 0.1971, -0.4685, 1.3124], [ 0.3061, -0.4846, 1.3491]]) Attention Weights: tensor([[0.4672, 0.0538, 0.0879, 0.0829, 0.3082], [0.3362, 0.1266, 0.1921, 0.1077, 0.2375], [0.4212, 0.0851, 0.1492, 0.0815, 0.2631], [0.2728, 0.1547, 0.1886, 0.1526, 0.2314], [0.3922, 0.0832, 0.1179, 0.1143, 0.2923]]) So this ends up this blog. Congratulations on making it to the end. Feel free to experiment with different dimensions and input data to see how the self-attention mechanism adapts and scales.\nReferences Implementing Self-Attention from Scratch in PyTorch | by Mohd Faraaz | Medium\nUnderstanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\nSelf Attention in Transformers | Deep Learning | Simple Explanation with Code!\nSelf-attention from scratch | Blogs by Anil\n","permalink":"http://localhost:1313/prtfoio/blog/self-attention-pytorch/","summary":"\u003cp\u003eLMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\u003c/p\u003e\n\u003cp\u003eIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u003cem\u003e\u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/em\u003e shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\u003c/p\u003e","title":"Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch"},{"content":"Implementing Multihead attention from scratch with pytorch In our previous article, we built Self-Attention from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\nNow, let’s take things a step further and implement Multi-Head Attention from scratch. This post focuses more on the implementation rather than the theory, so I assume you’re already familiar with how self-attention works.\nLet’s get started!\nMultihead Attention Similar to Self-Attention, Multi-Head Attention also creates a new context vector. But instead of using a single attention mechanism, it uses multiple heads, each learning different relationships between words.\nFor example, consider these two sentences:\n1️⃣ I went to the bank to withdraw cash.\n2️⃣ I went for a picnic by the river bank.\nHere, the word \u0026ldquo;bank\u0026rdquo; has different meanings depending on the context. The idea behind Multi-Head Attention is that different heads can focus on different aspects of a word’s meaning, capturing various relationships more effectively.\nIn the above example with a single attention head, the model might have difficulty capturing both the financial and geographical meanings of the word \u0026ldquo;bank\u0026rdquo; at the same time. Multi-Head Attention, on the other hand, allows the model to learn both meanings separately—one head could focus on financial context while another focuses on the geographical context. This way, the model learns richer, more nuanced embeddings of the word \u0026ldquo;bank\u0026rdquo; based on different contexts.\nFor a more intuitive explanation of Multi-Head Attention, I highly recommend checking out this medium post.\nNow Lets Implement it.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 8 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) Similar to our previous Self-Attention implementation, we’ll use the following sentence with a random embedding of dimension 8. However, in real-world applications, embeddings are typically a combination of word embeddings and positional encodings. To keep things simple, we won’t dive into that here. Now, let’s define the Wq, Wk, and Wv matrices. We could use separate smaller weight matrices to get the Query, Key, and Value vectors directly, but a more efficient approach is to use a single large weight matrix and split it afterward for multiple attention heads. This method is more computationally efficient, as it allows for parallel processing across heads.\nWq = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wk = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wv = torch.rand(embedding_dim, embedding_dim) # (8, 8) Lets apply the projection to Get Query Key and Value matrix.\n# Apply projection Q = embeddings @ Wq # Shape: (5, 8) K = embeddings @ Wk # Shape: (5, 8) V = embeddings @ Wv # Shape: (5, 8) Now ,We’ll split the projected vectors into multiple heads . Remember the Embedding dimension must be exactly divisible by head\nd_k = embedding_dim // num_heads Now we obtain the Query Key and Value Matrix for multiple heads.\nQ = Q.view(seq_len, num_heads, d_k).transpose(0, 1) K = K.view(seq_len, num_heads, d_k).transpose(0, 1) V = V.view(seq_len, num_heads, d_k).transpose(0, 1) The output shape of Q, K, and V after the split will be (2, 5, 4), where:\n2 corresponds to the number of heads, 5 corresponds to the sequence length (number of words), and 4 is the new dimension of each vector for each head. We calculate attention matrix by,\nattention_scores=Q @ K.transpose(-2,-1) \u0026#39;\u0026#39;\u0026#39;For 3D matrix multiplication, the first dimension of both matrices should match, #while the second dimension of the first matrix must align with the third dimension of the second matrix according to the matrix multiplication rule\u0026#39;\u0026#39;\u0026#39; Here, we get two attention scores from each head. We’ll proceed with scaling these scores and applying the softmax activation, just like in Self-Attention. For a detailed explanation of this process, please refer to my previous post on Self-Attention\nBut before applying the scaling and softmax activation, we’ll mask the attention scores. Masking is necessary to ensure that the prediction for a given token only depends on the current and previous tokens. Without masking, the model could \u0026ldquo;cheat\u0026rdquo; by looking ahead at future tokens which in not desirable.\nMasked Multi-Head Attention In the decoder of a transformer, we use masked multi-head attention to prevent the model from looking at future tokens when predicting the next one. This ensures that the model can only base its prediction on past tokens, not the future ones. In contrast, the encoder doesn’t need masking because it has access to the entire sequence at once, so it can freely attend to all parts of the input.\nThis masking enables the model to process all tokens in parallel during training, enhancing efficiency.\nWe will implement Masked Multi-Head Attention, ensuring that the model only attends to past words by masking the attention scores. This prevents the decoder from seeing future words, enforcing an autoregressive learning process\nMasking is applied before softmax by setting forbidden positions to−∞ to ensure they receive zero probability after normalization.\nmask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) mask = mask.bool() mask=mask.unsqueeze(0) attention_scores=attention_scores.masked_fill(mask,-1e11) attention_scores=attention_scores/math.sqrt(d_k) #This is mask i.e Upper triangular matrix tensor([[0., 1., 1., 1., 1.], [0., 0., 1., 1., 1.], [0., 0., 0., 1., 1.], [0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.]]) #Attention Scores before masking tensor([[[24.6185, 22.6470, 19.6726, 10.3703, 23.2266], [25.3424, 23.5643, 20.2438, 10.8568, 24.0848], [20.2856, 18.6674, 16.2010, 8.5272, 19.1661], [11.0366, 10.1522, 8.8451, 4.5771, 10.3952], [23.4003, 21.7570, 18.6598, 10.0209, 22.2936]], [[24.1949, 25.1107, 20.5201, 12.3558, 24.2538], [24.3185, 25.1608, 20.5903, 12.5245, 24.4242], [19.3101, 20.0426, 16.3541, 9.9390, 19.3887], [11.4448, 11.6265, 9.6817, 5.8675, 11.3187], [23.9947, 24.8808, 20.3205, 12.3524, 24.1055]]]) #Attention Scores after masking tensor([[[ 2.4619e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.5342e+01, 2.3564e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.0286e+01, 1.8667e+01, 1.6201e+01, -1.0000e+11, -1.0000e+11], [ 1.1037e+01, 1.0152e+01, 8.8451e+00, 4.5771e+00, -1.0000e+11], [ 2.3400e+01, 2.1757e+01, 1.8660e+01, 1.0021e+01, 2.2294e+01]], [[ 2.4195e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.4318e+01, 2.5161e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 1.9310e+01, 2.0043e+01, 1.6354e+01, -1.0000e+11, -1.0000e+11], [ 1.1445e+01, 1.1626e+01, 9.6817e+00, 5.8675e+00, -1.0000e+11], [ 2.3995e+01, 2.4881e+01, 2.0321e+01, 1.2352e+01, 2.4105e+01]]]) So what we did here was created a lower triangular matrix and applied it as a mask to attention score and scaled it and passed to softmax to ensure each token can only attend to itself and previous tokens.\nAfter softmax, the row sum will be one and this ensures the attention weights are a valid probability distribution, allowing the model to learn proportional contributions from past tokens.\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5130, 0.4870, 0.0000, 0.0000, 0.0000], [0.3420, 0.3315, 0.3265, 0.0000, 0.0000], [0.2540, 0.2510, 0.2486, 0.2465, 0.0000], [0.2029, 0.2000, 0.1984, 0.1980, 0.2007]], [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.4935, 0.5065, 0.0000, 0.0000, 0.0000], [0.3342, 0.3391, 0.3267, 0.0000, 0.0000], [0.2521, 0.2528, 0.2485, 0.2466, 0.0000], [0.2006, 0.2022, 0.1984, 0.1981, 0.2007]]]) Here is what out attention scores looks like.\nNow we multiply out Attention score with Value matrix and concatenate it,\noutput=attention_scores@V output.transpose(0,1).reshape(seq_len,embedding_dim) After Concatenation we achieve the final Contextual embedding of each vectors.\nHere\u0026rsquo;s a cleaner and more efficient implementation of the Multi-Head Attention module in PyTorch, wrapped in a class:\nimport torch import math class MultiHeadAttention(torch.nn.Module): def __init__(self, num_heads, embedding_dim): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.embedding_dim = embedding_dim self.d_k = embedding_dim // num_heads self.Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) def forward(self, embeddings): seq_len = embeddings.size(0) Q = embeddings @ self.Wq # (seq_len, embedding_dim) K = embeddings @ self.Wk V = embeddings @ self.Wv # Converting to multiheaded attention Q = Q.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # (num_heads, seq_len, d_k) K = K.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) V = V.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # Compute attention scores attention_scores = torch.matmul(Q, K.transpose(-2, -1)) # (num_heads, seq_len, seq_len) # Apply mask (upper triangular mask for causal attention) mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() mask = mask.unsqueeze(0).expand_as(attention_scores) attention_scores = attention_scores.masked_fill(mask, -1e11) # Scale the attention scores attention_scores = attention_scores / math.sqrt(self.d_k) # Apply softmax to get attention weights attention_weights = torch.softmax(attention_scores, dim=-1) # (num_heads, seq_len, seq_len) # Compute the output (weighted sum of values) output = torch.matmul(attention_weights, V) output = output.transpose(0, 1).contiguous().view(seq_len, self.embedding_dim) return output # Example usage torch.manual_seed(42) embedding_dim = 8 num_heads = 2 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) mha = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim) # Forward pass through the model output = mha(embeddings) print(\u0026#34;Output shape:\u0026#34;, output.shape) ","permalink":"http://localhost:1313/prtfoio/blog/multi-head-attention-pytorch/","summary":"\u003ch1 id=\"implementing-multihead-attention-from-scratch-with-pytorch\"\u003eImplementing Multihead attention from scratch with pytorch\u003c/h1\u003e\n\u003cp\u003eIn our \u003ca href=\"https://agrimpaneru.com.np/blog/self-attention-pytorch/\"\u003eprevious article\u003c/a\u003e, we built \u003cstrong\u003eSelf-Attention\u003c/strong\u003e from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\u003c/p\u003e\n\u003cp\u003eNow, let’s take things a step further and implement \u003cstrong\u003eMulti-Head Attention\u003c/strong\u003e from scratch. This post focuses more on the \u003cstrong\u003eimplementation\u003c/strong\u003e rather than the theory, so I assume you’re already familiar with how self-attention works.\u003c/p\u003e\n\u003cp\u003eLet’s get started!\u003c/p\u003e","title":"MultiHead Attention Explained:Implementing Masked Multihead attention from Scratch in PyTorch"},{"content":"LSTM from Scratch In this post, we will implement a simple next word predictor LSTM from scratch using torch.\nA gentle Introduction to LSTM Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997). As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\nThe LSTM model we’re gonna implement follows this architecture.\nTo learn about their detailed structure here is a reference to this awesome blog Understanding LSTM Networks \u0026ndash; colah\u0026rsquo;s blog.\nFor simplified architecture:\nImports Lets import the required library\nimport torch import torch.nn as nn import torch.nn.functional as F Data Preparation We’ll be using autoregressive sentence generation for our dataset. This approach involves predicting the next word in a sequence based on the words that came before it.\nConsider a sentence,\n\u0026ldquo;I am Peter the hero.\u0026rdquo;\nInput Sequence Target Output [I] AM [I, AM] Peter [ I, AM, Peter] THE [I, AM, Peter, THE] HERO We\u0026rsquo;ll be working with George Orwell\u0026rsquo;s essay The Spike as our dataset. Since the dataset is relatively small, it allows us to focus on developing a deeper intuition without getting overwhelmed by the volume of data.\nTokenization Next, I\u0026rsquo;ll create a dictionary to convert words into indices—a common practice in Natural Language Processing (NLP). Instead of relying on libraries, I\u0026rsquo;ll build it from scratch to better understand how it works.\nchar2idx={char:i for i,char in enumerate(set(data))} # char2idx[\u0026#39;.\u0026#39;]=0 idx2char={i:char for i,char in enumerate(set(data))} We can run char2idx to view the word mapping to indices.\nx=[] for i in orgi.split(\u0026#34;\\n\u0026#34;): x.append([char2idx[j] for j in i.split(\u0026#34; \u0026#34;)]) This snippet processes a text dataset to convert words into their corresponding indices using the char2idx dictionary we created earlier.\nX_train = [] # List to store input sequences Y_train = [] # List to store corresponding outputs for sequence in x: for i in range(1, len(sequence)-1): # Input is the subsequence from start to the ith element X_train.append(sequence[:i]) # Output is the ith element (next token) Y_train.append(sequence[i]) This code prepares the dataset for autoregressive sentence generation, a technique where the model predicts the next token in a sequence based on previous tokens\nFor example, if the sequence is [1, 2, 3, 4], the resulting pairs would look like this:\nInput: [1], Output: 2 Input: [1, 2], Output: 3 Input: [1, 2, 3], Output: 4 Since LSTMs require input sequences of the same length, I padded all sequences to a fixed maximum length. I chose to pad at the beginning of the sequence because it preserves the actual data at the end, which is where the LSTM focuses more when processing the sequence. This ensures that the more meaningful tokens remain closer to the output\nmax_len=max([len(i) for i in X_train]) vocab_size=len(set(data)) def pre_pad_sequences_pytorch(sequences, max_len): padded_sequences = [] for seq in sequences: # If the sequence is shorter than max_len, pad with zeros at the beginning if len(seq) \u0026lt; max_len: padded_seq = [0] * (max_len - len(seq)) + seq # Pre-padding with 0 # If the sequence is longer than max_len, truncate it else: padded_seq = seq[-max_len:] padded_sequences.append(padded_seq) return torch.tensor(padded_sequences) X_train_padded = pre_pad_sequences_pytorch(X_train, max_len) X_train_padded=X_train_padded.unsqueeze(-1) Y_train=torch.tensor(Y_train) This creates a dataset ready to be set into LSTM network.\nFinally, the LSTM ##### Long Short-Term Memory Network Class ##### class LSTM: def __init__(self, vocab_size, embedding_dim, hidden_size, output_size): self.hidden_size = hidden_size #embedding layer self.embedding = torch.randn(vocab_size, embedding_dim, requires_grad=True) # Initialize weights with requires_grad=True #forget gate self.Wf = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bf = torch.zeros(hidden_size, requires_grad=True) #input gate self.Wi = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bi = torch.zeros(hidden_size, requires_grad=True) #candidate gate self.Wc = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bc = torch.zeros(hidden_size, requires_grad=True) #output gate self.Wo = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bo = torch.zeros(hidden_size, requires_grad=True) #final gate self.Wv = torch.randn(output_size, hidden_size, requires_grad=True) self.bv = torch.zeros(output_size, requires_grad=True) self._initialize_weights() This is a custom implementation of a Long Short-Term Memory (LSTM) network. It takes four inputs\nvocab_size,embedding_dim,hidden_size and output_size vocab_size is the number of unique words in your dataset.\nembedding_dim is the size of the word embeddings. Instead of representing words as simple indices (like 1, 2, 3, etc.), we represent them as dense vectors (e.g., [0.2, -0.5, 0.7, \u0026hellip;]).\nWhy embedding layer is needed ? Words are initially represented as indices (e.g., \u0026ldquo;cat\u0026rdquo; = 1, \u0026ldquo;dog\u0026rdquo; = 2, etc.). But these indices don’t carry any meaningful information about the words. The embedding layer converts these indices into dense vectors of size embedding_dim. These vectors are trainable, meaning the model will learn the best way to represent each word during training. For example, if embedding_dim =4, the word \u0026ldquo;cat\u0026rdquo; might be represented as a vector like [0.1, -0.3, 0.5, 0.9]. https://medium.com/analytics-vidhya/embeddings-the-what-the-why-and-the-how-15a6a3c99ce8 This blogs deeps dives into the working and importance of Embeddings layer.\nThe other function we’ll be defining is a Xavier Initialization. Recurrent neural networks are very sensitive to the initialization used, so choosing the right one is important. It’s probably enough to know that the Xavier Initialization is a good choice; however, we can take a look at the math here.\ndef _initialize_weights(self): nn.init.xavier_uniform_(self.Wf) nn.init.xavier_uniform_(self.Wi) nn.init.xavier_uniform_(self.Wc) nn.init.xavier_uniform_(self.Wo) nn.init.xavier_uniform_(self.Wv) def parameters(self): # Return a list of all parameters (weights and biases) in the model return [self.Wf, self.bf, self.Wi, self.bi, self.Wc, self.bc, self.Wo, self.bo, self.Wv, self.bv, self.embedding] The next function to define is the forward function.\ndef forward(self, x, init_states=None): # Apply embedding layer to input indices x=x.squeeze(dim=-1) x = self.embedding[x] batch_size, seq_len, _ = x.size() # Initialize h_t and c_t if init_states is None if init_states is None: h_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) c_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) else: h_t, c_t = init_states outputs = [] for t in range(seq_len): x_t = x[:, t, :] # Shape: (batch_size, embedding_dim) Z_t = torch.cat([x_t, h_t], dim=1) # Shape: (batch_size, embedding_dim + hidden_size) # Forget gate ft = torch.sigmoid(Z_t @ self.Wf.t() + self.bf) # Input gate it = torch.sigmoid(Z_t @ self.Wi.t() + self.bi) # Candidate cell state can = torch.tanh(Z_t @ self.Wc.t() + self.bc) # Output gate ot = torch.sigmoid(Z_t @ self.Wo.t() + self.bo) c_t = c_t * ft + can * it h_t = ot * torch.tanh(c_t) # Compute output for current time step y_t = h_t @ self.Wv.t() + self.bv return y_t, (h_t, c_t) When building a model for next-word prediction, the goal is to predict the most likely word that follows a given sequence. In this context, we don’t need outputs from every timestep of the sequence—only the final output (from the last timestep) is necessary to make the prediction.\nNow lets initialize our Model.\nWe’ll be using a hidden size of 128 and an embedding dimension of 10 for our model. For comparison, GPT-2 Small (with 117 million parameters) used an embedding dimension of 768, and the latest GPT-4 uses 16,384. Since we are working with a small dataset, a smaller embedding dimension should work, and I chose 10 . You can play with this value to see how it plays out.\nmodel = LSTM(vocab_size=vocab_size, embedding_dim=128, hidden_size=128, output_size=vocab_size) params = model.parameters() optimizer = torch.optim.Adam(params, lr=0.005) Training Loop hidden_state = None # Initialize hidden state for _ in range(500): # Sample a batch batch_indices = torch.randint(0, X_train_padded.shape[0], (128,)) x_train = X_train_padded[batch_indices] # Shape: (batch_size, seq_len) # Forward pass outputs,hidden_state = model.forward(x_train, init_states=hidden_state) # print(outputs.shape) h_t, c_t = hidden_state hidden_state = (h_t.detach(), c_t.detach()) # Detach hidden state for next batch # Compute loss y_batch = Y_train[batch_indices] # Shape: (batch_size, seq_len, vocab_size) loss = criterion(outputs, y_batch) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(loss) This training approach grabs a random batch of 128 data points, puts them into the model, and hopes for the best, kind of like cramming for an exam with random notes you found in your bag. While it’s not the most efficient way to train, I went with it to really wrap my head around how batches flow through the network. After each iteration, the hidden state is politely told, \u0026ldquo;Take a break, you’ve done enough,\u0026rdquo; and detached. This prevents PyTorch from becoming a hoarder and building a massive computation graph that would slow things down to a crawl. For optimization, I used the Adam optimizer. Though simple, this setup helped me grasp the inner workings of batching and hidden state management in LSTMs.\nBefore we begin training the network, let’s see how our model performs with its randomly initialized weights. This will give us a baseline idea of what kind of text it generates before learning anything meaningful from the data.\nThe following code preprocess the input and passes it to the model\ndef generate_sequence(model, seed_string, char2idx, idx2char, sequence_length, max_len=55): seed_indices = [char2idx[word] for word in seed_string.split(\u0026#34; \u0026#34;) if word in char2idx] seed_tensor = torch.tensor(seed_indices).unsqueeze(0) # Shape: (1, seq_len) generated_indices = seed_indices[:] hidden_state = None for _ in range(sequence_length): # Pre-pad the input sequence to match the model\u0026#39;s expected input size padded_input = pre_pad_sequences_pytorch([generated_indices], max_len).unsqueeze(-1) # Get the model output and hidden state output, (hidden_state) = model.forward(padded_input, hidden_state) # Take the output corresponding to the last token next_token_logits = output # Shape: (1, vocab_size) # Use softmax to get probabilities and sample the next token next_token_prob = torch.softmax(next_token_logits, dim=-1) # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item() next_token_idx=torch.argmax(next_token_prob).item() # Append the predicted token to the sequence generated_indices.append(next_token_idx) # Convert indices back to characters generated_words = [idx2char[idx] for idx in generated_indices] return \u0026#34; \u0026#34;.join(generated_words) # Example usage: seed_string = \u0026#34;scum\u0026#34; sequence_length = 20 generated_text = generate_sequence(model, seed_string, char2idx, idx2char, sequence_length) print(\u0026#34;Generated Text:\u0026#34;) print(generated_text) Now for a sequence_length of 20 this is what our model outputs.\nIt is.\u0026#39; if bath, is.\u0026#39; if bath, cockney side, black serve is.\u0026#39; go three asleep straw bath, is.\u0026#39; cotton when when This up apparatus kind where Majors tub a stripped eight Doesn’t makes any sense.\nNow Lets train the model and run this code again.\nNow I trained the model for 300 iteration, it started with an initial loss of 6.85 reaching to 0.1084 at the end of 300 iteration. So, our model learnt well from the data. Lets see how our model performs after training on the same seed string.\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to Much better . Lets generate a paragraph now .\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to make off and hide in a shed used for storing potatoes, together with some workhouse paupers who were skulking to avoid the Sunday-morning service. There was a stove burning there, and comfortable packing cases to sit on, and back numbers of the Family Herald, and even a copy of Raffles from the workhouse library. It was paradise after the grime, of Sunday. It appeared the Tramp Major served the than before and the hot pipes. The cotton blankets were almost useless. One spent the night in turning from side to side, falling asleep for ten minutes and waking half frozen, and watching for the medical tables. It was a gloomy, chilly, limewashed place, consisting only of a bathroom and dining-room and about a hundred narrow stone cells. The terrible Tramp Major met us at the door and herded us into the bathroom to be stripped and searched. He was a gruff, soldierly man of forty, a pound of bread, a bit of margarine, and a pint of so-called tea. It took us five minutes to gulp down the cheap, noxious food. Then the Tramp Major served us with three cotton blankets each, and drove us off to our cells for the night. The doors were locked on the outside a little before seven in the evening, and would stay locked for the next twelve twelve hours in the spike, told tales of mooching, of pints stood him in the boozers, of the night for giving a back answer. When You, came to be searched, he fair held you upside down and shook you. If you were caught with tobacco there was bell to. Pay, and if you went in with money (which is against the law) God help help had no socks, except the tramps had already washed to our about and and for the lot of spring, perhaps—the authorities little It of the water, and decided to go dirty for the queer habit of sleeping in his hat, grumbled about a parcel of tommy that he had lost on the toad. Bill the moocher, the best built man of But it looks like our small model memorized the paragraph instead of learning any real patterns. This is a case of overfitting. Since we trained the model for 300 epochs on such a small dataset, it essentially “rote learned” the paragraph and just outputs the exact same text from the dataset\nIn generating text, we initially used argmax with torch.argmax(next_token_prob).item(), which simply picks the word with the highest probability from the output distribution. This approach is deterministic, meaning it will always choose the same word if the probabilities don\u0026rsquo;t change.\nNow, let’s try using a multinomial distribution instead. A multinomial distribution creates a probability distribution over all possible tokens, allowing us to sample the next token randomly based on the probabilities. Instead of always choosing the word with the highest probability, the multinomial distribution introduces some randomness by selecting tokens based on their likelihood.\nFor this uncomment the line # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item().\nThis method introduces variations in the text output, as the model can now generate different sequences even when given the same starting point. It\u0026rsquo;s like giving the model a little creative freedom, kind of like letting it freestyle instead of sticking to a strict script.\nThat wraps up this blog. Here, we generated text sequences using a trained LSTM model. I know there’s a lot of room for improvement, but the goal was to get a basic understanding of data preprocessing and how LSTMs work. For more structured and efficient code, it would make sense to use PyTorch’s nn.LSTM module.\nWhat\u0026rsquo;s Next? To improve this model:\nAdd Dropout: Prevent overfitting with regularization techniques. Use Better Sampling: Replace the random batch sampling with more structured approaches like sequential data loading. Increase Dataset Size: Larger datasets will yield more meaningful insights during training. Thanks for sticking around till the end! This being my first blog, I’m sure it’s not perfect, but I’ll work on improving my writing in the future. Appreciate you reading through!\nHere’s the GitHub repository with the code and dataset used for this project. Feel free to check it out!\nResources The Long Short-Term Memory (LSTM) Network from Scratch | Medium Implementing a LSTM from scratch with Numpy - Christina Kouridi Building makemore Part 2: MLP ","permalink":"http://localhost:1313/prtfoio/blog/lstm-from-scratch/","summary":"\u003ch1 id=\"lstm-from-scratch\"\u003eLSTM from Scratch\u003c/h1\u003e\n\u003cp\u003eIn this post, we will implement a simple next word predictor LSTM from scratch using torch.\u003c/p\u003e\n\u003ch2 id=\"a-gentle-introduction-to-lstm\"\u003eA gentle Introduction to LSTM\u003c/h2\u003e\n\u003cp\u003eLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by \u003ca href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\"\u003eHochreiter \u0026amp; Schmidhuber (1997)\u003c/a\u003e. As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\u003c/p\u003e","title":"Implementing LSTM from scratch in PyTorch step-by-step."},{"content":"Jenkins 🚀 What is Jenkins? Jenkins is an open-source automation server used primarily for continuous integration (CI) and continuous deployment (CD).\nContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a Jenkins pipeline to automate CI/CD for a project.\nIn this Blog, I’ll cover:\nJenkins architecture Writing a Jenkins pipeline for automating CI/CD Setting up webhook to trigger deployments automatically. Jenkins Architecture Jenkins follows a master-agent setup (formerly called master-slave), where the Jenkins server (master) manages everything, and agents (slaves) do the actual work.\nJenkins Server (Master)\nManages jobs, users, security, and plugins. Schedules tasks and decides which agent should run them. Provides the web interface where you monitor and control everything. Jenkins Agent (Slave)\nRun the actual build and deployment tasks. Communicate with the master to receive instructions. Report back with logs and results. 💡Note: Since we\u0026rsquo;re running Jenkins locally, both the server and agent are on the same machine. The server manages jobs and the web interface, while the agent executes tasks—everything happens on our local system.\n📝 Jenkinsfile A Jenkinsfile defines the automation process using a declarative syntax. It tells Jenkins what to do, when to do it, and how to handle failures This is a Jenkins pipeline file written in Declarative Pipeline Syntax.\npipeline { agent any stages { stage(\u0026#39;Checkout\u0026#39;) { steps { checkout scm } } stage(\u0026#39;Cleanup\u0026#39;) { steps { script { // Stop and remove existing containers sh \u0026#39;\u0026#39;\u0026#39; docker-compose down # Additional cleanup in case docker-compose down didn\u0026#39;t work docker rm -f mysql-db || true \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Start MySQL\u0026#39;) { steps { script { // Start only MySQL sh \u0026#39;docker-compose up -d db\u0026#39; // Wait for 1 minute sh \u0026#39;sleep 60\u0026#39; echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; } } } stage(\u0026#39;Start Other Services\u0026#39;) { steps { script { // Start remaining services sh \u0026#39;docker-compose up -d\u0026#39; } } } } post { failure { sh \u0026#39;docker-compose down\u0026#39; echo \u0026#39;Deployment failed\u0026#39; } success { echo \u0026#39;Deployment successful\u0026#39; } } } Understanding Jenkinsfile 1. Checkout Stage: It Retrieves the code from the repository (using SCM — Source Code Management) so that the pipeline can work with the latest code. This command checks out the repository, ensuring that the pipeline works with the latest version of the code.\ncheckout scm 2. Cleanup Stage: It Stops and removes any existing containers from a previous deployment to ensure a clean environment before starting fresh\ndocker-compose down docker rm -f mysql-db || true docker-compose down stops all services defined in the docker-compose.yml and removes the containers. docker rm -f mysql-db || true forces the removal of the MySQL container (mysql-db), even if it\u0026rsquo;s not running. The || true ensures the pipeline doesn\u0026rsquo;t fail if the container doesn\u0026rsquo;t exist. 3. Start MySQL Stage: It Starts the MySQL container using docker-compose and waits for it to initialize before starting other services.\ndocker-compose up -d db sleep 60 echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; docker-compose up -d db starts the MySQL container (db service) in detached mode (d), allowing the pipeline to continue without waiting for MySQL to be ready. sleep 60 pauses the pipeline for 1 minute, giving MySQL time to start and initialize. 4.Start Other Services Stage: Starts all the remaining services defined in the docker-compose.yml (other than MySQL, which was started separately in the previous stage).This command starts the remaining services (e.g., frontend, backend) in detached mode.\ndocker-compose up -d 5. Post Section: Defines steps that should be executed after the pipeline has completed, regardless of success or failure.\nIf the deployment fails, the docker-compose down command is executed to clean up containers, and a failure message is printed:\ndocker-compose down echo \u0026#39;Deployment failed\u0026#39; If the deployment is successful, a success message is printed:\necho \u0026#39;Deployment successful\u0026#39; 🔧 Setting Up Jenkins on Ubuntu Follow this to install Jenkins in Ubuntu.\n1. Add the Jenkins Repository Key Run the following command to download the Jenkins repository key and store it:\nsudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\ https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key 2. Add the Jenkins Repository Now, add the Jenkins repository to your package sources list using the key you just downloaded:\necho \u0026#34;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\u0026#34; \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null 3. Update Package Lists Next, update your package list to include the newly added Jenkins repository:\nsudo apt-get update 4. Install Jenkins Now, install Jenkins using the following command:\nsudo apt-get install jenkins 5. Start Jenkins After the installation completes, start Jenkins:\nsudo systemctl start jenkins Visit for web interface.\nhttp://localhost:8080/ After you login to Jenkins, - Run the command to copy the Jenkins Admin Password - sudo cat /var/lib/jenkins/secrets/initialAdminPassword - Enter the Administrator password\n🐳 Grant Docker Permissions to Jenkins Grant Jenkins User permission to use docker daemon. Jenkins User is automatically created when installing Jenkins.\nsudo su - usermod -aG docker jenkins systemctl restart docker Now switch to Jenkins and see if Docker is running inside it\nsu - jenkins docker run hello-world Great. Now lets setup Jenkins from web.\nInstall Docker plugin inside Jenkins to run docker as agent.\nAfter successful installation restart the Jenkins. You can restart by simply adding /restart in URL.\nCreating Jenkins Pipeline For this project I\u0026rsquo;m using this repo. It has a frontend, backend, and SQL database—a simple to-do list app. The frontend calls the backend, and the backend interacts with the database to fetch, add, and delete list items.\nClick on Pipeline . Here we’ll have Jenkins file in our repo so Jenkins will fetch the file from there.\nNow select on apply and Save.\nGo to dashboard and start the build.\nBuild Success Lets check our application running on port 8000.\nSo this verifies our task running smoothly that is deployed using Jenkins.\nSetting Up Webhook Let\u0026rsquo;s set up a webhook in Jenkins and GitHub to enable a seamless CI process. When the webhook is enabled, GitHub sends an HTTP POST request to Jenkins with information about the push event. Jenkins listens for this request and triggers the configured pipeline to run, deploying the updated code.\nTo make Jenkins accessible to GitHub, we need to expose Jenkins running on localhost to the public. Since GitHub needs to send an HTTP POST request to a public IP, we’ll use ngrok for port forwarding. Ngrok creates a secure tunnel to your local machine, allowing GitHub to access Jenkins through a public UR\nSetting up ngrok Go to ngrok website and follow the instruction provided there for WSL.\nInstall ngrok via Apt with the following command:\ncurl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\ | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc \u0026gt;/dev/null \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://ngrok-agent.s3.amazonaws.com buster main\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/ngrok.list \\ \u0026amp;\u0026amp; sudo apt update \\ \u0026amp;\u0026amp; sudo apt install ngrok Run the following command to add your authtoken to the default ngrok.yml\nngrok config add-authtoken \u0026lt;your_auth_token\u0026gt; Now run the command:\nngrok http http://localhost:8080 The command ngrok http http://localhost:8080 creates a secure tunnel to your local Jenkins server running on port 8080, exposing it to the public internet via a unique ngrok URL.\nNow we have can access our port 8080 from any device with the given ngrok URL.\nSetting Up Web Hook Create a new pipeline and In trigger option select this option.\nGo to your GitHub repo\u0026rsquo;s Settings \u0026gt; Webhooks \u0026gt; Add webhook to configure the webhook.\nNow add URL provided by ngrok appended with /github-webhook.\nThen click on Add Webhook.\nLet\u0026rsquo;s make a change in our code and push it to GitHub to test if the webhook works:\nLets commit the changes and push it to Github.\nAs soon as i push the changes to GitHub the webhook triggers the Jenkins and runs our pipeline.\nBuild 4 is successful. Now lets check if our changes has been implemented or not.\nHere we can see our changes has been implemented.\nIn this blog, we wrote a Jenkins pipeline and set up a webhook to automate the CI/CD process in our local environment. To make Jenkins accessible to GitHub, we used ngrok to tunnel into our local machine. After pushing code to the repository, we observed Jenkins automatically triggering and running the pipeline, handling the deployment process without any manual intervention.\nThe repo used for this blog is https://github.com/agrimpaneru/dockerized-todo. Feel free to fork it and experiment with it.\n","permalink":"http://localhost:1313/prtfoio/blog/jenkins-webhooks-cicd/","summary":"\u003ch1 id=\"jenkins\"\u003eJenkins\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"-what-is-jenkins\"\u003e🚀 \u003cstrong\u003eWhat is Jenkins?\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eJenkins is an open-source automation server used primarily for \u003cstrong\u003econtinuous integration (CI) and continuous deployment (CD)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e to automate CI/CD for a project.\u003c/p\u003e\n\u003cp\u003eIn this Blog, I’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJenkins architecture\u003c/li\u003e\n\u003cli\u003eWriting a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e for automating CI/CD\u003c/li\u003e\n\u003cli\u003eSetting up webhook to trigger deployments automatically.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"jenkins-architecture\"\u003e\u003cstrong\u003eJenkins Architecture\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image%201.png\"\u003e\u003c/p\u003e","title":"Integrating Jenkins with Webhooks for Efficient CI/CD Automation"},{"content":"Jenkins 🚀 What is Jenkins? Jenkins is an open-source automation server used primarily for continuous integration (CI) and continuous deployment (CD).\nContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a Jenkins pipeline to automate CI/CD for a project.\nIn this Blog, I’ll cover:\nJenkins architecture Writing a Jenkins pipeline for automating CI/CD Setting up webhook to trigger deployments automatically. Jenkins Architecture Jenkins follows a master-agent setup (formerly called master-slave), where the Jenkins server (master) manages everything, and agents (slaves) do the actual work.\nJenkins Server (Master)\nManages jobs, users, security, and plugins. Schedules tasks and decides which agent should run them. Provides the web interface where you monitor and control everything. Jenkins Agent (Slave)\nRun the actual build and deployment tasks. Communicate with the master to receive instructions. Report back with logs and results. 💡Note: Since we\u0026rsquo;re running Jenkins locally, both the server and agent are on the same machine. The server manages jobs and the web interface, while the agent executes tasks—everything happens on our local system.\n📝 Jenkinsfile A Jenkinsfile defines the automation process using a declarative syntax. It tells Jenkins what to do, when to do it, and how to handle failures This is a Jenkins pipeline file written in Declarative Pipeline Syntax.\npipeline { agent any stages { stage(\u0026#39;Checkout\u0026#39;) { steps { checkout scm } } stage(\u0026#39;Cleanup\u0026#39;) { steps { script { // Stop and remove existing containers sh \u0026#39;\u0026#39;\u0026#39; docker-compose down # Additional cleanup in case docker-compose down didn\u0026#39;t work docker rm -f mysql-db || true \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Start MySQL\u0026#39;) { steps { script { // Start only MySQL sh \u0026#39;docker-compose up -d db\u0026#39; // Wait for 1 minute sh \u0026#39;sleep 60\u0026#39; echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; } } } stage(\u0026#39;Start Other Services\u0026#39;) { steps { script { // Start remaining services sh \u0026#39;docker-compose up -d\u0026#39; } } } } post { failure { sh \u0026#39;docker-compose down\u0026#39; echo \u0026#39;Deployment failed\u0026#39; } success { echo \u0026#39;Deployment successful\u0026#39; } } } Understanding Jenkinsfile 1. Checkout Stage: It Retrieves the code from the repository (using SCM — Source Code Management) so that the pipeline can work with the latest code. This command checks out the repository, ensuring that the pipeline works with the latest version of the code.\ncheckout scm 2. Cleanup Stage: It Stops and removes any existing containers from a previous deployment to ensure a clean environment before starting fresh\ndocker-compose down docker rm -f mysql-db || true docker-compose down stops all services defined in the docker-compose.yml and removes the containers. docker rm -f mysql-db || true forces the removal of the MySQL container (mysql-db), even if it\u0026rsquo;s not running. The || true ensures the pipeline doesn\u0026rsquo;t fail if the container doesn\u0026rsquo;t exist. 3. Start MySQL Stage: It Starts the MySQL container using docker-compose and waits for it to initialize before starting other services.\ndocker-compose up -d db sleep 60 echo \u0026#39;Waited 1 minute for MySQL to start\u0026#39; docker-compose up -d db starts the MySQL container (db service) in detached mode (d), allowing the pipeline to continue without waiting for MySQL to be ready. sleep 60 pauses the pipeline for 1 minute, giving MySQL time to start and initialize. 4.Start Other Services Stage: Starts all the remaining services defined in the docker-compose.yml (other than MySQL, which was started separately in the previous stage).This command starts the remaining services (e.g., frontend, backend) in detached mode.\ndocker-compose up -d 5. Post Section: Defines steps that should be executed after the pipeline has completed, regardless of success or failure.\nIf the deployment fails, the docker-compose down command is executed to clean up containers, and a failure message is printed:\ndocker-compose down echo \u0026#39;Deployment failed\u0026#39; If the deployment is successful, a success message is printed:\necho \u0026#39;Deployment successful\u0026#39; 🔧 Setting Up Jenkins on Ubuntu Follow this to install Jenkins in Ubuntu.\n1. Add the Jenkins Repository Key Run the following command to download the Jenkins repository key and store it:\nsudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\ https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key 2. Add the Jenkins Repository Now, add the Jenkins repository to your package sources list using the key you just downloaded:\necho \u0026#34;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\u0026#34; \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null 3. Update Package Lists Next, update your package list to include the newly added Jenkins repository:\nsudo apt-get update 4. Install Jenkins Now, install Jenkins using the following command:\nsudo apt-get install jenkins 5. Start Jenkins After the installation completes, start Jenkins:\nsudo systemctl start jenkins Visit for web interface.\nhttp://localhost:8080/ After you login to Jenkins, - Run the command to copy the Jenkins Admin Password - sudo cat /var/lib/jenkins/secrets/initialAdminPassword - Enter the Administrator password\n🐳 Grant Docker Permissions to Jenkins Grant Jenkins User permission to use docker daemon. Jenkins User is automatically created when installing Jenkins.\nsudo su - usermod -aG docker jenkins systemctl restart docker Now switch to Jenkins and see if Docker is running inside it\nsu - jenkins docker run hello-world Great. Now lets setup Jenkins from web.\nInstall Docker plugin inside Jenkins to run docker as agent.\nAfter successful installation restart the Jenkins. You can restart by simply adding /restart in URL.\nCreating Jenkins Pipeline For this project I\u0026rsquo;m using this repo. It has a frontend, backend, and SQL database—a simple to-do list app. The frontend calls the backend, and the backend interacts with the database to fetch, add, and delete list items.\nClick on Pipeline . Here we’ll have Jenkins file in our repo so Jenkins will fetch the file from there.\nNow select on apply and Save.\nGo to dashboard and start the build.\nBuild Success Lets check our application running on port 8000.\nSo this verifies our task running smoothly that is deployed using Jenkins.\nSetting Up Webhook Let\u0026rsquo;s set up a webhook in Jenkins and GitHub to enable a seamless CI process. When the webhook is enabled, GitHub sends an HTTP POST request to Jenkins with information about the push event. Jenkins listens for this request and triggers the configured pipeline to run, deploying the updated code.\nTo make Jenkins accessible to GitHub, we need to expose Jenkins running on localhost to the public. Since GitHub needs to send an HTTP POST request to a public IP, we’ll use ngrok for port forwarding. Ngrok creates a secure tunnel to your local machine, allowing GitHub to access Jenkins through a public UR\nSetting up ngrok Go to ngrok website and follow the instruction provided there for WSL.\nInstall ngrok via Apt with the following command:\ncurl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\ | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc \u0026gt;/dev/null \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://ngrok-agent.s3.amazonaws.com buster main\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/ngrok.list \\ \u0026amp;\u0026amp; sudo apt update \\ \u0026amp;\u0026amp; sudo apt install ngrok Run the following command to add your authtoken to the default ngrok.yml\nngrok config add-authtoken \u0026lt;your_auth_token\u0026gt; Now run the command:\nngrok http http://localhost:8080 The command ngrok http http://localhost:8080 creates a secure tunnel to your local Jenkins server running on port 8080, exposing it to the public internet via a unique ngrok URL.\nNow we have can access our port 8080 from any device with the given ngrok URL.\nSetting Up Web Hook Create a new pipeline and In trigger option select this option.\nGo to your GitHub repo\u0026rsquo;s Settings \u0026gt; Webhooks \u0026gt; Add webhook to configure the webhook.\nNow add URL provided by ngrok appended with /github-webhook.\nThen click on Add Webhook.\nLet\u0026rsquo;s make a change in our code and push it to GitHub to test if the webhook works:\nLets commit the changes and push it to Github.\nAs soon as i push the changes to GitHub the webhook triggers the Jenkins and runs our pipeline.\nBuild 4 is successful. Now lets check if our changes has been implemented or not.\nHere we can see our changes has been implemented.\nIn this blog, we wrote a Jenkins pipeline and set up a webhook to automate the CI/CD process in our local environment. To make Jenkins accessible to GitHub, we used ngrok to tunnel into our local machine. After pushing code to the repository, we observed Jenkins automatically triggering and running the pipeline, handling the deployment process without any manual intervention.\nThe repo used for this blog is https://github.com/agrimpaneru/dockerized-todo. Feel free to fork it and experiment with it.\n","permalink":"http://localhost:1313/prtfoio/blog/jenkins-webhooks-cicd/","summary":"\u003ch1 id=\"jenkins\"\u003eJenkins\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"-what-is-jenkins\"\u003e🚀 \u003cstrong\u003eWhat is Jenkins?\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eJenkins is an open-source automation server used primarily for \u003cstrong\u003econtinuous integration (CI) and continuous deployment (CD)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eContinuous Integration is an integral part of DevOps, and Jenkins is the most famous Continuous Integration tool. In this article, I will focus on Jenkins architecture then, I\u0026rsquo;ll walk you through writing a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e to automate CI/CD for a project.\u003c/p\u003e\n\u003cp\u003eIn this Blog, I’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJenkins architecture\u003c/li\u003e\n\u003cli\u003eWriting a \u003cstrong\u003eJenkins pipeline\u003c/strong\u003e for automating CI/CD\u003c/li\u003e\n\u003cli\u003eSetting up webhook to trigger deployments automatically.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"jenkins-architecture\"\u003e\u003cstrong\u003eJenkins Architecture\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"image%201.png\"\u003e\u003c/p\u003e","title":"Integrating Jenkins with Webhooks for Efficient CI/CD Automation"},{"content":"LMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\nIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u0026ldquo;Attention Is All You Need\u0026rdquo; shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\nEmbeddings representing the Words as vectors Initially the words are converted into tokens, Here we assume tokens are equal to index.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] Output for this code is :\n[tensor([0.8823, 0.9150, 0.3829]), tensor([0.9593, 0.3904, 0.6009]), tensor([0.2566, 0.7936, 0.9408]), tensor([0.1332, 0.9346, 0.5936]), tensor([0.8694, 0.5677, 0.7411])] Here the corresponding Embeddings for each word is initialized randomly. Here we represented each word as 3 dimensional embedding.\n{\u0026#39;the\u0026#39;: tensor([0.8823, 0.9150, 0.3829]), \u0026#39;cat\u0026#39;: tensor([0.9593, 0.3904, 0.6009]), \u0026#39;sat\u0026#39;: tensor([0.2566, 0.7936, 0.9408]), \u0026#39;on\u0026#39;: tensor([0.1332, 0.9346, 0.5936]), \u0026#39;mat\u0026#39;: tensor([0.8694, 0.5677, 0.7411])} Attention Mechanism Attention Mechanism is inspired by how humans focus on specific parts of information when processing it—just like when we read a sentence, we don\u0026rsquo;t focus on every word equally. Instead, we \u0026ldquo;attend\u0026rdquo; to the most relevant words that help us understand the meaning.\nLets take an example for understanding attention mechanism:\nImagine reading the sentence: \u0026ldquo;The cat sat on the mat.\u0026rdquo;\nIf asked, \u0026ldquo;Where is the cat?\u0026rdquo;, attention would primarily focus on the word \u0026ldquo;cat\u0026rdquo; and the word \u0026ldquo;mat\u0026rdquo;. Other words like \u0026ldquo;the\u0026rdquo; or \u0026ldquo;on\u0026rdquo; are ignored since they don’t carry much relevance to the question.\nLets take a sentence:\nI am going to cinema to watch ………?\nThe most probable answers would be \u0026ldquo;movie,\u0026rdquo; \u0026ldquo;action movie,\u0026rdquo; or something similar. Words like \u0026ldquo;book\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; don\u0026rsquo;t fit the context and are irrelevant to predicting the correct word. The key to predicting the correct word is in the context and information provided in the preceding words. To make accurate predictions, we need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the model to focus on the most relevant words to that context to make predictions.\nUnderstanding Scaled Dot-Product Attention The paper Attention Is All You Need introduced Scaled Dot-Product Attention .\nLets first calculate the Similarity between Query and Key\nThis is done as\nQ = K = V = embeddings similarity_matrix=torch.matmul(Q,K.T) In our case the dimension of output is 5*5.\nSimilarity Matrix:\ntensor([[1.7622, 1.4337, 1.3127, 1.1999, 1.5702], [1.4337, 1.4338, 1.1213, 0.8494, 1.5010], [1.3127, 1.1213, 1.5807, 1.3343, 1.3708], [1.1999, 0.8494, 1.3343, 1.2435, 1.0863], [1.5702, 1.5010, 1.3708, 1.0863, 1.6274]]) Here’s each value represents the similarity between words as described below.\nthe cat sat on mat the the-the the-cat the-sat the-on the-mat cat cat-the cat-cat cat-sat cat-on cat-mat sat sat-the sat-cat sat-sat sat-on sat-mat on on-the on-cat on-sat on-on on-mat mat mat-the mat-cat mat-sat mat-on mat-mat The similarity matrix captures the similarities between the query and key vectors. Higher similarity means the model should focus more on that word when making predictions, while lower similarity means the word is less relevant.\nScaling the compatibility matrix In the second step, we scale the dot-product of the query and key vectors by a factor of,\n$$ \\frac{1}{d_k} $$ . In our case We’ve only taken dimension of 3.\nHowever, when the dimension d is very large, dot-product attention without this scaling factor performs poorly. The authors of the paper “Attention Is All You Need” attribute this poor performance to having large variance.\nAs d_k increases, the variance of the dot product also becomes larger, leading to some values becoming extremely small after being passed through the softmax function. This causes issues during gradient calculation, similar to the vanishing gradient problem, where the gradients become too small to effectively update the model. To mitigate this, the dot product is scaled by $$ \\sqrt{\\frac{1}{d_k}} $$\nbefore applying softmax , which stabilizes the training process and improves performance.\nTo visualize this effect lets take output from softmax before and after scaling for embedding dimension of 256.\nApplying Softmax Activation The softmax function is used to convert the logits into probabilities. It takes a vector of raw scores (logits) and transforms them into a probability distribution. The softmax function is defined as:\n$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\nSoftmax is applied pointwise, so it doesn\u0026rsquo;t change the dimension of the input vector.\nimport torch.nn.functional as F similarity_matrix_scaled=similarity_matrix/(1/torch.sqrt(torch.tensor(3))) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) softmax_similarity_matrix_scaled Its output is:\ntensor([[0.2372, 0.1962, 0.1830, 0.1714, 0.2123], [0.2179, 0.2180, 0.1820, 0.1555, 0.2266], [0.1957, 0.1752, 0.2285, 0.1982, 0.2024], [0.2058, 0.1681, 0.2224, 0.2110, 0.1927], [0.2154, 0.2070, 0.1920, 0.1629, 0.2227]]) It can be visualized in Heatmap as\nComputing the Context Vector as final output. So for final step We multiply the attention matrix from previous step and multiply it with the Value matrix V. The final product gives the new context vector for all the words in vocabulary in our case 5.\nnew_context=torch.matmul(softmax_similarity_matrix_scaled,V) new_context Output:\ntensor([[0.6518, 0.7195, 0.6399], [0.6658, 0.7029, 0.6459], [0.6018, 0.7289, 0.6628], [0.5955, 0.7371, 0.6571], [0.6532, 0.7090, 0.6492]]) Conclusion In this section, we saw how contextual embeddings are generated using self-attention. We also discussed the importance of scaling and how to implement a simple self-attention block. Notably, there were no trainable parameters involved here, and we treated the Query (Q), Key (K), and Value (V) matrices as all being the same.\nHowever, to make the attention block trainable, we need to introduce learnable weights when deriving the Q, K, and V vectors from the original embedding. These weights will allow the model to adjust and fine-tune the attention mechanism during training. In the next section, we\u0026rsquo;ll explore how to implement this by adding trainable parameters to the attention block\nAttention Mechanism with trainable Parameters Alright! Now that we understand how self-attention is calculated, let\u0026rsquo;s take it a step further by introducing trainable parameters so that the model can learn how to find the context of words effectively.\nAdding Trainable Parameters to Self-Attention Q (Query), K (Key), and V (Value) matrices are not just direct copies of word embeddings. Instead, they are learned transformations of these embeddings.\nTo achieve this, we introduce three trainable weight matrices:\nW_q (Query weight matrix): Learns how to project word embeddings into the query space. W_k (Key weight matrix): Learns how to project word embeddings into the key space. W_v (Value weight matrix): Learns how to project word embeddings into the value space. Each of these matrices will be optimized during training.\nW_q=torch.randn(embedding_dim,embedding_dim) W_v=torch.randn(embedding_dim,embedding_dim W_k=torch.randn(embedding_dim,embedding_dim) Note that the dimension of these matrix can be made different. However the first dimension must be equal to embedding dimension to satisfy the matrix multiplication condition. Changing the second dimension implies projecting the embedding dimension from one dimension to other .\nLets get the new Query, Key and Value vector.\nQ = torch.matmul(embeddings, W_q) # (5, 3) K = torch.matmul(embeddings, W_k) # (5, 3) V = torch.matmul(embeddings, W_v) # (5, 3) Then we calculate the self attention similar to how we did above.\nsimilarity_matrix = torch.matmul(Q, K.T) # (5, 5) similarity_matrix_scaled = similarity_matrix / torch.sqrt(torch.tensor(embedding_dim, dtype=torch.float32)) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(softmax_similarity_matrix_scaled, V) This is what the final flow looks like.\nHere Lets wrap up this in Class Based Implementation\nimport torch import torch.nn.functional as F class SelfAttention: def __init__(self, embedding_dim): torch.manual_seed(42) self.embedding_dim = embedding_dim # Initialize weight matrices self.W_q = torch.randn(embedding_dim, embedding_dim) self.W_k = torch.randn(embedding_dim, embedding_dim) self.W_v = torch.randn(embedding_dim, embedding_dim) def forward(self, embeddings): # Compute Query, Key, and Value matrices Q = torch.matmul(embeddings, self.W_q) K = torch.matmul(embeddings, self.W_k) V = torch.matmul(embeddings, self.W_v) # Compute similarity (dot product attention) similarity_matrix = torch.matmul(Q, K.T) # (num_words, num_words) # Scale by sqrt(embedding_dim) similarity_matrix_scaled = similarity_matrix / torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32) ) # Apply softmax to get attention weights attention_weights = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(attention_weights, V) return new_context, attention_weights embedding_dim = 3 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] # Generate random embeddings for words embeddings = torch.stack([torch.rand(embedding_dim) for _ in words]) # Initialize attention mechanism self_attention = SelfAttention(embedding_dim) # Forward pass to compute attention new_context, attention_weights = self_attention.forward(embeddings) print(\u0026#34;New Context Vectors:\\n\u0026#34;, new_context) print(\u0026#34;\\nAttention Weights:\\n\u0026#34;, attention_weights) Output:\nNew Context Vectors: tensor([[ 0.3689, -0.4890, 1.3677], [ 0.2569, -0.4673, 1.3297], [ 0.3284, -0.4752, 1.3515], [ 0.1971, -0.4685, 1.3124], [ 0.3061, -0.4846, 1.3491]]) Attention Weights: tensor([[0.4672, 0.0538, 0.0879, 0.0829, 0.3082], [0.3362, 0.1266, 0.1921, 0.1077, 0.2375], [0.4212, 0.0851, 0.1492, 0.0815, 0.2631], [0.2728, 0.1547, 0.1886, 0.1526, 0.2314], [0.3922, 0.0832, 0.1179, 0.1143, 0.2923]]) So this ends up this blog. Congratulations on making it to the end. Feel free to experiment with different dimensions and input data to see how the self-attention mechanism adapts and scales.\nReferences Implementing Self-Attention from Scratch in PyTorch | by Mohd Faraaz | Medium\nUnderstanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\nSelf Attention in Transformers | Deep Learning | Simple Explanation with Code!\nSelf-attention from scratch | Blogs by Anil\n","permalink":"http://localhost:1313/prtfoio/blog/self-attention-pytorch/","summary":"\u003cp\u003eLMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\u003c/p\u003e\n\u003cp\u003eIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper \u003cem\u003e\u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/em\u003e shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\u003c/p\u003e","title":"Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch"},{"content":"Implementing Multihead attention from scratch with pytorch In our previous article, we built Self-Attention from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\nNow, let’s take things a step further and implement Multi-Head Attention from scratch. This post focuses more on the implementation rather than the theory, so I assume you’re already familiar with how self-attention works.\nLet’s get started!\nMultihead Attention Similar to Self-Attention, Multi-Head Attention also creates a new context vector. But instead of using a single attention mechanism, it uses multiple heads, each learning different relationships between words.\nFor example, consider these two sentences:\n1️⃣ I went to the bank to withdraw cash.\n2️⃣ I went for a picnic by the river bank.\nHere, the word \u0026ldquo;bank\u0026rdquo; has different meanings depending on the context. The idea behind Multi-Head Attention is that different heads can focus on different aspects of a word’s meaning, capturing various relationships more effectively.\nIn the above example with a single attention head, the model might have difficulty capturing both the financial and geographical meanings of the word \u0026ldquo;bank\u0026rdquo; at the same time. Multi-Head Attention, on the other hand, allows the model to learn both meanings separately—one head could focus on financial context while another focuses on the geographical context. This way, the model learns richer, more nuanced embeddings of the word \u0026ldquo;bank\u0026rdquo; based on different contexts.\nFor a more intuitive explanation of Multi-Head Attention, I highly recommend checking out this medium post.\nNow Lets Implement it.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 8 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) Similar to our previous Self-Attention implementation, we’ll use the following sentence with a random embedding of dimension 8. However, in real-world applications, embeddings are typically a combination of word embeddings and positional encodings. To keep things simple, we won’t dive into that here. Now, let’s define the Wq, Wk, and Wv matrices. We could use separate smaller weight matrices to get the Query, Key, and Value vectors directly, but a more efficient approach is to use a single large weight matrix and split it afterward for multiple attention heads. This method is more computationally efficient, as it allows for parallel processing across heads.\nWq = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wk = torch.rand(embedding_dim, embedding_dim) # (8, 8) Wv = torch.rand(embedding_dim, embedding_dim) # (8, 8) Lets apply the projection to Get Query Key and Value matrix.\n# Apply projection Q = embeddings @ Wq # Shape: (5, 8) K = embeddings @ Wk # Shape: (5, 8) V = embeddings @ Wv # Shape: (5, 8) Now ,We’ll split the projected vectors into multiple heads . Remember the Embedding dimension must be exactly divisible by head\nd_k = embedding_dim // num_heads Now we obtain the Query Key and Value Matrix for multiple heads.\nQ = Q.view(seq_len, num_heads, d_k).transpose(0, 1) K = K.view(seq_len, num_heads, d_k).transpose(0, 1) V = V.view(seq_len, num_heads, d_k).transpose(0, 1) The output shape of Q, K, and V after the split will be (2, 5, 4), where:\n2 corresponds to the number of heads, 5 corresponds to the sequence length (number of words), and 4 is the new dimension of each vector for each head. We calculate attention matrix by,\nattention_scores=Q @ K.transpose(-2,-1) \u0026#39;\u0026#39;\u0026#39;For 3D matrix multiplication, the first dimension of both matrices should match, #while the second dimension of the first matrix must align with the third dimension of the second matrix according to the matrix multiplication rule\u0026#39;\u0026#39;\u0026#39; Here, we get two attention scores from each head. We’ll proceed with scaling these scores and applying the softmax activation, just like in Self-Attention. For a detailed explanation of this process, please refer to my previous post on Self-Attention\nBut before applying the scaling and softmax activation, we’ll mask the attention scores. Masking is necessary to ensure that the prediction for a given token only depends on the current and previous tokens. Without masking, the model could \u0026ldquo;cheat\u0026rdquo; by looking ahead at future tokens which in not desirable.\nMasked Multi-Head Attention In the decoder of a transformer, we use masked multi-head attention to prevent the model from looking at future tokens when predicting the next one. This ensures that the model can only base its prediction on past tokens, not the future ones. In contrast, the encoder doesn’t need masking because it has access to the entire sequence at once, so it can freely attend to all parts of the input.\nThis masking enables the model to process all tokens in parallel during training, enhancing efficiency.\nWe will implement Masked Multi-Head Attention, ensuring that the model only attends to past words by masking the attention scores. This prevents the decoder from seeing future words, enforcing an autoregressive learning process\nMasking is applied before softmax by setting forbidden positions to−∞ to ensure they receive zero probability after normalization.\nmask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) mask = mask.bool() mask=mask.unsqueeze(0) attention_scores=attention_scores.masked_fill(mask,-1e11) attention_scores=attention_scores/math.sqrt(d_k) #This is mask i.e Upper triangular matrix tensor([[0., 1., 1., 1., 1.], [0., 0., 1., 1., 1.], [0., 0., 0., 1., 1.], [0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.]]) #Attention Scores before masking tensor([[[24.6185, 22.6470, 19.6726, 10.3703, 23.2266], [25.3424, 23.5643, 20.2438, 10.8568, 24.0848], [20.2856, 18.6674, 16.2010, 8.5272, 19.1661], [11.0366, 10.1522, 8.8451, 4.5771, 10.3952], [23.4003, 21.7570, 18.6598, 10.0209, 22.2936]], [[24.1949, 25.1107, 20.5201, 12.3558, 24.2538], [24.3185, 25.1608, 20.5903, 12.5245, 24.4242], [19.3101, 20.0426, 16.3541, 9.9390, 19.3887], [11.4448, 11.6265, 9.6817, 5.8675, 11.3187], [23.9947, 24.8808, 20.3205, 12.3524, 24.1055]]]) #Attention Scores after masking tensor([[[ 2.4619e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.5342e+01, 2.3564e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.0286e+01, 1.8667e+01, 1.6201e+01, -1.0000e+11, -1.0000e+11], [ 1.1037e+01, 1.0152e+01, 8.8451e+00, 4.5771e+00, -1.0000e+11], [ 2.3400e+01, 2.1757e+01, 1.8660e+01, 1.0021e+01, 2.2294e+01]], [[ 2.4195e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 2.4318e+01, 2.5161e+01, -1.0000e+11, -1.0000e+11, -1.0000e+11], [ 1.9310e+01, 2.0043e+01, 1.6354e+01, -1.0000e+11, -1.0000e+11], [ 1.1445e+01, 1.1626e+01, 9.6817e+00, 5.8675e+00, -1.0000e+11], [ 2.3995e+01, 2.4881e+01, 2.0321e+01, 1.2352e+01, 2.4105e+01]]]) So what we did here was created a lower triangular matrix and applied it as a mask to attention score and scaled it and passed to softmax to ensure each token can only attend to itself and previous tokens.\nAfter softmax, the row sum will be one and this ensures the attention weights are a valid probability distribution, allowing the model to learn proportional contributions from past tokens.\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5130, 0.4870, 0.0000, 0.0000, 0.0000], [0.3420, 0.3315, 0.3265, 0.0000, 0.0000], [0.2540, 0.2510, 0.2486, 0.2465, 0.0000], [0.2029, 0.2000, 0.1984, 0.1980, 0.2007]], [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.4935, 0.5065, 0.0000, 0.0000, 0.0000], [0.3342, 0.3391, 0.3267, 0.0000, 0.0000], [0.2521, 0.2528, 0.2485, 0.2466, 0.0000], [0.2006, 0.2022, 0.1984, 0.1981, 0.2007]]]) Here is what out attention scores looks like.\nNow we multiply out Attention score with Value matrix and concatenate it,\noutput=attention_scores@V output.transpose(0,1).reshape(seq_len,embedding_dim) After Concatenation we achieve the final Contextual embedding of each vectors.\nHere\u0026rsquo;s a cleaner and more efficient implementation of the Multi-Head Attention module in PyTorch, wrapped in a class:\nimport torch import math class MultiHeadAttention(torch.nn.Module): def __init__(self, num_heads, embedding_dim): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.embedding_dim = embedding_dim self.d_k = embedding_dim // num_heads self.Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) self.Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim)) def forward(self, embeddings): seq_len = embeddings.size(0) Q = embeddings @ self.Wq # (seq_len, embedding_dim) K = embeddings @ self.Wk V = embeddings @ self.Wv # Converting to multiheaded attention Q = Q.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # (num_heads, seq_len, d_k) K = K.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) V = V.view(seq_len, self.num_heads, self.d_k).transpose(0, 1) # Compute attention scores attention_scores = torch.matmul(Q, K.transpose(-2, -1)) # (num_heads, seq_len, seq_len) # Apply mask (upper triangular mask for causal attention) mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() mask = mask.unsqueeze(0).expand_as(attention_scores) attention_scores = attention_scores.masked_fill(mask, -1e11) # Scale the attention scores attention_scores = attention_scores / math.sqrt(self.d_k) # Apply softmax to get attention weights attention_weights = torch.softmax(attention_scores, dim=-1) # (num_heads, seq_len, seq_len) # Compute the output (weighted sum of values) output = torch.matmul(attention_weights, V) output = output.transpose(0, 1).contiguous().view(seq_len, self.embedding_dim) return output # Example usage torch.manual_seed(42) embedding_dim = 8 num_heads = 2 words = [\u0026#39;the\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;sat\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;mat\u0026#39;] embeddings = [torch.rand(embedding_dim) for word in words] embeddings = torch.stack(embeddings) mha = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim) # Forward pass through the model output = mha(embeddings) print(\u0026#34;Output shape:\u0026#34;, output.shape) ","permalink":"http://localhost:1313/prtfoio/blog/multi-head-attention-pytorch/","summary":"\u003ch1 id=\"implementing-multihead-attention-from-scratch-with-pytorch\"\u003eImplementing Multihead attention from scratch with pytorch\u003c/h1\u003e\n\u003cp\u003eIn our \u003ca href=\"https://agrimpaneru.com.np/blog/self-attention-pytorch/\"\u003eprevious article\u003c/a\u003e, we built \u003cstrong\u003eSelf-Attention\u003c/strong\u003e from scratch using PyTorch. If you haven’t checked that out yet, I highly recommend giving it a read before reading this one!\u003c/p\u003e\n\u003cp\u003eNow, let’s take things a step further and implement \u003cstrong\u003eMulti-Head Attention\u003c/strong\u003e from scratch. This post focuses more on the \u003cstrong\u003eimplementation\u003c/strong\u003e rather than the theory, so I assume you’re already familiar with how self-attention works.\u003c/p\u003e\n\u003cp\u003eLet’s get started!\u003c/p\u003e","title":"MultiHead Attention Explained:Implementing Masked Multihead attention from Scratch in PyTorch"},{"content":"LSTM from Scratch In this post, we will implement a simple next word predictor LSTM from scratch using torch.\nA gentle Introduction to LSTM Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997). As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\nThe LSTM model we’re gonna implement follows this architecture.\nTo learn about their detailed structure here is a reference to this awesome blog Understanding LSTM Networks \u0026ndash; colah\u0026rsquo;s blog.\nFor simplified architecture:\nImports Lets import the required library\nimport torch import torch.nn as nn import torch.nn.functional as F Data Preparation We’ll be using autoregressive sentence generation for our dataset. This approach involves predicting the next word in a sequence based on the words that came before it.\nConsider a sentence,\n\u0026ldquo;I am Peter the hero.\u0026rdquo;\nInput Sequence Target Output [I] AM [I, AM] Peter [ I, AM, Peter] THE [I, AM, Peter, THE] HERO We\u0026rsquo;ll be working with George Orwell\u0026rsquo;s essay The Spike as our dataset. Since the dataset is relatively small, it allows us to focus on developing a deeper intuition without getting overwhelmed by the volume of data.\nTokenization Next, I\u0026rsquo;ll create a dictionary to convert words into indices—a common practice in Natural Language Processing (NLP). Instead of relying on libraries, I\u0026rsquo;ll build it from scratch to better understand how it works.\nchar2idx={char:i for i,char in enumerate(set(data))} # char2idx[\u0026#39;.\u0026#39;]=0 idx2char={i:char for i,char in enumerate(set(data))} We can run char2idx to view the word mapping to indices.\nx=[] for i in orgi.split(\u0026#34;\\n\u0026#34;): x.append([char2idx[j] for j in i.split(\u0026#34; \u0026#34;)]) This snippet processes a text dataset to convert words into their corresponding indices using the char2idx dictionary we created earlier.\nX_train = [] # List to store input sequences Y_train = [] # List to store corresponding outputs for sequence in x: for i in range(1, len(sequence)-1): # Input is the subsequence from start to the ith element X_train.append(sequence[:i]) # Output is the ith element (next token) Y_train.append(sequence[i]) This code prepares the dataset for autoregressive sentence generation, a technique where the model predicts the next token in a sequence based on previous tokens\nFor example, if the sequence is [1, 2, 3, 4], the resulting pairs would look like this:\nInput: [1], Output: 2 Input: [1, 2], Output: 3 Input: [1, 2, 3], Output: 4 Since LSTMs require input sequences of the same length, I padded all sequences to a fixed maximum length. I chose to pad at the beginning of the sequence because it preserves the actual data at the end, which is where the LSTM focuses more when processing the sequence. This ensures that the more meaningful tokens remain closer to the output\nmax_len=max([len(i) for i in X_train]) vocab_size=len(set(data)) def pre_pad_sequences_pytorch(sequences, max_len): padded_sequences = [] for seq in sequences: # If the sequence is shorter than max_len, pad with zeros at the beginning if len(seq) \u0026lt; max_len: padded_seq = [0] * (max_len - len(seq)) + seq # Pre-padding with 0 # If the sequence is longer than max_len, truncate it else: padded_seq = seq[-max_len:] padded_sequences.append(padded_seq) return torch.tensor(padded_sequences) X_train_padded = pre_pad_sequences_pytorch(X_train, max_len) X_train_padded=X_train_padded.unsqueeze(-1) Y_train=torch.tensor(Y_train) This creates a dataset ready to be set into LSTM network.\nFinally, the LSTM ##### Long Short-Term Memory Network Class ##### class LSTM: def __init__(self, vocab_size, embedding_dim, hidden_size, output_size): self.hidden_size = hidden_size #embedding layer self.embedding = torch.randn(vocab_size, embedding_dim, requires_grad=True) # Initialize weights with requires_grad=True #forget gate self.Wf = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bf = torch.zeros(hidden_size, requires_grad=True) #input gate self.Wi = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bi = torch.zeros(hidden_size, requires_grad=True) #candidate gate self.Wc = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bc = torch.zeros(hidden_size, requires_grad=True) #output gate self.Wo = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True) self.bo = torch.zeros(hidden_size, requires_grad=True) #final gate self.Wv = torch.randn(output_size, hidden_size, requires_grad=True) self.bv = torch.zeros(output_size, requires_grad=True) self._initialize_weights() This is a custom implementation of a Long Short-Term Memory (LSTM) network. It takes four inputs\nvocab_size,embedding_dim,hidden_size and output_size vocab_size is the number of unique words in your dataset.\nembedding_dim is the size of the word embeddings. Instead of representing words as simple indices (like 1, 2, 3, etc.), we represent them as dense vectors (e.g., [0.2, -0.5, 0.7, \u0026hellip;]).\nWhy embedding layer is needed ? Words are initially represented as indices (e.g., \u0026ldquo;cat\u0026rdquo; = 1, \u0026ldquo;dog\u0026rdquo; = 2, etc.). But these indices don’t carry any meaningful information about the words. The embedding layer converts these indices into dense vectors of size embedding_dim. These vectors are trainable, meaning the model will learn the best way to represent each word during training. For example, if embedding_dim =4, the word \u0026ldquo;cat\u0026rdquo; might be represented as a vector like [0.1, -0.3, 0.5, 0.9]. https://medium.com/analytics-vidhya/embeddings-the-what-the-why-and-the-how-15a6a3c99ce8 This blogs deeps dives into the working and importance of Embeddings layer.\nThe other function we’ll be defining is a Xavier Initialization. Recurrent neural networks are very sensitive to the initialization used, so choosing the right one is important. It’s probably enough to know that the Xavier Initialization is a good choice; however, we can take a look at the math here.\ndef _initialize_weights(self): nn.init.xavier_uniform_(self.Wf) nn.init.xavier_uniform_(self.Wi) nn.init.xavier_uniform_(self.Wc) nn.init.xavier_uniform_(self.Wo) nn.init.xavier_uniform_(self.Wv) def parameters(self): # Return a list of all parameters (weights and biases) in the model return [self.Wf, self.bf, self.Wi, self.bi, self.Wc, self.bc, self.Wo, self.bo, self.Wv, self.bv, self.embedding] The next function to define is the forward function.\ndef forward(self, x, init_states=None): # Apply embedding layer to input indices x=x.squeeze(dim=-1) x = self.embedding[x] batch_size, seq_len, _ = x.size() # Initialize h_t and c_t if init_states is None if init_states is None: h_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) c_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True) else: h_t, c_t = init_states outputs = [] for t in range(seq_len): x_t = x[:, t, :] # Shape: (batch_size, embedding_dim) Z_t = torch.cat([x_t, h_t], dim=1) # Shape: (batch_size, embedding_dim + hidden_size) # Forget gate ft = torch.sigmoid(Z_t @ self.Wf.t() + self.bf) # Input gate it = torch.sigmoid(Z_t @ self.Wi.t() + self.bi) # Candidate cell state can = torch.tanh(Z_t @ self.Wc.t() + self.bc) # Output gate ot = torch.sigmoid(Z_t @ self.Wo.t() + self.bo) c_t = c_t * ft + can * it h_t = ot * torch.tanh(c_t) # Compute output for current time step y_t = h_t @ self.Wv.t() + self.bv return y_t, (h_t, c_t) When building a model for next-word prediction, the goal is to predict the most likely word that follows a given sequence. In this context, we don’t need outputs from every timestep of the sequence—only the final output (from the last timestep) is necessary to make the prediction.\nNow lets initialize our Model.\nWe’ll be using a hidden size of 128 and an embedding dimension of 10 for our model. For comparison, GPT-2 Small (with 117 million parameters) used an embedding dimension of 768, and the latest GPT-4 uses 16,384. Since we are working with a small dataset, a smaller embedding dimension should work, and I chose 10 . You can play with this value to see how it plays out.\nmodel = LSTM(vocab_size=vocab_size, embedding_dim=128, hidden_size=128, output_size=vocab_size) params = model.parameters() optimizer = torch.optim.Adam(params, lr=0.005) Training Loop hidden_state = None # Initialize hidden state for _ in range(500): # Sample a batch batch_indices = torch.randint(0, X_train_padded.shape[0], (128,)) x_train = X_train_padded[batch_indices] # Shape: (batch_size, seq_len) # Forward pass outputs,hidden_state = model.forward(x_train, init_states=hidden_state) # print(outputs.shape) h_t, c_t = hidden_state hidden_state = (h_t.detach(), c_t.detach()) # Detach hidden state for next batch # Compute loss y_batch = Y_train[batch_indices] # Shape: (batch_size, seq_len, vocab_size) loss = criterion(outputs, y_batch) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(loss) This training approach grabs a random batch of 128 data points, puts them into the model, and hopes for the best, kind of like cramming for an exam with random notes you found in your bag. While it’s not the most efficient way to train, I went with it to really wrap my head around how batches flow through the network. After each iteration, the hidden state is politely told, \u0026ldquo;Take a break, you’ve done enough,\u0026rdquo; and detached. This prevents PyTorch from becoming a hoarder and building a massive computation graph that would slow things down to a crawl. For optimization, I used the Adam optimizer. Though simple, this setup helped me grasp the inner workings of batching and hidden state management in LSTMs.\nBefore we begin training the network, let’s see how our model performs with its randomly initialized weights. This will give us a baseline idea of what kind of text it generates before learning anything meaningful from the data.\nThe following code preprocess the input and passes it to the model\ndef generate_sequence(model, seed_string, char2idx, idx2char, sequence_length, max_len=55): seed_indices = [char2idx[word] for word in seed_string.split(\u0026#34; \u0026#34;) if word in char2idx] seed_tensor = torch.tensor(seed_indices).unsqueeze(0) # Shape: (1, seq_len) generated_indices = seed_indices[:] hidden_state = None for _ in range(sequence_length): # Pre-pad the input sequence to match the model\u0026#39;s expected input size padded_input = pre_pad_sequences_pytorch([generated_indices], max_len).unsqueeze(-1) # Get the model output and hidden state output, (hidden_state) = model.forward(padded_input, hidden_state) # Take the output corresponding to the last token next_token_logits = output # Shape: (1, vocab_size) # Use softmax to get probabilities and sample the next token next_token_prob = torch.softmax(next_token_logits, dim=-1) # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item() next_token_idx=torch.argmax(next_token_prob).item() # Append the predicted token to the sequence generated_indices.append(next_token_idx) # Convert indices back to characters generated_words = [idx2char[idx] for idx in generated_indices] return \u0026#34; \u0026#34;.join(generated_words) # Example usage: seed_string = \u0026#34;scum\u0026#34; sequence_length = 20 generated_text = generate_sequence(model, seed_string, char2idx, idx2char, sequence_length) print(\u0026#34;Generated Text:\u0026#34;) print(generated_text) Now for a sequence_length of 20 this is what our model outputs.\nIt is.\u0026#39; if bath, is.\u0026#39; if bath, cockney side, black serve is.\u0026#39; go three asleep straw bath, is.\u0026#39; cotton when when This up apparatus kind where Majors tub a stripped eight Doesn’t makes any sense.\nNow Lets train the model and run this code again.\nNow I trained the model for 300 iteration, it started with an initial loss of 6.85 reaching to 0.1084 at the end of 300 iteration. So, our model learnt well from the data. Lets see how our model performs after training on the same seed string.\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to Much better . Lets generate a paragraph now .\nGenerated Text:\rscum my was much luckier than the others, because at ten o\u0026#39;clock the Tramp Major picked me out for the most coveted of all jobs in the spike, the job of helping in the workhouse kitchen. There was not really any work to be done there, and I was able to make off and hide in a shed used for storing potatoes, together with some workhouse paupers who were skulking to avoid the Sunday-morning service. There was a stove burning there, and comfortable packing cases to sit on, and back numbers of the Family Herald, and even a copy of Raffles from the workhouse library. It was paradise after the grime, of Sunday. It appeared the Tramp Major served the than before and the hot pipes. The cotton blankets were almost useless. One spent the night in turning from side to side, falling asleep for ten minutes and waking half frozen, and watching for the medical tables. It was a gloomy, chilly, limewashed place, consisting only of a bathroom and dining-room and about a hundred narrow stone cells. The terrible Tramp Major met us at the door and herded us into the bathroom to be stripped and searched. He was a gruff, soldierly man of forty, a pound of bread, a bit of margarine, and a pint of so-called tea. It took us five minutes to gulp down the cheap, noxious food. Then the Tramp Major served us with three cotton blankets each, and drove us off to our cells for the night. The doors were locked on the outside a little before seven in the evening, and would stay locked for the next twelve twelve hours in the spike, told tales of mooching, of pints stood him in the boozers, of the night for giving a back answer. When You, came to be searched, he fair held you upside down and shook you. If you were caught with tobacco there was bell to. Pay, and if you went in with money (which is against the law) God help help had no socks, except the tramps had already washed to our about and and for the lot of spring, perhaps—the authorities little It of the water, and decided to go dirty for the queer habit of sleeping in his hat, grumbled about a parcel of tommy that he had lost on the toad. Bill the moocher, the best built man of But it looks like our small model memorized the paragraph instead of learning any real patterns. This is a case of overfitting. Since we trained the model for 300 epochs on such a small dataset, it essentially “rote learned” the paragraph and just outputs the exact same text from the dataset\nIn generating text, we initially used argmax with torch.argmax(next_token_prob).item(), which simply picks the word with the highest probability from the output distribution. This approach is deterministic, meaning it will always choose the same word if the probabilities don\u0026rsquo;t change.\nNow, let’s try using a multinomial distribution instead. A multinomial distribution creates a probability distribution over all possible tokens, allowing us to sample the next token randomly based on the probabilities. Instead of always choosing the word with the highest probability, the multinomial distribution introduces some randomness by selecting tokens based on their likelihood.\nFor this uncomment the line # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item().\nThis method introduces variations in the text output, as the model can now generate different sequences even when given the same starting point. It\u0026rsquo;s like giving the model a little creative freedom, kind of like letting it freestyle instead of sticking to a strict script.\nThat wraps up this blog. Here, we generated text sequences using a trained LSTM model. I know there’s a lot of room for improvement, but the goal was to get a basic understanding of data preprocessing and how LSTMs work. For more structured and efficient code, it would make sense to use PyTorch’s nn.LSTM module.\nWhat\u0026rsquo;s Next? To improve this model:\nAdd Dropout: Prevent overfitting with regularization techniques. Use Better Sampling: Replace the random batch sampling with more structured approaches like sequential data loading. Increase Dataset Size: Larger datasets will yield more meaningful insights during training. Thanks for sticking around till the end! This being my first blog, I’m sure it’s not perfect, but I’ll work on improving my writing in the future. Appreciate you reading through!\nHere’s the GitHub repository with the code and dataset used for this project. Feel free to check it out!\nResources The Long Short-Term Memory (LSTM) Network from Scratch | Medium Implementing a LSTM from scratch with Numpy - Christina Kouridi Building makemore Part 2: MLP ","permalink":"http://localhost:1313/prtfoio/blog/lstm-from-scratch/","summary":"\u003ch1 id=\"lstm-from-scratch\"\u003eLSTM from Scratch\u003c/h1\u003e\n\u003cp\u003eIn this post, we will implement a simple next word predictor LSTM from scratch using torch.\u003c/p\u003e\n\u003ch2 id=\"a-gentle-introduction-to-lstm\"\u003eA gentle Introduction to LSTM\u003c/h2\u003e\n\u003cp\u003eLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by \u003ca href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\"\u003eHochreiter \u0026amp; Schmidhuber (1997)\u003c/a\u003e. As LSTMs are also a type of Recurrent Neural Network, they too have a hidden state, but they have another memory cell called the cell state as well.\u003c/p\u003e","title":"Implementing LSTM from scratch in PyTorch step-by-step."}]