<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/prtfoio/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=prtfoio/livereload" data-no-instant defer></script>
<meta property="og:type" content="website">
<meta property="og:title" content="Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch">
<meta property="og:description" content="A beginner-friendly explanation of self-attention and its implementation in PyTorch.">
<meta property="og:url" content="http://localhost:1313/prtfoio/blog/self-attention-pytorch/">
<meta property="og:image" content="">

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch | Agrim Paneru</title>
<meta name="keywords" content="Self-Attention, Attention Mechanism, Scaled Dot-Product Attention, Transformers, PyTorch Tutorial">
<meta name="description" content="A beginner-friendly explanation of self-attention and its implementation in PyTorch.">
<meta name="author" content="Agrim Paneru">
<link rel="canonical" href="http://localhost:1313/prtfoio/blog/self-attention-pytorch/">
<link crossorigin="anonymous" href="/prtfoio/assets/css/stylesheet.96c7d6443f7296882c1368d79c4a3a4eb0beea7637a5bb0631f2d1dcdee56598.css" integrity="sha256-lsfWRD9ylogsE2jXnEo6TrC&#43;6nY3pbsGMfLR3N7lZZg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/prtfoio/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/prtfoio/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/prtfoio/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/prtfoio/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/prtfoio/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<link rel="alternate" hreflang="en" href="http://localhost:1313/prtfoio/blog/self-attention-pytorch/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/prtfoio/blog/self-attention-pytorch/">
  <meta property="og:site_name" content="Agrim Paneru">
  <meta property="og:title" content="Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch">
  <meta property="og:description" content="A beginner-friendly explanation of self-attention and its implementation in PyTorch.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="Self-Attention">
    <meta property="article:tag" content="Transformers">
    <meta property="article:tag" content="PyTorch">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="College Notes">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch">
<meta name="twitter:description" content="A beginner-friendly explanation of self-attention and its implementation in PyTorch.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": " ",
      "item": "http://localhost:1313/prtfoio/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch",
      "item": "http://localhost:1313/prtfoio/blog/self-attention-pytorch/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch",
  "name": "Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch",
  "description": "A beginner-friendly explanation of self-attention and its implementation in PyTorch.",
  "keywords": [
    "Self-Attention", "Attention Mechanism", "Scaled Dot-Product Attention", "Transformers", "PyTorch Tutorial"
  ],
  "articleBody": "LMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !\nIn this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper “Attention Is All You Need” shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.\nEmbeddings representing the Words as vectors Initially the words are converted into tokens, Here we assume tokens are equal to index.\nimport torch torch.manual_seed(42) # Set seed for reproducibility embedding_dim = 3 words = ['the', 'cat', 'sat', 'on', 'mat'] embeddings = [torch.rand(embedding_dim) for word in words] Output for this code is :\n[tensor([0.8823, 0.9150, 0.3829]), tensor([0.9593, 0.3904, 0.6009]), tensor([0.2566, 0.7936, 0.9408]), tensor([0.1332, 0.9346, 0.5936]), tensor([0.8694, 0.5677, 0.7411])] Here the corresponding Embeddings for each word is initialized randomly. Here we represented each word as 3 dimensional embedding.\n{'the': tensor([0.8823, 0.9150, 0.3829]), 'cat': tensor([0.9593, 0.3904, 0.6009]), 'sat': tensor([0.2566, 0.7936, 0.9408]), 'on': tensor([0.1332, 0.9346, 0.5936]), 'mat': tensor([0.8694, 0.5677, 0.7411])} Attention Mechanism Attention Mechanism is inspired by how humans focus on specific parts of information when processing it—just like when we read a sentence, we don’t focus on every word equally. Instead, we “attend” to the most relevant words that help us understand the meaning.\nLets take an example for understanding attention mechanism:\nImagine reading the sentence: “The cat sat on the mat.”\nIf asked, “Where is the cat?”, attention would primarily focus on the word “cat” and the word “mat”. Other words like “the” or “on” are ignored since they don’t carry much relevance to the question.\nLets take a sentence:\nI am going to cinema to watch ………?\nThe most probable answers would be “movie,” “action movie,” or something similar. Words like “book” or “cat” don’t fit the context and are irrelevant to predicting the correct word. The key to predicting the correct word is in the context and information provided in the preceding words. To make accurate predictions, we need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the model to focus on the most relevant words to that context to make predictions.\nUnderstanding Scaled Dot-Product Attention The paper Attention Is All You Need introduced Scaled Dot-Product Attention .\nLets first calculate the Similarity between Query and Key\nThis is done as\nQ = K = V = embeddings similarity_matrix=torch.matmul(Q,K.T) In our case the dimension of output is 5*5.\nSimilarity Matrix:\ntensor([[1.7622, 1.4337, 1.3127, 1.1999, 1.5702], [1.4337, 1.4338, 1.1213, 0.8494, 1.5010], [1.3127, 1.1213, 1.5807, 1.3343, 1.3708], [1.1999, 0.8494, 1.3343, 1.2435, 1.0863], [1.5702, 1.5010, 1.3708, 1.0863, 1.6274]]) Here’s each value represents the similarity between words as described below.\nthe cat sat on mat the the-the the-cat the-sat the-on the-mat cat cat-the cat-cat cat-sat cat-on cat-mat sat sat-the sat-cat sat-sat sat-on sat-mat on on-the on-cat on-sat on-on on-mat mat mat-the mat-cat mat-sat mat-on mat-mat The similarity matrix captures the similarities between the query and key vectors. Higher similarity means the model should focus more on that word when making predictions, while lower similarity means the word is less relevant.\nScaling the compatibility matrix In the second step, we scale the dot-product of the query and key vectors by a factor of,\n$$ \\frac{1}{d_k} $$ . In our case We’ve only taken dimension of 3.\nHowever, when the dimension d is very large, dot-product attention without this scaling factor performs poorly. The authors of the paper “Attention Is All You Need” attribute this poor performance to having large variance.\nAs d_k increases, the variance of the dot product also becomes larger, leading to some values becoming extremely small after being passed through the softmax function. This causes issues during gradient calculation, similar to the vanishing gradient problem, where the gradients become too small to effectively update the model. To mitigate this, the dot product is scaled by $$ \\sqrt{\\frac{1}{d_k}} $$\nbefore applying softmax , which stabilizes the training process and improves performance.\nTo visualize this effect lets take output from softmax before and after scaling for embedding dimension of 256.\nApplying Softmax Activation The softmax function is used to convert the logits into probabilities. It takes a vector of raw scores (logits) and transforms them into a probability distribution. The softmax function is defined as:\n$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\nSoftmax is applied pointwise, so it doesn’t change the dimension of the input vector.\nimport torch.nn.functional as F similarity_matrix_scaled=similarity_matrix/(1/torch.sqrt(torch.tensor(3))) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) softmax_similarity_matrix_scaled Its output is:\ntensor([[0.2372, 0.1962, 0.1830, 0.1714, 0.2123], [0.2179, 0.2180, 0.1820, 0.1555, 0.2266], [0.1957, 0.1752, 0.2285, 0.1982, 0.2024], [0.2058, 0.1681, 0.2224, 0.2110, 0.1927], [0.2154, 0.2070, 0.1920, 0.1629, 0.2227]]) It can be visualized in Heatmap as\nComputing the Context Vector as final output. So for final step We multiply the attention matrix from previous step and multiply it with the Value matrix V. The final product gives the new context vector for all the words in vocabulary in our case 5.\nnew_context=torch.matmul(softmax_similarity_matrix_scaled,V) new_context Output:\ntensor([[0.6518, 0.7195, 0.6399], [0.6658, 0.7029, 0.6459], [0.6018, 0.7289, 0.6628], [0.5955, 0.7371, 0.6571], [0.6532, 0.7090, 0.6492]]) Conclusion In this section, we saw how contextual embeddings are generated using self-attention. We also discussed the importance of scaling and how to implement a simple self-attention block. Notably, there were no trainable parameters involved here, and we treated the Query (Q), Key (K), and Value (V) matrices as all being the same.\nHowever, to make the attention block trainable, we need to introduce learnable weights when deriving the Q, K, and V vectors from the original embedding. These weights will allow the model to adjust and fine-tune the attention mechanism during training. In the next section, we’ll explore how to implement this by adding trainable parameters to the attention block\nAttention Mechanism with trainable Parameters Alright! Now that we understand how self-attention is calculated, let’s take it a step further by introducing trainable parameters so that the model can learn how to find the context of words effectively.\nAdding Trainable Parameters to Self-Attention Q (Query), K (Key), and V (Value) matrices are not just direct copies of word embeddings. Instead, they are learned transformations of these embeddings.\nTo achieve this, we introduce three trainable weight matrices:\nW_q (Query weight matrix): Learns how to project word embeddings into the query space. W_k (Key weight matrix): Learns how to project word embeddings into the key space. W_v (Value weight matrix): Learns how to project word embeddings into the value space. Each of these matrices will be optimized during training.\nW_q=torch.randn(embedding_dim,embedding_dim) W_v=torch.randn(embedding_dim,embedding_dim W_k=torch.randn(embedding_dim,embedding_dim) Note that the dimension of these matrix can be made different. However the first dimension must be equal to embedding dimension to satisfy the matrix multiplication condition. Changing the second dimension implies projecting the embedding dimension from one dimension to other .\nLets get the new Query, Key and Value vector.\nQ = torch.matmul(embeddings, W_q) # (5, 3) K = torch.matmul(embeddings, W_k) # (5, 3) V = torch.matmul(embeddings, W_v) # (5, 3) Then we calculate the self attention similar to how we did above.\nsimilarity_matrix = torch.matmul(Q, K.T) # (5, 5) similarity_matrix_scaled = similarity_matrix / torch.sqrt(torch.tensor(embedding_dim, dtype=torch.float32)) softmax_similarity_matrix_scaled = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(softmax_similarity_matrix_scaled, V) This is what the final flow looks like.\nHere Lets wrap up this in Class Based Implementation\nimport torch import torch.nn.functional as F class SelfAttention: def __init__(self, embedding_dim): torch.manual_seed(42) self.embedding_dim = embedding_dim # Initialize weight matrices self.W_q = torch.randn(embedding_dim, embedding_dim) self.W_k = torch.randn(embedding_dim, embedding_dim) self.W_v = torch.randn(embedding_dim, embedding_dim) def forward(self, embeddings): # Compute Query, Key, and Value matrices Q = torch.matmul(embeddings, self.W_q) K = torch.matmul(embeddings, self.W_k) V = torch.matmul(embeddings, self.W_v) # Compute similarity (dot product attention) similarity_matrix = torch.matmul(Q, K.T) # (num_words, num_words) # Scale by sqrt(embedding_dim) similarity_matrix_scaled = similarity_matrix / torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32) ) # Apply softmax to get attention weights attention_weights = F.softmax(similarity_matrix_scaled, dim=1) new_context = torch.matmul(attention_weights, V) return new_context, attention_weights embedding_dim = 3 words = ['the', 'cat', 'sat', 'on', 'mat'] # Generate random embeddings for words embeddings = torch.stack([torch.rand(embedding_dim) for _ in words]) # Initialize attention mechanism self_attention = SelfAttention(embedding_dim) # Forward pass to compute attention new_context, attention_weights = self_attention.forward(embeddings) print(\"New Context Vectors:\\n\", new_context) print(\"\\nAttention Weights:\\n\", attention_weights) Output:\nNew Context Vectors: tensor([[ 0.3689, -0.4890, 1.3677], [ 0.2569, -0.4673, 1.3297], [ 0.3284, -0.4752, 1.3515], [ 0.1971, -0.4685, 1.3124], [ 0.3061, -0.4846, 1.3491]]) Attention Weights: tensor([[0.4672, 0.0538, 0.0879, 0.0829, 0.3082], [0.3362, 0.1266, 0.1921, 0.1077, 0.2375], [0.4212, 0.0851, 0.1492, 0.0815, 0.2631], [0.2728, 0.1547, 0.1886, 0.1526, 0.2314], [0.3922, 0.0832, 0.1179, 0.1143, 0.2923]]) So this ends up this blog. Congratulations on making it to the end. Feel free to experiment with different dimensions and input data to see how the self-attention mechanism adapts and scales.\nReferences Implementing Self-Attention from Scratch in PyTorch | by Mohd Faraaz | Medium\nUnderstanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\nSelf Attention in Transformers | Deep Learning | Simple Explanation with Code!\nSelf-attention from scratch | Blogs by Anil\n",
  "wordCount" : "1541",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Agrim Paneru"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/prtfoio/blog/self-attention-pytorch/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Agrim Paneru",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/prtfoio/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/prtfoio/" accesskey="h" title="Agrim Paneru (Alt + H)">Agrim Paneru</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/prtfoio/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/prtfoio/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/prtfoio/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/prtfoio/experience" title="Experience">
                    <span>Experience</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/prtfoio/search" title="Search">
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/prtfoio/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/prtfoio/blog/"> </a></div>
    <h1 class="post-title entry-hint-parent">
      Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch
    </h1>
    <div class="post-description">
      A beginner-friendly explanation of self-attention and its implementation in PyTorch.
    </div>
    <div class="post-meta">⏱️ 8 min&nbsp;·&nbsp;Agrim Paneru

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#embeddings-representing-the-words-as-vectors" aria-label="Embeddings representing the Words as vectors">Embeddings representing the Words as vectors</a><ul>
                        
                <li>
                    <a href="#attention-mechanism" aria-label="Attention Mechanism">Attention Mechanism</a></li>
                <li>
                    <a href="#understanding-scaled-dot-product-attention" aria-label="Understanding Scaled Dot-Product Attention">Understanding Scaled Dot-Product Attention</a></li>
                <li>
                    <a href="#scaling-the-compatibility-matrix" aria-label="Scaling the compatibility matrix">Scaling the compatibility matrix</a></li>
                <li>
                    <a href="#applying-softmax-activation" aria-label="Applying Softmax Activation">Applying Softmax Activation</a></li>
                <li>
                    <a href="#computing-the-context-vector-as-final-output" aria-label="Computing the Context Vector as final output.">Computing the Context Vector as final output.</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#attention-mechanism-with-trainable-parameters" aria-label="Attention Mechanism with trainable Parameters">Attention Mechanism with trainable Parameters</a><ul>
                        
                <li>
                    <a href="#adding-trainable-parameters-to-self-attention" aria-label="Adding Trainable Parameters to Self-Attention">Adding Trainable Parameters to Self-Attention</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>LMs are trained to predict the next word based on the context of the previous words. However, to make accurate predictions, LMs need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the LM to focus on the most relevant words to that context to make predictions. !</p>
<p>In this post, we’ll implement scaled dot-product attention in a simple way. Back in the day, RNNs were the standard for sequence-to-sequence tasks, but everything changed when attention mechanisms came into the picture. Then, the groundbreaking paper <em>&ldquo;Attention Is All You Need&rdquo;</em> shook things up even more, showing that RNNs weren’t necessary at all—attention alone could handle it. Since then, attention has become the backbone of modern architectures like Transformers.</p>
<h2 id="embeddings-representing-the-words-as-vectors">Embeddings representing the Words as vectors<a hidden class="anchor" aria-hidden="true" href="#embeddings-representing-the-words-as-vectors">#</a></h2>
<p>Initially the words are converted into tokens, Here we assume tokens are equal to index.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">42</span>)  <span style="color:#75715e"># Set seed for reproducibility</span>
</span></span><span style="display:flex;"><span>embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>   
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>words <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;the&#39;</span>, <span style="color:#e6db74">&#39;cat&#39;</span>, <span style="color:#e6db74">&#39;sat&#39;</span>, <span style="color:#e6db74">&#39;on&#39;</span>, <span style="color:#e6db74">&#39;mat&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> [torch<span style="color:#f92672">.</span>rand(embedding_dim) <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> words]
</span></span></code></pre></div><p>Output for this code is :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[tensor([<span style="color:#ae81ff">0.8823</span>, <span style="color:#ae81ff">0.9150</span>, <span style="color:#ae81ff">0.3829</span>]),
</span></span><span style="display:flex;"><span> tensor([<span style="color:#ae81ff">0.9593</span>, <span style="color:#ae81ff">0.3904</span>, <span style="color:#ae81ff">0.6009</span>]),
</span></span><span style="display:flex;"><span> tensor([<span style="color:#ae81ff">0.2566</span>, <span style="color:#ae81ff">0.7936</span>, <span style="color:#ae81ff">0.9408</span>]),
</span></span><span style="display:flex;"><span> tensor([<span style="color:#ae81ff">0.1332</span>, <span style="color:#ae81ff">0.9346</span>, <span style="color:#ae81ff">0.5936</span>]),
</span></span><span style="display:flex;"><span> tensor([<span style="color:#ae81ff">0.8694</span>, <span style="color:#ae81ff">0.5677</span>, <span style="color:#ae81ff">0.7411</span>])]
</span></span></code></pre></div><p>Here the corresponding Embeddings for each word is initialized randomly. Here we represented each word as 3 dimensional embedding.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>{<span style="color:#e6db74">&#39;the&#39;</span>: tensor([<span style="color:#ae81ff">0.8823</span>, <span style="color:#ae81ff">0.9150</span>, <span style="color:#ae81ff">0.3829</span>]),
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;cat&#39;</span>: tensor([<span style="color:#ae81ff">0.9593</span>, <span style="color:#ae81ff">0.3904</span>, <span style="color:#ae81ff">0.6009</span>]),
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;sat&#39;</span>: tensor([<span style="color:#ae81ff">0.2566</span>, <span style="color:#ae81ff">0.7936</span>, <span style="color:#ae81ff">0.9408</span>]),
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;on&#39;</span>: tensor([<span style="color:#ae81ff">0.1332</span>, <span style="color:#ae81ff">0.9346</span>, <span style="color:#ae81ff">0.5936</span>]),
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;mat&#39;</span>: tensor([<span style="color:#ae81ff">0.8694</span>, <span style="color:#ae81ff">0.5677</span>, <span style="color:#ae81ff">0.7411</span>])}
</span></span></code></pre></div><h3 id="attention-mechanism"><strong>Attention Mechanism</strong><a hidden class="anchor" aria-hidden="true" href="#attention-mechanism">#</a></h3>
<p>Attention Mechanism is inspired by how humans focus on specific parts of information when processing it—just like when we read a sentence, we don&rsquo;t focus on every word equally. Instead, we &ldquo;attend&rdquo; to the most relevant words that help us understand the meaning.</p>
<p>Lets take an example for understanding attention mechanism:</p>
<p>Imagine reading the sentence: <em>&ldquo;The cat sat on the mat.&rdquo;</em></p>
<p>If  asked, <em>&ldquo;Where is the cat?&rdquo;</em>, attention would primarily focus on the word <em>&ldquo;cat&rdquo;</em> and the word <em>&ldquo;mat&rdquo;</em>. Other words  like <em>&ldquo;the&rdquo;</em> or <em>&ldquo;on&rdquo;</em>  are ignored since they don’t carry much relevance to the question.</p>
<p>Lets take a sentence:</p>
<p>I am going to cinema to watch ………?</p>
<p>The most probable answers would be &ldquo;movie,&rdquo; &ldquo;action movie,&rdquo; or something similar. Words like &ldquo;book&rdquo; or &ldquo;cat&rdquo; don&rsquo;t fit the context and are irrelevant to predicting the correct word. The key to predicting the correct word is in the context and information provided in the preceding words. To make accurate predictions, we need to understand the relationship between words in the sentence. This is the objective of attention mechanism — it helps the model to focus on the most relevant words to that context to make predictions.</p>
<h3 id="understanding-scaled-dot-product-attention"><strong>Understanding Scaled Dot-Product Attention</strong><a hidden class="anchor" aria-hidden="true" href="#understanding-scaled-dot-product-attention">#</a></h3>
<p>The paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> introduced Scaled Dot-Product Attention .</p>
<p><img alt="image.png" loading="lazy" src="image.png"></p>
<p>Lets first calculate the Similarity between Query and Key</p>
<p>This is done as</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> K <span style="color:#f92672">=</span> V <span style="color:#f92672">=</span> embeddings
</span></span><span style="display:flex;"><span>similarity_matrix<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>matmul(Q,K<span style="color:#f92672">.</span>T)
</span></span></code></pre></div><p><img alt="image.png" loading="lazy" src="image%201.png"></p>
<p>In our case the dimension of output is 5*5.</p>
<p>Similarity Matrix:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([[<span style="color:#ae81ff">1.7622</span>, <span style="color:#ae81ff">1.4337</span>, <span style="color:#ae81ff">1.3127</span>, <span style="color:#ae81ff">1.1999</span>, <span style="color:#ae81ff">1.5702</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">1.4337</span>, <span style="color:#ae81ff">1.4338</span>, <span style="color:#ae81ff">1.1213</span>, <span style="color:#ae81ff">0.8494</span>, <span style="color:#ae81ff">1.5010</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">1.3127</span>, <span style="color:#ae81ff">1.1213</span>, <span style="color:#ae81ff">1.5807</span>, <span style="color:#ae81ff">1.3343</span>, <span style="color:#ae81ff">1.3708</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">1.1999</span>, <span style="color:#ae81ff">0.8494</span>, <span style="color:#ae81ff">1.3343</span>, <span style="color:#ae81ff">1.2435</span>, <span style="color:#ae81ff">1.0863</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">1.5702</span>, <span style="color:#ae81ff">1.5010</span>, <span style="color:#ae81ff">1.3708</span>, <span style="color:#ae81ff">1.0863</span>, <span style="color:#ae81ff">1.6274</span>]])
</span></span></code></pre></div><p>Here’s each value represents the similarity between words as described below.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th><strong>the</strong></th>
          <th><strong>cat</strong></th>
          <th><strong>sat</strong></th>
          <th><strong>on</strong></th>
          <th><strong>mat</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>the</strong></td>
          <td>the-the</td>
          <td>the-cat</td>
          <td>the-sat</td>
          <td>the-on</td>
          <td>the-mat</td>
      </tr>
      <tr>
          <td><strong>cat</strong></td>
          <td>cat-the</td>
          <td>cat-cat</td>
          <td>cat-sat</td>
          <td>cat-on</td>
          <td>cat-mat</td>
      </tr>
      <tr>
          <td><strong>sat</strong></td>
          <td>sat-the</td>
          <td>sat-cat</td>
          <td>sat-sat</td>
          <td>sat-on</td>
          <td>sat-mat</td>
      </tr>
      <tr>
          <td><strong>on</strong></td>
          <td>on-the</td>
          <td>on-cat</td>
          <td>on-sat</td>
          <td>on-on</td>
          <td>on-mat</td>
      </tr>
      <tr>
          <td><strong>mat</strong></td>
          <td>mat-the</td>
          <td>mat-cat</td>
          <td>mat-sat</td>
          <td>mat-on</td>
          <td>mat-mat</td>
      </tr>
  </tbody>
</table>
<p>The similarity matrix captures the similarities between the query and key vectors. Higher similarity means the model should focus more on that word when making predictions, while lower similarity means the word is less relevant.</p>
<h3 id="scaling-the-compatibility-matrix"><strong>Scaling the compatibility matrix</strong><a hidden class="anchor" aria-hidden="true" href="#scaling-the-compatibility-matrix">#</a></h3>
<p>In the second step, we scale the dot-product of the query and key vectors by a factor of,</p>
<p>$$
\frac{1}{d_k}
$$
. In our case We’ve only taken dimension of 3.</p>
<p>However, when the dimension d is very large, dot-product attention without this scaling factor performs poorly. The authors of the paper “<em>Attention Is All You Need</em>” attribute this poor performance to having large variance.</p>
<p>As d_k increases, the variance of the dot product also becomes larger, leading to some values becoming extremely small after being passed through the softmax function. This causes issues during gradient calculation, similar to the vanishing gradient problem, where the gradients become too small to effectively update the model. To mitigate this, the dot product is scaled by $$
\sqrt{\frac{1}{d_k}}
$$</p>
<p>before applying softmax , which stabilizes the training process and improves performance.</p>
<p>To visualize this effect lets take output from softmax before and after scaling for embedding dimension of 256.</p>
<p><img alt="image.png" loading="lazy" src="image%202.png"></p>
<h3 id="applying-softmax-activation">Applying Softmax Activation<a hidden class="anchor" aria-hidden="true" href="#applying-softmax-activation">#</a></h3>
<p>The <strong>softmax</strong> function is used to convert the logits into probabilities. It takes a vector of raw scores (logits) and transforms them into a probability distribution. The softmax function is defined as:</p>
<h3></h3>
<p>$$
\text{softmax}(x) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$</p>
<p><img alt="image.png" loading="lazy" src="image%203.png"></p>
<p>Softmax is applied pointwise, so it doesn&rsquo;t change the dimension of the input vector.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>similarity_matrix_scaled<span style="color:#f92672">=</span>similarity_matrix<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span>)))
</span></span><span style="display:flex;"><span>softmax_similarity_matrix_scaled <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(similarity_matrix_scaled, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>softmax_similarity_matrix_scaled
</span></span></code></pre></div><p>Its output is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([[<span style="color:#ae81ff">0.2372</span>, <span style="color:#ae81ff">0.1962</span>, <span style="color:#ae81ff">0.1830</span>, <span style="color:#ae81ff">0.1714</span>, <span style="color:#ae81ff">0.2123</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.2179</span>, <span style="color:#ae81ff">0.2180</span>, <span style="color:#ae81ff">0.1820</span>, <span style="color:#ae81ff">0.1555</span>, <span style="color:#ae81ff">0.2266</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.1957</span>, <span style="color:#ae81ff">0.1752</span>, <span style="color:#ae81ff">0.2285</span>, <span style="color:#ae81ff">0.1982</span>, <span style="color:#ae81ff">0.2024</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.2058</span>, <span style="color:#ae81ff">0.1681</span>, <span style="color:#ae81ff">0.2224</span>, <span style="color:#ae81ff">0.2110</span>, <span style="color:#ae81ff">0.1927</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.2154</span>, <span style="color:#ae81ff">0.2070</span>, <span style="color:#ae81ff">0.1920</span>, <span style="color:#ae81ff">0.1629</span>, <span style="color:#ae81ff">0.2227</span>]])
</span></span><span style="display:flex;"><span>       
</span></span></code></pre></div><p>It can be visualized in Heatmap as</p>
<p><img alt="image.png" loading="lazy" src="image%204.png"></p>
<h3 id="computing-the-context-vector-as-final-output"><strong>Computing the Context Vector as final output.</strong><a hidden class="anchor" aria-hidden="true" href="#computing-the-context-vector-as-final-output">#</a></h3>
<p>So for final step We multiply the attention matrix from previous step and multiply it with the Value matrix V. The final product gives the new context vector for all the words in vocabulary in our case 5.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>new_context<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>matmul(softmax_similarity_matrix_scaled,V)
</span></span><span style="display:flex;"><span>new_context
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([[<span style="color:#ae81ff">0.6518</span>, <span style="color:#ae81ff">0.7195</span>, <span style="color:#ae81ff">0.6399</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.6658</span>, <span style="color:#ae81ff">0.7029</span>, <span style="color:#ae81ff">0.6459</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.6018</span>, <span style="color:#ae81ff">0.7289</span>, <span style="color:#ae81ff">0.6628</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.5955</span>, <span style="color:#ae81ff">0.7371</span>, <span style="color:#ae81ff">0.6571</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.6532</span>, <span style="color:#ae81ff">0.7090</span>, <span style="color:#ae81ff">0.6492</span>]])
</span></span></code></pre></div><p><img alt="image.png" loading="lazy" src="image%205.png"></p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>In this section, we saw how contextual embeddings are generated using self-attention. We also discussed the importance of scaling and how to implement a simple self-attention block. Notably, there were no trainable parameters involved here, and we treated the Query (Q), Key (K), and Value (V) matrices as all being the same.</p>
<p>However, to make the attention block trainable, we need to introduce learnable weights when deriving the Q, K, and V vectors from the original embedding. These weights will allow the model to adjust and fine-tune the attention mechanism during training. In the next section, we&rsquo;ll explore how to implement this by adding trainable parameters to the attention block</p>
<h2 id="attention-mechanism-with-trainable-parameters">Attention Mechanism with trainable Parameters<a hidden class="anchor" aria-hidden="true" href="#attention-mechanism-with-trainable-parameters">#</a></h2>
<p>Alright! Now that we understand how self-attention is calculated, let&rsquo;s take it a step further by introducing <strong>trainable parameters</strong> so that the model can learn how to find the context of words effectively.</p>
<h3 id="adding-trainable-parameters-to-self-attention"><strong>Adding Trainable Parameters to Self-Attention</strong><a hidden class="anchor" aria-hidden="true" href="#adding-trainable-parameters-to-self-attention">#</a></h3>
<p><strong>Q (Query), K (Key), and V (Value)</strong> matrices are <strong>not</strong> just direct copies of word embeddings. Instead, they are <strong>learned transformations</strong> of these embeddings.</p>
<p>To achieve this, we introduce three <strong>trainable weight matrices</strong>:</p>
<ul>
<li><strong>W_q (Query weight matrix)</strong>: Learns how to project word embeddings into the query space.</li>
<li><strong>W_k (Key weight matrix)</strong>: Learns how to project word embeddings into the key space.</li>
<li><strong>W_v (Value weight matrix)</strong>: Learns how to project word embeddings into the value space.</li>
</ul>
<p>Each of these matrices will be optimized during training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>W_q<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>randn(embedding_dim,embedding_dim)
</span></span><span style="display:flex;"><span>W_v<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>randn(embedding_dim,embedding_dim
</span></span><span style="display:flex;"><span>W_k<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>randn(embedding_dim,embedding_dim)
</span></span></code></pre></div><p>Note that the dimension of these matrix can be made different. However the first dimension must be equal to embedding dimension to satisfy the matrix multiplication condition. Changing the second dimension implies projecting the embedding dimension from one dimension to other .</p>
<p>Lets get the new Query, Key and Value vector.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(embeddings, W_q)  <span style="color:#75715e"># (5, 3)</span>
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(embeddings, W_k)  <span style="color:#75715e"># (5, 3)</span>
</span></span><span style="display:flex;"><span>V <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(embeddings, W_v)  <span style="color:#75715e"># (5, 3)</span>
</span></span></code></pre></div><p>Then we calculate the self attention similar to how we did above.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>similarity_matrix <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(Q, K<span style="color:#f92672">.</span>T)  <span style="color:#75715e"># (5, 5)</span>
</span></span><span style="display:flex;"><span>similarity_matrix_scaled <span style="color:#f92672">=</span> similarity_matrix <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>tensor(embedding_dim, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32))
</span></span><span style="display:flex;"><span>softmax_similarity_matrix_scaled <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(similarity_matrix_scaled, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>new_context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(softmax_similarity_matrix_scaled, V)
</span></span></code></pre></div><p><img alt="image.png" loading="lazy" src="image%206.png"></p>
<p>This is what the final flow looks like.</p>
<p>Here Lets wrap up this in Class Based Implementation</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SelfAttention</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, embedding_dim):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embedding_dim <span style="color:#f92672">=</span> embedding_dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Initialize weight matrices</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_q <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(embedding_dim, embedding_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_k <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(embedding_dim, embedding_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_v <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(embedding_dim, embedding_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, embeddings):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute Query, Key, and Value matrices</span>
</span></span><span style="display:flex;"><span>        Q <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(embeddings, self<span style="color:#f92672">.</span>W_q)  
</span></span><span style="display:flex;"><span>        K <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(embeddings, self<span style="color:#f92672">.</span>W_k)  
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(embeddings, self<span style="color:#f92672">.</span>W_v)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute similarity (dot product attention)</span>
</span></span><span style="display:flex;"><span>        similarity_matrix <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(Q, K<span style="color:#f92672">.</span>T)  <span style="color:#75715e"># (num_words, num_words)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Scale by sqrt(embedding_dim)</span>
</span></span><span style="display:flex;"><span>        similarity_matrix_scaled <span style="color:#f92672">=</span> similarity_matrix <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>sqrt(
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>tensor(self<span style="color:#f92672">.</span>embedding_dim, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Apply softmax to get attention weights</span>
</span></span><span style="display:flex;"><span>        attention_weights <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(similarity_matrix_scaled, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      
</span></span><span style="display:flex;"><span>        new_context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(attention_weights, V)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> new_context, attention_weights
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>words <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;the&#39;</span>, <span style="color:#e6db74">&#39;cat&#39;</span>, <span style="color:#e6db74">&#39;sat&#39;</span>, <span style="color:#e6db74">&#39;on&#39;</span>, <span style="color:#e6db74">&#39;mat&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate random embeddings for words</span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack([torch<span style="color:#f92672">.</span>rand(embedding_dim) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> words])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize attention mechanism</span>
</span></span><span style="display:flex;"><span>self_attention <span style="color:#f92672">=</span> SelfAttention(embedding_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Forward pass to compute attention</span>
</span></span><span style="display:flex;"><span>new_context, attention_weights <span style="color:#f92672">=</span> self_attention<span style="color:#f92672">.</span>forward(embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;New Context Vectors:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, new_context)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Attention Weights:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, attention_weights)
</span></span></code></pre></div><p><strong>Output:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>New Context Vectors:
</span></span><span style="display:flex;"><span> tensor([[ <span style="color:#ae81ff">0.3689</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4890</span>,  <span style="color:#ae81ff">1.3677</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">0.2569</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4673</span>,  <span style="color:#ae81ff">1.3297</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">0.3284</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4752</span>,  <span style="color:#ae81ff">1.3515</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">0.1971</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4685</span>,  <span style="color:#ae81ff">1.3124</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">0.3061</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4846</span>,  <span style="color:#ae81ff">1.3491</span>]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Attention Weights:
</span></span><span style="display:flex;"><span> tensor([[<span style="color:#ae81ff">0.4672</span>, <span style="color:#ae81ff">0.0538</span>, <span style="color:#ae81ff">0.0879</span>, <span style="color:#ae81ff">0.0829</span>, <span style="color:#ae81ff">0.3082</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.3362</span>, <span style="color:#ae81ff">0.1266</span>, <span style="color:#ae81ff">0.1921</span>, <span style="color:#ae81ff">0.1077</span>, <span style="color:#ae81ff">0.2375</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.4212</span>, <span style="color:#ae81ff">0.0851</span>, <span style="color:#ae81ff">0.1492</span>, <span style="color:#ae81ff">0.0815</span>, <span style="color:#ae81ff">0.2631</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.2728</span>, <span style="color:#ae81ff">0.1547</span>, <span style="color:#ae81ff">0.1886</span>, <span style="color:#ae81ff">0.1526</span>, <span style="color:#ae81ff">0.2314</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.3922</span>, <span style="color:#ae81ff">0.0832</span>, <span style="color:#ae81ff">0.1179</span>, <span style="color:#ae81ff">0.1143</span>, <span style="color:#ae81ff">0.2923</span>]])
</span></span></code></pre></div><p>So this ends up this blog. Congratulations on making it to the end. Feel free to experiment with different dimensions and input data to see how the self-attention mechanism adapts and scales.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p><a href="https://mohdfaraaz.medium.com/implementing-self-attention-from-scratch-in-pytorch-776ef7b8f13e">Implementing Self-Attention from Scratch in PyTorch | by Mohd Faraaz | Medium</a></p>
<p><a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</a></p>
<p><a href="https://www.youtube.com/watch?v=-tCKPl_8Xb8">Self Attention in Transformers | Deep Learning | Simple Explanation with Code!</a></p>
<p><a href="https://paudelanil9.com.np/posts/self-attention-from-scratch/">Self-attention from scratch | Blogs by Anil</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/prtfoio/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="http://localhost:1313/prtfoio/tags/self-attention/">Self-Attention</a></li>
      <li><a href="http://localhost:1313/prtfoio/tags/transformers/">Transformers</a></li>
      <li><a href="http://localhost:1313/prtfoio/tags/pytorch/">PyTorch</a></li>
      <li><a href="http://localhost:1313/prtfoio/tags/ai/">AI</a></li>
      <li><a href="http://localhost:1313/prtfoio/tags/college-notes/">College Notes</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/prtfoio/">Agrim Paneru</a></span> · 

    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
